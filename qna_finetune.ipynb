{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import kaggle\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,  DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, trim train and val for easier computation\n",
    "dataset = load_dataset(\"squad\")\n",
    "traindata = dataset[\"train\"].shuffle().select(range(10000))\n",
    "valdata = dataset[\"validation\"].shuffle().select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50263, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add extra special characters\n",
    "\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<|bos|>\",\n",
    "                              \"eos_token\": \"<|eos|>\",\n",
    "                              \"unk_token\": \"<|unk|>\",\n",
    "                              \"sep_token\": \"<|sep|>\",\n",
    "                              \"pad_token\": \"<|pad|>\",\n",
    "                              \"cls_token\": \"<|cls|>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7736.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for t in tqdm(traindata):\n",
    "    if len(t['answers']['text']) > 1:\n",
    "        print(t)\n",
    "# traindata[0]['answers'][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50263, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = tokenizer(traindata[0][\"context\"] + \"<|bos|>\" + traindata[0][\"question\"] + \"<|sep|>\" + traindata[0][\"answers\"][\"text\"][0] + \"<|eos|>\",\n",
    "                           padding=True, truncation=True)\n",
    "default_label = tokenizer(\"<|pad|>\" * (len(traindata[0][\"context\"] + \"<|bos|>\" + traindata[0][\"question\"]) - 1)\n",
    "                          + \"<|sep|>\" + traindata[0][\"answers\"][\"text\"][0] + \"<|eos|>\" \n",
    "                          + \"<|pad|>\",\n",
    "                          padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all examples in the dataset w/ huggingfaces dataset.map()\n",
    "\n",
    "def tokenize_function(item):\n",
    "       lengths = [len(tokenizer(context  + \"<|bos|>\" + q + \"<|sep|>\",\n",
    "                  truncation=True)['input_ids']) \\\n",
    "              for (context, q) in zip(item[\"context\"], item[\"question\"])]\n",
    "\n",
    "\n",
    "       default_prompt = [tokenizer(context + \"<|bos|>\" + q + \"<|sep|>\" + a,\n",
    "                            truncation=True) \\\n",
    "                     for (context, q, a) in zip(item[\"context\"], item[\"question\"], [k[\"text\"][0] for k in item[\"answers\"]])]\n",
    "       # default_label = [tokenizer(\"<|pad|> \" * (len(context + \"<|bos|>\" + q) - 1)\n",
    "\n",
    "       default_label = [tokenizer(context + \"<|bos|>\" + q + \"<|sep|>\" + a + \"<|eos|>\",\n",
    "                            truncation=True) \\\n",
    "                     for (context, q, a) in zip(item[\"context\"], item[\"question\"], [k[\"text\"][0] for k in item[\"answers\"]])]\n",
    "\n",
    "       for i, l in enumerate(lengths):\n",
    "              # default_label[i][\"input_ids\"][:l] = [tokenizer.pad_token_id for i in range(l)]\n",
    "              default_label[i][\"input_ids\"][:l] = [-100 for i in range(l)]\n",
    "\n",
    "       ret = {\"input_ids\": [i[\"input_ids\"] for i in default_prompt],\n",
    "              \"attention_mask\": [a[\"attention_mask\"] for a in default_prompt],\n",
    "              \"labels\": [i[\"input_ids\"][1:] for i in default_label]}         # shift labels by one\n",
    "\n",
    "       return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258c91dcfe7e4077b9cf24fd755934ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = valdata.map(tokenize_function, batched=True, remove_columns=valdata.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0b4fe26f99443582d11a95f07331bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca28701ca9649d7b1c0f1ff065a248a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_traindata = traindata.map(tokenize_function, batched=True, remove_columns=traindata.column_names)\n",
    "tokenized_valdata = valdata.map(tokenize_function, batched=True, remove_columns=valdata.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_valdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator =  DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39424, 262, 15993, 2647, 11, 340, 373, 832, 262, 314, 27625, 14807, 357, 4758, 547, 739, 8830, 3896, 290, 4588, 8, 326, 477, 262, 1688, 14901, 286, 262, 8830, 3427, 15993, 2647, 547, 5495, 284, 22779, 25059, 13, 383, 3814, 318, 12411, 329, 262, 4082, 286, 262, 717, 3961, 286, 3660, 8312, 15993, 2647, 357, 1544, 457, 272, 2771, 272, 393, 314, 27625, 3961, 11, 8312, 25, 7377, 243, 46582, 32830, 17394, 26180, 138, 115, 38392, 29945, 17394, 43000, 138, 106, 7377, 96, 139, 229, 26517, 39377, 138, 106, 828, 4920, 287, 1248, 1314, 13, 10335, 7233, 10826, 286, 428, 12121, 2291, 47817, 418, 36987, 41046, 418, 11, 1338, 2417, 47287, 1395, 88, 358, 292, 11, 1338, 2417, 47287, 34077, 292, 290, 24081, 33280, 1879, 11751, 13, 1869, 8506, 12612, 296, 29616, 318, 3177, 262, 9119, 286, 262, 8312, 2351, 3961, 286, 7849, 13, 50257, 2215, 373, 262, 717, 1524, 286, 3660, 8312, 15993, 2647, 7042, 30, 50260, 1507, 1314], [23672, 13345, 72, 373, 20056, 284, 262, 2457, 11100, 286, 262, 369, 44281, 379, 604, 25, 405, 9114, 13, 679, 373, 7018, 26850, 379, 604, 25, 1270, 9114, 351, 257, 2472, 286, 4353, 5690, 13, 2293, 262, 890, 45443, 22460, 286, 13258, 350, 3754, 39284, 11, 262, 2657, 6897, 7690, 257, 582, 508, 784, 340, 373, 25751, 780, 286, 465, 6190, 2479, 784, 561, 307, 257, 1790, 12, 4354, 393, 366, 11338, 12, 43554, 1, 26850, 13, 1119, 16555, 284, 3853, 257, 4540, 508, 561, 466, 1310, 1141, 262, 649, 45443, 22460, 13, 14438, 465, 3071, 11, 25564, 24532, 309, 747, 263, 415, 1965, 683, 262, 12146, 2683, 286, 1771, 339, 561, 2453, 290, 611, 523, 11, 644, 1438, 339, 561, 1011, 329, 2241, 13, 6575, 13345, 72, 2921, 262, 717, 286, 465, 867, 24072, 618, 339, 7690, 366, 7554, 1, 355, 465, 842, 77, 282, 1438, 13, 6575, 13345, 72, 338, 2748, 2456, 547, 366, 40, 481, 307, 1444, 1757, 1911, 770, 373, 262, 717, 640, 287, 625, 5323, 812, 326, 428, 1438, 550, 587, 7147, 26, 2180, 1461, 274, 550, 13941, 663, 779, 1201, 262, 640, 286, 262, 3738, 541, 3008, 1757, 21044, 10855, 1141, 262, 4885, 3059, 1042, 1811, 10675, 878, 13, 50257, 2215, 373, 262, 2457, 11100, 286, 262, 369, 44281, 30, 50260, 19, 25, 405, 9114], [464, 938, 1049, 2278, 286, 7993, 47076, 1242, 373, 262, 1105, 400, 1906, 1485, 400, 4289, 618, 10598, 4166, 663, 898, 18778, 17290, 3918, 11, 1479, 422, 262, 7646, 3173, 286, 10183, 6761, 290, 351, 257, 517, 12653, 33578, 286, 5538, 287, 262, 2272, 13, 3894, 12, 4002, 2499, 286, 428, 2278, 389, 262, 38838, 39788, 873, 286, 262, 32520, 3970, 2566, 2986, 27311, 68, 11, 262, 24685, 16175, 671, 286, 8909, 14200, 287, 833, 4594, 4119, 290, 2986, 11243, 14057, 14035, 10145, 443, 337, 5330, 13, 383, 4950, 2471, 325, 47076, 286, 8909, 14200, 287, 833, 4594, 4119, 357, 1157, 1821, 8, 28539, 1951, 290, 5335, 5586, 1306, 284, 1123, 584, 319, 262, 32271, 19262, 11, 262, 717, 1672, 286, 428, 7196, 6826, 7791, 13, 317, 2092, 47076, 11, 262, 2744, 261, 341, 286, 262, 5283, 11, 11705, 689, 262, 2471, 325, 286, 8909, 14200, 2944, 12397, 382, 13, 632, 318, 257, 670, 286, 8445, 404, 78, 4022, 799, 72, 422, 1105, 3865, 13, 383, 39788, 873, 286, 4022, 799, 72, 290, 8445, 404, 78, 12379, 8014, 2879, 287, 262, 2471, 325, 286, 2986, 50191, 287, 11450, 5733, 422, 1105, 3459, 1906, 5824, 547, 13770, 15032, 287, 1248, 5705, 13, 383, 2471, 325, 47076, 286, 2986, 15088, 519, 29941, 318, 14183, 284, 36548, 305, 19931, 439, 5362, 11, 262, 6000, 7993, 34537, 286, 262, 1511, 400, 4289, 13, 9699, 8188, 422, 262, 1204, 286, 5335, 287, 8909, 14200, 287, 833, 4594, 4119, 547, 635, 10945, 416, 19931, 439, 5362, 287, 1105, 3829, 13, 2312, 39788, 873, 389, 15342, 329, 511, 12653, 33578, 290, 6370, 286, 6650, 13, 1318, 318, 281, 3499, 47076, 1117, 64, 23027, 422, 1105, 940, 2029, 262, 8946, 286, 262, 4928, 286, 2986, 309, 2002, 292, 78, 287, 5178, 271, 4478, 1951, 920, 71, 1313, 276, 1022, 257, 2330, 290, 257, 2042, 11778, 13, 383, 4928, 19611, 284, 262, 8284, 286, 262, 33822, 7940, 1547, 543, 373, 13378, 284, 374, 504, 3383, 4302, 13384, 13, 50257, 8496, 318, 262, 4928, 286, 8909, 1667, 544, 30, 50260, 2898, 4594, 4119], [7191, 262, 2278, 422, 4751, 1906, 14315, 340, 318, 6108, 326, 5125, 11, 22, 1828, 661, 1364, 262, 1181, 329, 262, 1578, 1829, 13, 2773, 9415, 11, 830, 661, 389, 1807, 284, 423, 2296, 38769, 284, 262, 1181, 422, 4751, 1906, 14315, 8384, 2406, 422, 4643, 330, 622, 89, 357, 1558, 13, 21, 15920, 1578, 1829, 357, 1433, 13, 17, 15920, 11164, 14208, 357, 1485, 13, 17, 15920, 1766, 12196, 10102, 357, 23, 13, 15, 4407, 290, 21380, 499, 292, 357, 19, 13, 20, 18823, 632, 318, 4762, 326, 612, 318, 257, 1588, 1271, 286, 21829, 7971, 287, 326, 1181, 262, 1282, 422, 5694, 290, 2520, 2253, 543, 8384, 12259, 287, 37685, 463, 324, 12585, 6557, 21107, 13, 4784, 284, 262, 5075, 21649, 11, 262, 3265, 6348, 352, 13, 3312, 4, 422, 4751, 284, 5075, 13, 383, 1181, 468, 281, 30690, 9443, 286, 661, 290, 262, 9016, 3265, 12109, 286, 597, 10816, 1181, 26, 1864, 284, 262, 5075, 21649, 612, 547, 1105, 661, 583, 10571, 17, 13, 3226, 477, 262, 513, 11, 28872, 11, 30272, 661, 287, 262, 1181, 11, 734, 12, 17936, 357, 17, 11, 2998, 17, 11, 18741, 8, 2107, 287, 262, 4736, 286, 37685, 463, 324, 12585, 6557, 21107, 290, 609, 48406, 993, 6413, 13, 5514, 1115, 584, 4736, 423, 9684, 625, 1802, 11, 830, 25, 2547, 1373, 8949, 11, 20198, 11, 14496, 559, 4352, 2634, 76, 420, 13343, 11, 45151, 11, 290, 4216, 291, 4448, 15495, 11, 23451, 13, 50257, 2437, 867, 661, 1364, 262, 1181, 329, 262, 471, 13, 50, 13, 1022, 4751, 12, 14315, 30, 50260, 2920, 11, 22, 1828], [34919, 32503, 389, 635, 8811, 973, 11, 3573, 287, 7064, 326, 257, 1097, 743, 407, 307, 1498, 284, 3151, 11, 284, 1630, 2785, 1171, 1502, 7445, 7411, 8292, 286, 5584, 15539, 1023, 290, 1690, 287, 27675, 10741, 810, 18757, 1644, 3790, 460, 2952, 1598, 257, 3108, 329, 30037, 5672, 13, 48444, 34141, 389, 973, 287, 617, 3006, 780, 484, 1249, 329, 517, 1280, 10375, 351, 262, 1171, 13, 554, 3090, 11, 511, 43880, 4905, 460, 15570, 13885, 11826, 555, 707, 3565, 290, 460, 1037, 287, 15461, 606, 9361, 284, 6654, 319, 2366, 13, 50257, 5195, 466, 617, 14073, 779, 35426, 284, 13969, 30, 50260, 9930, 1249, 329, 517, 1280, 10375, 351, 262, 1171], [464, 26464, 82, 286, 4492, 290, 262, 1578, 7526, 550, 14138, 287, 4150, 484, 4624, 2041, 3774, 290, 508, 547, 11987, 355, 262, 1182, 286, 262, 1230, 13, 21066, 547, 5658, 39131, 4053, 739, 8616, 25197, 26, 3977, 32234, 11, 4453, 5481, 456, 1636, 739, 10674, 314, 26, 22736, 358, 261, 739, 7516, 2873, 290, 1793, 27161, 739, 7542, 15397, 13, 2312, 14138, 2714, 257, 4996, 286, 8766, 6851, 11, 475, 547, 8811, 1900, 355, 366, 1169, 5342, 1600, 262, 366, 17351, 5342, 1600, 262, 366, 11085, 5342, 1, 290, 3443, 262, 366, 35505, 5342, 1911, 50257, 8241, 12030, 262, 2597, 326, 561, 1568, 1716, 6994, 5342, 739, 8616, 25197, 30, 50260, 22405, 39131, 4053], [5841, 1424, 1950, 326, 26461, 338, 14673, 22829, 318, 4097, 276, 416, 15114, 286, 15874, 33543, 6906, 319, 20334, 13, 383, 6727, 12, 5715, 15114, 6486, 379, 18895, 2174, 530, 2318, 11, 810, 262, 5951, 318, 11080, 329, 25006, 284, 1779, 1072, 13, 1114, 18895, 1022, 530, 290, 1936, 9210, 357, 3064, 290, 5323, 479, 28875, 828, 15114, 286, 44957, 290, 17669, 22443, 485, 389, 1807, 284, 1296, 13, 23302, 257, 3833, 286, 1936, 9210, 11, 262, 15114, 743, 3473, 286, 44957, 11, 34946, 1505, 22443, 485, 11, 17669, 22443, 485, 290, 1660, 13, 1024, 5723, 15114, 286, 1660, 4771, 815, 307, 1043, 379, 18895, 286, 546, 2026, 9210, 357, 20, 13, 15, 4904, 64, 828, 810, 262, 5951, 12229, 38549, 509, 357, 15, 22074, 34, 737, 4698, 13725, 11, 15114, 286, 44957, 290, 17669, 22443, 485, 743, 307, 1043, 13, 50257, 2061, 15114, 1296, 1022, 530, 290, 1936, 9210, 319, 26461, 30, 220, 50260, 321, 2144, 544, 290, 17669, 22443, 485], [464, 16064, 2938, 284, 16674, 281, 886, 284, 262, 1175, 475, 3393, 6149, 281, 5963, 319, 6342, 26, 416, 1315, 2693, 337, 5978, 365, 4884, 262, 6266, 329, 281, 4896, 286, 6342, 290, 319, 1160, 2693, 262, 2207, 1980, 1732, 373, 1844, 13, 347, 1042, 283, 694, 1138, 34395, 260, 319, 1248, 2693, 379, 262, 609, 22940, 660, 559, 390, 12880, 380, 14064, 411, 290, 12284, 257, 27580, 10900, 284, 257, 4141, 1175, 286, 15827, 11, 543, 3017, 47561, 24256, 11, 978, 82, 558, 290, 749, 262, 337, 577, 75, 293, 5011, 287, 26068, 23440, 286, 543, 3395, 89, 373, 262, 3139, 13, 554, 1441, 329, 281, 3211, 396, 501, 329, 262, 4141, 284, 1742, 257, 2351, 10006, 11, 347, 1042, 283, 694, 12284, 262, 16908, 286, 47561, 24256, 290, 262, 28757, 1748, 286, 309, 2852, 13, 1675, 1249, 9416, 656, 6342, 11, 530, 286, 262, 25317, 329, 912, 550, 284, 307, 10158, 625, 13, 34395, 260, 373, 17261, 326, 262, 1103, 4031, 286, 347, 1042, 283, 694, 287, 1642, 884, 38583, 378, 8665, 373, 284, 4474, 257, 20923, 4167, 319, 262, 649, 8830, 27580, 286, 4486, 11, 29203, 416, 257, 4167, 351, 257, 8030, 1230, 11, 319, 2846, 10909, 284, 4141, 1171, 4459, 13, 1052, 848, 28321, 540, 2422, 27580, 373, 281, 18536, 5559, 284, 683, 11, 38931, 691, 416, 262, 18614, 32442, 319, 262, 2679, 1735, 13, 50257, 33, 1042, 283, 694, 290, 34395, 260, 1138, 319, 644, 3128, 30, 50260, 1507, 2693]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1507, 1314, 50258], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19, 25, 405, 9114, 50258], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2898, 4594, 4119, 50258], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2920, 11, 22, 1828, 50258], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9930, 1249, 329, 517, 1280, 10375, 351, 262, 1171, 50258], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 22405, 39131, 4053, 50258], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 321, 2144, 544, 290, 17669, 22443, 485, 50258], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1507, 2693, 50258]]\n"
     ]
    }
   ],
   "source": [
    "samples = {k: v for k,v in tokenized_traindata[:8].items()}\n",
    "\n",
    "for i in ['input_ids', 'attention_mask', 'labels']:\n",
    "    print([x for x in samples[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 239]),\n",
       " 'attention_mask': torch.Size([8, 239]),\n",
       " 'labels': torch.Size([8, 239])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_valdata[i] for i in range(8)])\n",
    "batch\n",
    "{k: v.shape for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for training\n",
    "\n",
    "tokenized_traindata.set_format(\"torch\")\n",
    "tokenized_valdata.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_traindata, shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_valdata, batch_size=1, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([1, 153]),\n",
       " 'attention_mask': torch.Size([1, 153]),\n",
       " 'labels': torch.Size([1, 153])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 15 21:58:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   35C    P5    13W / 250W |   1049MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   33C    P8     9W / 250W |      2MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     18917      C   ...nvs/droid-slam/bin/python     1047MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59.5894, device='cuda:0', grad_fn=<NllLossBackward0>) torch.Size([1, 153, 50263])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch.to(device))\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "# outputs\n",
    "# model.transformer.wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50263, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "#     print(name)\n",
    "#     # break\n",
    "# # model.lm_head.requires_grad = True\n",
    "for p in model.lm_head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 120]) torch.cuda.LongTensor\n",
      "attention_mask torch.Size([1, 120]) torch.cuda.LongTensor\n",
      "labels torch.Size([1, 120]) torch.cuda.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    for k in batch.keys():\n",
    "        print(k, batch[k].size(), batch[k].type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from copy import copy\n",
    "\n",
    "class PrefixTunedGPT2(nn.Module):\n",
    "    def __init__(self, num_prompts, model, tokenizer, init_prompts=None):\n",
    "        super(PrefixTunedGPT2, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.soft_prompt_length = num_prompts\n",
    "        self.soft_prompt = nn.Embedding(num_prompts, 768)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # init\n",
    "        if init_prompts != None and len(init_prompts.split(' ')) == num_prompts:\n",
    "            idx = torch.tensor([[i[0]] for i in self.tokenizer(init_prompts.split(' '))['input_ids']]).int().flatten().to(self.device)\n",
    "            print(\"idx: \", idx)\n",
    "            self.soft_prompt.weights = self.model.transformer.wte(idx)\n",
    "\n",
    "        # freeze the internal model\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        # get all embs\n",
    "        batch_size = input_ids.shape[0]     # assuming batch first\n",
    "\n",
    "        # batch x N x emb_size\n",
    "        soft_prompt = self.soft_prompt(torch.arange(self.soft_prompt_length).to(self.device))\n",
    "        soft_prompt = torch.tile(soft_prompt.unsqueeze(0), (batch_size, 1, 1))\n",
    "\n",
    "        input_embs = self.model.transformer.wte(input_ids)\n",
    "        input_embs += self.model.transformer.wpe(torch.arange(input_ids.shape[-1]).to(self.device))\n",
    "\n",
    "        # k = copy(input_embs)\n",
    "\n",
    "        # apppend the soft prompt\n",
    "        input_embs = torch.cat((soft_prompt, input_embs), -2)\n",
    "        attention_mask = torch.cat((torch.ones((batch_size, self.soft_prompt_length)).int().to(self.device), attention_mask), -1)\n",
    "        labels = torch.cat(((\n",
    "            torch.ones((batch_size, self.soft_prompt_length)).long() * -100 \n",
    "                            ).to(self.device), input_ids), -1)\n",
    "        \n",
    "        # print(\"Shapes: \", input_embs.shape, attention_mask.shape, labels.shape, \" | \", input_ids.shape)\n",
    "        # print(\"nan check (should be false): \", input_embs.isnan().any(), attention_mask.isnan().any(), labels.isnan().any())\n",
    "        # print(\"attention mask:\", attention_mask, attention_mask.dtype)\n",
    "\n",
    "        outputs = self.model(input_ids=None, attention_mask=attention_mask, labels=labels, inputs_embeds=input_embs)\n",
    "        # return outputs, soft_prompt, input_embs, k\n",
    "        return outputs\n",
    "\n",
    "    def check_in(self):\n",
    "        k = (self.soft_prompt(torch.arange(self.soft_prompt_length).to(self.device))[:,:6])\n",
    "        return k\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  tensor([25652, 41484, 16794, 20274], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "ptgpt2 = PrefixTunedGPT2(4, model, tokenizer, \"question answer help result\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25652], [41484], [16794], [20274]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"question answer help result\".split(' '))['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs, soft, inp, k = ptgpt2(**batch)\n",
    "outputs = ptgpt2(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(98.1544, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=tensor(98.1544, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ -55.7074,  -56.3966,  -60.9171,  ...,    3.8213,   -3.3285,\n",
       "            -2.8394],\n",
       "         [ -69.5818,  -69.1536,  -72.6581,  ...,    4.3278,   -4.5861,\n",
       "            -3.2870],\n",
       "         [ -60.4988,  -58.0883,  -61.7677,  ...,    3.6855,   -4.0858,\n",
       "            -2.9560],\n",
       "         ...,\n",
       "         [-111.2705, -111.0775, -118.0889,  ...,    7.5938,   -5.4514,\n",
       "            -3.8237],\n",
       "         [ -59.1710,  -59.6123,  -64.5233,  ...,    4.1447,   -4.1973,\n",
       "            -2.6460],\n",
       "         [ -64.9427,  -66.9064,  -70.5449,  ...,    4.2045,   -4.5302,\n",
       "            -2.3400]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-1.4171, -0.9430,  1.1613,  ...,  1.7712, -0.5406,  0.7919],\n",
       "          [ 1.1553, -1.0167, -0.6283,  ..., -3.3433, -1.0641, -0.9319],\n",
       "          [ 2.3112, -1.4542, -1.0587,  ...,  2.8316, -1.0940, -1.2160],\n",
       "          ...,\n",
       "          [-1.4621, -0.4608,  1.1028,  ..., -0.6866, -0.0088,  1.5752],\n",
       "          [-1.5323, -0.0294,  0.9935,  ..., -0.7382, -0.3066,  1.1978],\n",
       "          [-2.2142, -0.0934,  1.1414,  ..., -0.9450, -0.8686,  0.9861]],\n",
       "\n",
       "         [[ 1.4353,  0.3988, -1.2650,  ...,  0.7222,  0.5112,  3.3592],\n",
       "          [ 1.0570,  0.6842, -0.0892,  ..., -0.5940,  1.4396, -1.4253],\n",
       "          [-0.6229,  1.8825, -0.0044,  ...,  2.4292, -0.5691, -0.8585],\n",
       "          ...,\n",
       "          [ 0.3035,  0.3426, -1.4245,  ..., -0.7027,  0.5746, -0.0937],\n",
       "          [ 0.5846,  0.9685, -0.5719,  ..., -0.0359,  1.3758,  0.6409],\n",
       "          [ 0.3778,  1.2396,  0.0665,  ...,  0.7460,  2.5232, -0.0303]],\n",
       "\n",
       "         [[-1.9357, -0.4356,  0.4767,  ..., -1.1866, -0.5778,  1.2505],\n",
       "          [ 0.8443, -1.6720,  1.4732,  ...,  2.1186, -0.5011, -1.2650],\n",
       "          [ 0.5212, -1.9930,  0.6488,  ..., -0.9382, -2.5546,  1.2178],\n",
       "          ...,\n",
       "          [ 1.2003, -0.3597,  0.1831,  ..., -0.5548,  1.2445,  1.7991],\n",
       "          [ 1.4692,  0.6301,  0.6891,  ..., -1.1388,  0.6138,  1.0242],\n",
       "          [ 1.1967,  0.3407,  0.3147,  ..., -1.4743,  1.1243,  1.0609]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3538,  0.3845, -1.1829,  ...,  0.3519,  1.0593, -0.2446],\n",
       "          [-0.0488,  0.2693,  0.3894,  ..., -0.8704, -1.1194,  0.5071],\n",
       "          [-1.0025, -0.4785, -1.8811,  ..., -0.5243,  1.4436, -0.6508],\n",
       "          ...,\n",
       "          [-0.2647, -0.0316, -0.1374,  ..., -0.2766, -0.1870,  0.0402],\n",
       "          [ 0.5726,  0.0614, -0.7121,  ...,  0.2460, -0.6255,  0.2790],\n",
       "          [-0.2066,  0.2057,  0.1933,  ...,  0.4370, -0.8745,  0.1753]],\n",
       "\n",
       "         [[ 0.3953,  0.2936, -0.9684,  ..., -0.6242,  0.3506,  0.1334],\n",
       "          [ 0.6477, -0.0704,  0.8741,  ...,  0.2120, -1.5587,  0.1102],\n",
       "          [ 0.3205,  0.1012, -0.3876,  ...,  1.0808, -0.2397, -1.2287],\n",
       "          ...,\n",
       "          [-0.3955,  0.2916, -0.1203,  ..., -0.1490, -0.3655, -0.4335],\n",
       "          [-0.3640, -0.3351, -0.0298,  ..., -0.7206,  0.0945, -0.4143],\n",
       "          [-0.3047,  0.2491,  0.0773,  ..., -0.1076, -0.0890, -0.0784]],\n",
       "\n",
       "         [[ 0.8045, -0.2046,  0.6931,  ...,  0.1830, -0.3726, -0.0400],\n",
       "          [-0.2443,  0.8591,  1.2091,  ..., -1.3602, -0.7366,  0.2485],\n",
       "          [-0.6826,  0.0801, -0.2164,  ...,  0.5063,  0.2744, -0.8745],\n",
       "          ...,\n",
       "          [-0.4364, -0.2349,  0.1569,  ...,  0.2237,  0.7239, -0.3713],\n",
       "          [ 0.3062,  0.5030, -0.3467,  ...,  0.4862,  0.1900,  0.0278],\n",
       "          [ 0.1745, -0.7526, -0.2339,  ...,  0.2969,  0.4091, -0.1570]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.1694,  0.2533,  0.2758,  ..., -0.1815,  0.3004,  0.3336],\n",
       "          [ 0.0760, -0.4096,  0.0217,  ...,  0.1419,  0.6828, -0.0180],\n",
       "          [-0.3938,  0.0261, -0.6575,  ...,  0.5460,  0.3653,  0.1360],\n",
       "          ...,\n",
       "          [-0.1331, -0.0177, -0.1307,  ...,  0.0389,  0.0465,  0.0645],\n",
       "          [-0.0262, -0.1181,  0.0848,  ...,  0.0394,  0.1195, -0.1151],\n",
       "          [-0.1383,  0.0041, -0.0700,  ...,  0.0708,  0.0657, -0.0566]],\n",
       "\n",
       "         [[ 0.4136, -0.1705, -0.2905,  ..., -0.4677, -0.4118,  0.1711],\n",
       "          [ 0.4577, -0.1354, -0.0168,  ..., -0.4338, -0.0223, -0.3244],\n",
       "          [ 0.1420, -0.1278, -0.1098,  ..., -0.2301,  0.6827,  0.0154],\n",
       "          ...,\n",
       "          [ 1.3892, -0.0069, -0.1262,  ...,  0.0150,  0.3489, -0.2879],\n",
       "          [ 1.2284, -0.1212,  0.0721,  ..., -0.1107,  0.2908,  0.1965],\n",
       "          [ 1.1224, -0.1383,  0.0646,  ...,  0.1066,  0.1198,  0.0825]],\n",
       "\n",
       "         [[-0.0466, -0.0671, -0.0092,  ..., -0.1150, -0.1515,  0.3653],\n",
       "          [-0.0469, -0.1354, -0.3229,  ..., -0.4371,  0.0760,  0.3399],\n",
       "          [ 0.2065,  0.2650,  0.1483,  ...,  0.1816, -0.2206, -0.4491],\n",
       "          ...,\n",
       "          [-0.1584,  0.1551, -0.5915,  ..., -0.0041, -0.0932, -0.1486],\n",
       "          [ 0.1731, -0.2309, -0.2923,  ..., -0.0499,  0.1081, -0.1578],\n",
       "          [ 0.0289, -0.0983, -0.4104,  ..., -0.1034,  0.1002, -0.0292]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0850,  0.2242,  0.3997,  ...,  0.4627,  0.1219,  0.4477],\n",
       "          [ 0.2357,  0.3257, -0.1450,  ...,  0.3856,  0.2664, -0.1828],\n",
       "          [ 0.6955,  0.0286, -0.1868,  ...,  0.0048,  0.0094,  0.5009],\n",
       "          ...,\n",
       "          [ 0.0090,  0.0796, -0.1165,  ..., -0.0831, -0.1040, -0.0325],\n",
       "          [-0.0123,  0.2708,  0.0649,  ...,  0.0645, -0.2111,  0.0343],\n",
       "          [-0.1785,  0.4582, -0.1009,  ..., -0.0196,  0.1385, -0.0564]],\n",
       "\n",
       "         [[ 0.2089,  0.4604, -0.3910,  ..., -0.7392, -0.1937, -0.3358],\n",
       "          [-0.1109,  0.1956, -0.2203,  ...,  0.2096, -0.3045,  0.4442],\n",
       "          [ 0.2605,  0.5093,  0.4023,  ...,  0.3364, -0.1507,  0.0948],\n",
       "          ...,\n",
       "          [ 0.0895,  0.1862, -0.0179,  ...,  0.1544,  0.0042, -0.1806],\n",
       "          [-0.1099, -0.0437,  0.1033,  ..., -0.1045, -0.0275,  0.0519],\n",
       "          [-0.1337,  0.0469, -0.0281,  ..., -0.0719,  0.0289,  0.0257]],\n",
       "\n",
       "         [[ 0.2796, -0.4484, -0.0319,  ..., -0.3489, -0.1554, -0.2778],\n",
       "          [-0.0160,  0.1220, -0.2536,  ..., -0.0862, -0.6329, -0.0301],\n",
       "          [ 0.0766,  0.0136, -0.2804,  ...,  0.5719,  0.5926, -0.2732],\n",
       "          ...,\n",
       "          [-0.0229,  0.0158,  0.0756,  ...,  0.0033, -0.1079,  0.0038],\n",
       "          [ 0.2070,  0.2299, -0.0755,  ..., -0.2316, -0.4628,  0.0918],\n",
       "          [ 0.0253,  0.1975, -0.0755,  ...,  0.0998, -0.2530,  0.1181]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[-5.5500e-01,  3.3582e-01, -2.7307e+00,  ...,  3.7707e-01,\n",
       "            7.1943e-01, -4.7150e-03],\n",
       "          [ 5.6437e-01, -1.1086e+00, -2.7265e+00,  ..., -9.3004e-01,\n",
       "           -4.9791e-01, -8.4302e-01],\n",
       "          [-5.5808e-01, -4.4957e-01, -3.4889e+00,  ..., -1.8854e-01,\n",
       "           -9.5140e-01, -3.7100e-02],\n",
       "          ...,\n",
       "          [-1.4745e+00, -5.5017e-01,  2.4716e+00,  ...,  4.7123e+00,\n",
       "           -1.4209e+00, -2.0095e+00],\n",
       "          [-1.3707e+00, -2.0171e+00,  2.7021e+00,  ...,  5.0015e+00,\n",
       "           -6.9935e-01, -1.2880e+00],\n",
       "          [-1.7059e+00, -2.3292e+00,  2.1658e+00,  ...,  5.2868e+00,\n",
       "           -2.0658e+00, -2.9677e-01]],\n",
       "\n",
       "         [[ 4.2243e-02, -1.4100e+00, -5.0917e-01,  ...,  1.9124e-01,\n",
       "            1.9996e+00, -8.0237e-01],\n",
       "          [-2.7336e-01, -1.5084e+00,  1.3330e-01,  ...,  5.6680e-01,\n",
       "            1.9039e+00, -3.4042e-01],\n",
       "          [-6.5799e-01, -1.2540e+00, -4.7101e-01,  ..., -7.1217e-01,\n",
       "            1.8665e+00, -4.0645e-01],\n",
       "          ...,\n",
       "          [ 2.3049e+00,  1.7189e+00,  9.5210e-01,  ...,  1.5405e+00,\n",
       "           -4.2942e+00, -6.4519e-01],\n",
       "          [ 1.3819e+00,  7.5144e-01,  1.3764e-01,  ...,  1.3288e+00,\n",
       "           -4.4129e+00, -1.2033e+00],\n",
       "          [ 9.1899e-01,  1.7746e+00,  5.3584e-02,  ...,  2.5107e+00,\n",
       "           -4.6774e+00, -5.8934e-01]],\n",
       "\n",
       "         [[ 2.0799e-01,  2.1969e-01, -8.0877e-02,  ..., -1.6422e+00,\n",
       "            9.0158e-01,  5.2624e-01],\n",
       "          [ 5.4114e-02,  2.7178e-02, -2.6222e-01,  ..., -1.2998e+00,\n",
       "            4.8449e-01,  8.8525e-01],\n",
       "          [ 2.2264e-01,  2.8855e-01, -3.9443e-01,  ..., -1.6685e+00,\n",
       "            4.7533e-01,  5.3881e-01],\n",
       "          ...,\n",
       "          [ 2.4947e-01, -4.3364e-02, -3.3995e-02,  ...,  1.8155e+00,\n",
       "           -1.7781e-01, -1.2996e-01],\n",
       "          [ 3.5824e-01,  6.2545e-02, -1.2272e-01,  ...,  2.0204e+00,\n",
       "           -4.9354e-01,  2.3802e-01],\n",
       "          [ 9.1278e-02, -3.6753e-02,  2.1417e-02,  ...,  1.8524e+00,\n",
       "           -4.7468e-01, -1.2109e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.3076e+00,  9.9685e-01, -1.6708e-01,  ..., -2.6164e-01,\n",
       "           -1.2145e-01,  5.1492e-01],\n",
       "          [-2.8813e+00,  2.2962e-01,  1.0835e+00,  ...,  3.7486e-01,\n",
       "            2.8032e-01,  2.0119e+00],\n",
       "          [-4.2140e-01, -1.3547e+00,  3.5857e-01,  ...,  3.2336e-01,\n",
       "            9.8848e-02, -3.3875e-01],\n",
       "          ...,\n",
       "          [ 1.7548e+00, -1.2178e+00,  1.1382e+00,  ...,  3.0038e-01,\n",
       "            3.7078e-01, -1.6676e-01],\n",
       "          [ 1.1526e+00, -1.2685e+00,  1.3779e+00,  ...,  2.2228e+00,\n",
       "            2.9217e+00, -8.9005e-01],\n",
       "          [ 1.8298e+00, -1.5728e+00,  1.0972e+00,  ...,  2.4466e+00,\n",
       "            1.6541e+00, -1.1267e+00]],\n",
       "\n",
       "         [[ 1.0800e+00, -1.6207e-01,  5.2406e-01,  ...,  8.3884e-01,\n",
       "           -6.1931e-01, -8.6532e-04],\n",
       "          [ 1.4534e+00,  4.8398e-01,  5.1192e-01,  ..., -2.2089e-01,\n",
       "           -9.7658e-01,  3.6190e-01],\n",
       "          [ 1.4425e+00,  1.9456e-01,  4.5043e-01,  ..., -2.6480e-01,\n",
       "           -7.9759e-01,  4.2611e-01],\n",
       "          ...,\n",
       "          [-1.3667e+00,  3.1951e-01, -2.4217e+00,  ..., -3.6160e-01,\n",
       "            1.1369e-01,  1.0895e-01],\n",
       "          [-1.5624e+00,  3.5791e-01, -2.4889e+00,  ..., -9.8699e-02,\n",
       "           -5.5351e-03,  1.6545e-01],\n",
       "          [-1.6507e+00,  5.9929e-01, -2.4754e+00,  ..., -1.5240e-02,\n",
       "           -2.7001e-02,  2.7152e-01]],\n",
       "\n",
       "         [[-4.6753e-01,  1.0587e+00, -1.8488e-01,  ...,  3.1748e+00,\n",
       "            9.6379e-02, -6.3027e-01],\n",
       "          [-3.1458e-01, -5.3927e-01,  7.0743e-02,  ...,  4.8622e+00,\n",
       "            5.8711e-01, -6.6376e-01],\n",
       "          [-1.7189e+00, -9.0302e-01, -1.5363e+00,  ...,  5.3225e+00,\n",
       "            3.8935e-01, -1.2641e+00],\n",
       "          ...,\n",
       "          [ 6.6019e-01,  1.3375e+00,  4.6870e-01,  ...,  4.2658e-01,\n",
       "            2.7540e-03, -1.8257e-01],\n",
       "          [ 2.4959e-01, -1.3575e-01,  2.7384e+00,  ...,  1.7112e+00,\n",
       "            1.9204e+00,  1.6534e-01],\n",
       "          [-3.1854e-02,  7.2962e-01,  1.6669e+00,  ...,  1.2678e+00,\n",
       "            4.9053e-01,  7.8333e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.2768,  0.0303,  0.2192,  ..., -0.0230,  0.2690,  0.5067],\n",
       "          [ 0.2988,  0.2352,  0.1078,  ..., -0.1000,  0.4985,  0.7162],\n",
       "          [ 0.1405,  0.0956,  0.4304,  ..., -0.4776,  0.1327, -0.2789],\n",
       "          ...,\n",
       "          [ 0.1226,  0.1940,  0.2192,  ...,  0.0370, -0.1990,  0.0295],\n",
       "          [-0.8274, -0.3322, -0.1386,  ..., -0.2399, -0.1657, -0.1560],\n",
       "          [ 0.1176, -0.1943,  0.0331,  ..., -0.6281, -0.3531, -0.7311]],\n",
       "\n",
       "         [[-0.5288, -0.4411, -0.0247,  ...,  0.3740, -0.8752, -0.4084],\n",
       "          [-1.0504, -0.7897, -0.0856,  ...,  0.1697, -0.6015, -0.4870],\n",
       "          [-0.7046, -1.6324,  0.0772,  ..., -0.0290, -1.0770, -0.1749],\n",
       "          ...,\n",
       "          [ 0.2983,  0.7391,  0.0788,  ...,  0.0319,  0.0805,  0.0068],\n",
       "          [ 0.1989,  0.7448,  0.3262,  ...,  0.0476,  0.0341,  0.4733],\n",
       "          [ 0.1490,  0.5187,  0.3189,  ..., -0.2089, -0.0158, -0.1072]],\n",
       "\n",
       "         [[-0.8752, -0.6155,  0.3440,  ..., -0.2486,  0.4554,  0.6903],\n",
       "          [-1.0788,  0.1204,  0.6357,  ...,  0.0861,  0.5506,  1.3790],\n",
       "          [-1.4764,  0.1867, -0.2676,  ...,  0.2478,  0.2656,  0.7791],\n",
       "          ...,\n",
       "          [-0.4111, -0.1397, -0.1917,  ..., -1.5080, -0.1517,  0.1146],\n",
       "          [-0.5030,  0.7345, -0.2249,  ..., -1.6492, -0.4951,  0.6471],\n",
       "          [-0.3364, -0.0884,  0.0874,  ..., -1.5302, -0.2165,  0.3954]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0611,  1.5223,  0.1269,  ...,  1.1315, -1.4555, -0.8194],\n",
       "          [-0.3739,  1.8767,  0.5354,  ...,  0.2880, -1.5397, -0.7103],\n",
       "          [ 0.4313,  2.1749,  0.1689,  ..., -0.0726, -1.0782, -1.8830],\n",
       "          ...,\n",
       "          [-0.0888, -0.1736,  0.0363,  ...,  0.4257,  1.4184,  0.2517],\n",
       "          [ 0.2421,  0.3804,  0.0757,  ...,  0.7169,  1.7638, -0.0174],\n",
       "          [ 0.3186,  0.6703, -0.1126,  ...,  0.7691,  1.5497,  0.7074]],\n",
       "\n",
       "         [[ 0.2831,  0.2226, -0.3222,  ...,  0.9171, -1.6464,  0.1516],\n",
       "          [-0.5164,  0.1185, -0.4557,  ...,  0.5337, -0.2158, -0.2531],\n",
       "          [ 0.2595,  0.4973, -0.4947,  ...,  0.8326, -0.1500, -0.4114],\n",
       "          ...,\n",
       "          [ 0.0454,  0.0286,  0.1382,  ...,  0.1613,  0.3160, -0.0978],\n",
       "          [ 0.0931,  0.2298, -0.0753,  ...,  0.2111,  0.2291,  0.3750],\n",
       "          [ 0.2515, -0.1829,  0.0149,  ...,  0.1171,  0.2868,  0.2320]],\n",
       "\n",
       "         [[-0.2682, -0.7001, -0.2075,  ...,  0.1635,  0.1027, -0.4401],\n",
       "          [-0.6359, -1.0738, -0.5067,  ...,  0.1295,  0.1670, -0.0908],\n",
       "          [-0.6591, -1.0562, -0.3602,  ...,  0.5446,  0.4843, -0.3654],\n",
       "          ...,\n",
       "          [-0.0662,  0.0890,  0.3533,  ..., -0.2664,  0.1082,  0.0133],\n",
       "          [-0.0091, -0.1008,  0.1557,  ..., -0.3406,  0.1307,  0.0761],\n",
       "          [ 0.0030,  0.1433,  0.0629,  ..., -0.4111,  0.1359,  0.1765]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[-4.2053e-01, -1.2091e+00,  5.0301e-01,  ..., -8.2852e-01,\n",
       "            1.9510e-01,  1.5227e-01],\n",
       "          [-5.1891e-01, -1.1468e+00,  3.7768e-01,  ..., -9.1681e-01,\n",
       "           -3.4400e-01,  1.8427e-01],\n",
       "          [-8.9802e-02, -1.1193e+00,  1.0061e+00,  ..., -2.9748e-01,\n",
       "           -2.5061e-01,  2.6343e-01],\n",
       "          ...,\n",
       "          [-7.0588e-01,  2.1928e+00, -3.5063e-01,  ...,  4.9566e+00,\n",
       "            2.2047e-01,  1.1848e-01],\n",
       "          [-3.8536e-01,  8.9975e-01, -3.2671e-01,  ...,  4.4852e+00,\n",
       "            1.6966e+00,  1.7924e-01],\n",
       "          [-1.6108e-01,  7.6828e-01,  1.2608e-01,  ...,  3.4277e+00,\n",
       "            1.3204e+00, -5.3497e-01]],\n",
       "\n",
       "         [[-4.9395e-02, -2.3746e-01, -2.4756e-02,  ...,  3.6960e-01,\n",
       "           -6.4779e-01, -6.9337e-01],\n",
       "          [-8.0460e-01, -8.4902e-02,  3.2520e-01,  ...,  3.9463e-01,\n",
       "           -6.6794e-01, -4.8293e-01],\n",
       "          [-1.2149e+00, -1.1654e-01,  1.2091e+00,  ...,  6.4175e-01,\n",
       "           -7.4005e-02, -4.2920e-01],\n",
       "          ...,\n",
       "          [-1.1703e-01, -1.8380e+00,  5.0230e-01,  ...,  3.8902e-01,\n",
       "            1.6697e+00,  4.4426e-01],\n",
       "          [ 1.5815e-01, -7.6155e-01,  1.5465e-01,  ...,  6.1215e-02,\n",
       "            1.2610e+00,  1.2733e+00],\n",
       "          [ 3.4933e-01, -5.9830e-01, -6.7100e-03,  ...,  1.1348e-01,\n",
       "            1.0314e+00,  7.5149e-01]],\n",
       "\n",
       "         [[ 1.5307e+00,  2.4742e+00,  3.9217e+00,  ...,  1.1001e+00,\n",
       "            2.0336e+00, -4.1039e-01],\n",
       "          [ 1.4838e+00,  2.9424e+00,  3.6666e+00,  ...,  9.8454e-01,\n",
       "            1.8279e+00, -2.6868e-01],\n",
       "          [ 1.9242e+00,  2.1871e+00,  3.8573e+00,  ...,  1.2827e+00,\n",
       "            2.9542e+00, -2.6602e-01],\n",
       "          ...,\n",
       "          [-2.4479e+00, -7.9474e+00, -9.1869e+00,  ..., -4.7112e+00,\n",
       "            2.9075e+00, -4.3150e+00],\n",
       "          [-1.2874e+00, -7.6092e+00, -8.7157e+00,  ..., -4.5528e+00,\n",
       "            3.1231e+00, -4.7573e+00],\n",
       "          [-9.8495e-01, -7.7360e+00, -8.1351e+00,  ..., -4.4526e+00,\n",
       "            2.2531e+00, -4.4851e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.7920e+00, -2.6821e+00, -2.9405e+00,  ...,  4.8464e-01,\n",
       "            4.1046e-01,  2.6398e+00],\n",
       "          [ 2.3168e+00, -2.5378e+00, -2.3222e+00,  ...,  6.5007e-02,\n",
       "            5.6009e-01,  2.5799e+00],\n",
       "          [ 2.0070e+00, -2.6890e+00, -2.6232e+00,  ...,  4.7252e-01,\n",
       "            2.4538e-01,  2.6062e+00],\n",
       "          ...,\n",
       "          [-1.1167e+00,  1.1405e+01,  5.0283e+00,  ..., -3.6824e+00,\n",
       "            2.7342e+00, -2.0728e+00],\n",
       "          [-8.0366e-01,  9.9071e+00,  5.5888e+00,  ..., -4.0674e+00,\n",
       "            1.7460e+00, -2.0711e+00],\n",
       "          [-5.2584e-01,  9.6527e+00,  6.1459e+00,  ..., -4.4928e+00,\n",
       "            1.8740e+00, -1.5395e+00]],\n",
       "\n",
       "         [[ 9.6467e-01,  3.3453e-01,  9.5952e-01,  ..., -1.3558e-01,\n",
       "           -8.0265e-01, -2.4189e-01],\n",
       "          [ 8.6831e-01,  1.8497e-01,  8.4571e-01,  ..., -5.1828e-02,\n",
       "           -6.5391e-01, -1.4483e-01],\n",
       "          [ 6.5178e-01,  8.4325e-02,  7.2853e-01,  ..., -3.0114e-01,\n",
       "           -5.2887e-01, -1.2299e-01],\n",
       "          ...,\n",
       "          [-3.4148e+00, -8.2138e-02,  6.5849e-02,  ...,  4.9233e-01,\n",
       "            1.5259e+00,  4.2619e-02],\n",
       "          [-2.8873e+00, -3.1921e-01,  4.6278e-01,  ...,  2.8097e-01,\n",
       "            2.4357e+00, -1.6633e-02],\n",
       "          [-2.8355e+00, -5.8335e-01,  5.3706e-01,  ...,  1.8218e-01,\n",
       "            2.3545e+00,  2.1869e-01]],\n",
       "\n",
       "         [[-2.9702e-01,  1.4685e-01, -1.0174e+00,  ...,  4.5932e-01,\n",
       "            5.1396e-01,  5.6694e-01],\n",
       "          [ 1.4868e-01,  1.0431e-01, -1.0670e+00,  ...,  4.2914e-01,\n",
       "            6.1704e-01,  7.7948e-01],\n",
       "          [-1.8923e-01,  4.6287e-01, -1.3185e+00,  ...,  9.1732e-01,\n",
       "            4.8511e-01,  5.5748e-01],\n",
       "          ...,\n",
       "          [ 9.4577e-01, -5.3973e-01, -2.1457e-01,  ...,  3.8213e-01,\n",
       "            2.6487e-01,  1.2080e+00],\n",
       "          [ 1.9109e-01, -1.6047e-01, -9.3948e-01,  ...,  3.0989e-01,\n",
       "            6.8943e-02,  6.3899e-01],\n",
       "          [ 4.8702e-01, -3.8979e-03,  2.1923e-01,  ...,  1.2258e-02,\n",
       "            1.5711e-01,  3.6195e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-9.7055e-02, -1.3235e-01, -6.8234e-02,  ...,  9.3509e-02,\n",
       "            3.9768e-02, -3.4629e-01],\n",
       "          [-8.5991e-02, -3.3197e-01,  3.4520e-01,  ...,  3.9119e-01,\n",
       "           -7.6830e-02, -4.4656e-01],\n",
       "          [-1.9293e-01, -3.3489e-01, -9.7141e-02,  ...,  2.2815e-01,\n",
       "           -2.7002e-01, -3.3246e-01],\n",
       "          ...,\n",
       "          [ 2.1727e-01,  3.3920e-01, -2.1611e-01,  ..., -3.1039e-01,\n",
       "           -9.0997e-02,  1.5264e-02],\n",
       "          [ 5.0327e-01, -6.8262e-01,  3.1905e-01,  ..., -4.8618e-01,\n",
       "           -7.7438e-02, -1.7991e-01],\n",
       "          [ 3.3878e-01, -6.9082e-01,  7.3293e-01,  ..., -2.4867e-01,\n",
       "           -3.0635e-01,  2.9368e-02]],\n",
       "\n",
       "         [[-1.8308e-01, -2.8437e-01,  7.4560e-01,  ...,  4.9692e-01,\n",
       "           -4.1874e-01, -7.4050e-01],\n",
       "          [-1.0901e-01, -3.1258e-02,  1.1840e+00,  ...,  3.9368e-01,\n",
       "           -3.6749e-01, -7.2960e-01],\n",
       "          [ 7.6787e-01, -2.9108e-01,  6.1723e-01,  ..., -1.5820e-01,\n",
       "           -4.5736e-01, -8.4782e-01],\n",
       "          ...,\n",
       "          [ 2.7429e-01,  3.7996e-01,  3.6098e-01,  ..., -1.2848e-01,\n",
       "            1.5784e-01,  1.7202e-01],\n",
       "          [ 1.3288e-01,  6.5896e-01,  1.9487e-01,  ...,  2.5124e-01,\n",
       "           -5.1803e-01,  4.8296e-01],\n",
       "          [ 3.6585e-01,  1.9155e-01,  2.7496e-01,  ...,  1.2847e-01,\n",
       "           -4.9492e-01,  1.3259e-01]],\n",
       "\n",
       "         [[ 3.4233e-02, -1.3109e+00, -1.4954e-03,  ..., -2.4795e-02,\n",
       "           -1.6893e-01, -1.5212e-01],\n",
       "          [ 1.7171e-01, -1.6227e+00, -2.6546e-01,  ...,  1.4637e-01,\n",
       "            4.3335e-02,  1.0376e-01],\n",
       "          [ 1.0487e-01, -1.6344e+00,  2.9891e-02,  ..., -1.8953e-02,\n",
       "           -2.3579e-01, -5.3038e-01],\n",
       "          ...,\n",
       "          [-1.9789e-01, -8.1028e-01, -3.2367e-02,  ...,  1.7470e-01,\n",
       "           -3.3698e-01,  2.2661e-01],\n",
       "          [-4.3468e-01, -1.4743e+00,  2.1224e-01,  ...,  1.5529e-01,\n",
       "            3.4744e-01,  1.7664e-01],\n",
       "          [-4.8647e-01, -1.6370e+00, -6.6531e-01,  ...,  2.3679e-01,\n",
       "            3.3239e-01, -5.0530e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0183e-01, -2.1826e-01,  1.6987e+00,  ..., -2.3757e-01,\n",
       "            2.4799e-01,  5.6359e-02],\n",
       "          [-2.1787e-02, -1.0598e-01,  1.7402e+00,  ..., -1.6592e-01,\n",
       "            5.3025e-01, -1.3602e-01],\n",
       "          [-1.2080e-01, -5.0090e-03,  1.6762e+00,  ..., -1.3148e-01,\n",
       "            2.3444e-01, -1.6835e-01],\n",
       "          ...,\n",
       "          [-6.5866e-02,  5.3509e-02,  2.6133e+00,  ...,  4.0058e-02,\n",
       "           -1.0460e-01,  2.8014e-01],\n",
       "          [ 4.3492e-01,  2.1861e-01,  1.2713e+00,  ...,  4.5658e-01,\n",
       "            3.3130e-01,  4.3291e-01],\n",
       "          [ 5.6076e-01, -1.4464e-01,  1.3987e+00,  ...,  1.3031e-03,\n",
       "           -1.5400e-01, -2.6535e-01]],\n",
       "\n",
       "         [[-8.7169e-03,  6.2624e-02, -1.2582e-01,  ...,  1.7768e-01,\n",
       "            1.2997e-01,  1.6432e-01],\n",
       "          [ 1.6858e-02,  1.9897e-01,  2.2750e-02,  ...,  6.3943e-01,\n",
       "            1.5914e-01,  1.5981e-01],\n",
       "          [ 2.3953e-01,  2.3182e-01,  2.9489e-01,  ...,  6.8879e-01,\n",
       "            1.3337e-01,  1.0243e-01],\n",
       "          ...,\n",
       "          [-2.3334e-01,  6.7752e-01, -3.9472e-02,  ..., -1.5648e-02,\n",
       "           -7.1337e-01, -5.7560e-02],\n",
       "          [-1.9405e-01,  4.4736e-01, -7.9713e-01,  ...,  5.8017e-02,\n",
       "           -9.7175e-02,  9.2178e-03],\n",
       "          [ 1.9818e-02,  6.2059e-01, -9.1094e-01,  ..., -2.6873e-01,\n",
       "            1.4219e-01, -4.5700e-01]],\n",
       "\n",
       "         [[ 3.7254e-01, -1.3741e-01, -1.5746e-01,  ..., -3.6999e-03,\n",
       "            1.7686e-01,  1.4373e-01],\n",
       "          [ 3.1589e-01,  4.5537e-02, -2.6303e-02,  ...,  7.6253e-02,\n",
       "            2.0835e-01,  4.1481e-01],\n",
       "          [ 2.7981e-01, -4.2845e-02, -3.8411e-01,  ...,  1.4824e-02,\n",
       "            2.5569e-01,  5.6458e-01],\n",
       "          ...,\n",
       "          [ 1.7775e-01, -2.6147e-01,  1.4609e-01,  ...,  1.2372e-01,\n",
       "            7.4525e-01,  5.0334e-02],\n",
       "          [ 2.9883e-02, -3.7632e-01, -3.3527e-01,  ..., -3.4070e-01,\n",
       "            4.9825e-01, -9.6389e-02],\n",
       "          [ 9.1128e-02,  1.8215e-02, -3.9844e-01,  ..., -1.7464e-01,\n",
       "            5.5053e-01, -1.9238e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.7018e-02, -8.3089e-02,  1.5470e-01,  ..., -7.2406e-01,\n",
       "            6.7786e-01, -1.0315e+00],\n",
       "          [ 1.9827e-01, -1.7518e-01,  1.9284e-01,  ..., -7.7024e-01,\n",
       "            5.0280e-01, -1.1789e+00],\n",
       "          [ 1.3946e-01, -2.1897e-02,  1.0860e-01,  ..., -7.7013e-01,\n",
       "            6.8041e-01, -1.0173e+00],\n",
       "          ...,\n",
       "          [ 7.9389e-01,  6.0496e-01,  6.5199e-01,  ...,  1.9741e+00,\n",
       "           -2.8487e-02,  9.8457e-01],\n",
       "          [ 9.0060e-01,  2.2908e+00, -5.7660e-01,  ...,  7.9860e-01,\n",
       "            3.2699e-01,  3.5404e+00],\n",
       "          [ 1.1184e+00, -1.3687e-01,  1.7171e+00,  ...,  1.6000e+00,\n",
       "           -3.5903e-01,  1.8426e+00]],\n",
       "\n",
       "         [[ 8.2132e-01,  3.4592e-01,  4.0102e-02,  ..., -1.6714e-01,\n",
       "           -1.1361e+00, -2.7197e-01],\n",
       "          [ 8.1584e-01,  2.5803e-01,  2.7320e-02,  ..., -1.7584e-01,\n",
       "           -1.1535e+00, -3.1959e-01],\n",
       "          [ 9.0115e-01,  2.8959e-01, -3.6699e-03,  ..., -8.7454e-02,\n",
       "           -1.1150e+00, -3.8113e-01],\n",
       "          ...,\n",
       "          [ 1.9051e-03, -4.6259e+00, -1.9419e-01,  ...,  1.1987e+00,\n",
       "            3.4567e+00,  1.9133e+00],\n",
       "          [ 9.1595e-01, -3.3085e+00, -2.2341e-01,  ...,  1.3229e+00,\n",
       "            3.7565e+00,  1.7863e+00],\n",
       "          [ 1.8628e+00, -4.0598e+00, -1.3089e+00,  ...,  2.0946e+00,\n",
       "            3.6646e+00,  1.9574e+00]],\n",
       "\n",
       "         [[ 1.2761e-01, -1.9146e-01, -4.3307e-01,  ...,  4.0853e-01,\n",
       "            1.3994e+00,  3.7057e-01],\n",
       "          [ 1.6110e-01, -2.3253e-01, -3.9474e-01,  ...,  4.3418e-01,\n",
       "            1.3629e+00,  3.3835e-01],\n",
       "          [ 8.3312e-02, -1.2948e-01, -5.1691e-01,  ...,  2.7077e-01,\n",
       "            1.4174e+00,  4.8045e-01],\n",
       "          ...,\n",
       "          [ 7.2453e+00, -1.6603e+00, -2.7010e-01,  ..., -1.0208e+00,\n",
       "           -1.3674e+00, -4.2233e+00],\n",
       "          [ 7.7290e+00, -1.4982e+00, -5.2053e-01,  ..., -5.4765e-01,\n",
       "           -2.3489e+00, -4.6374e+00],\n",
       "          [ 6.3211e+00, -6.3346e-01, -2.2774e-01,  ..., -1.7716e+00,\n",
       "           -2.5658e+00, -3.9761e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.7388e-01,  1.7042e+00,  3.7255e-01,  ...,  4.1652e-01,\n",
       "            2.4969e-01, -1.5955e+00],\n",
       "          [ 2.2271e-01,  1.6094e+00,  4.5572e-01,  ...,  3.1944e-01,\n",
       "            3.2211e-01, -1.6216e+00],\n",
       "          [ 2.2768e-01,  1.5973e+00,  2.7042e-01,  ...,  1.9458e-01,\n",
       "            6.7871e-02, -1.6047e+00],\n",
       "          ...,\n",
       "          [-2.8094e+00, -6.2955e+00,  1.2417e+00,  ...,  6.7267e-02,\n",
       "           -1.6517e+00,  4.0934e+00],\n",
       "          [-3.3341e+00, -5.6045e+00,  1.4590e-02,  ...,  9.3483e-01,\n",
       "           -2.6334e+00,  5.6653e+00],\n",
       "          [-3.3627e+00, -5.0122e+00,  4.6665e-01,  ..., -1.6090e-01,\n",
       "           -2.4685e+00,  5.7025e+00]],\n",
       "\n",
       "         [[ 9.8422e-02,  6.8828e-02,  8.5006e-02,  ..., -1.4983e-01,\n",
       "           -4.7412e-02, -2.1781e-01],\n",
       "          [ 3.4440e-02,  3.4323e-03,  3.6650e-02,  ..., -5.1264e-02,\n",
       "            6.2217e-02, -9.9036e-02],\n",
       "          [ 6.6657e-02,  5.2686e-02,  7.8903e-02,  ..., -2.2181e-01,\n",
       "           -2.5010e-02, -1.4880e-02],\n",
       "          ...,\n",
       "          [-1.2429e+00,  7.9289e-01,  3.3033e-01,  ...,  5.8236e-01,\n",
       "            6.9538e-01,  4.1842e-01],\n",
       "          [-1.2112e+00,  9.2963e-01,  9.7746e-01,  ...,  7.1472e-01,\n",
       "            5.3205e-01, -2.4754e-01],\n",
       "          [-1.3104e+00,  1.8746e+00,  1.9865e-01,  ...,  4.6062e-01,\n",
       "            4.7931e-01, -9.6842e-01]],\n",
       "\n",
       "         [[ 2.8218e-01,  2.1872e-02,  1.6265e+00,  ..., -2.8764e-01,\n",
       "           -1.5417e-01, -9.4813e-01],\n",
       "          [ 3.1909e-01,  2.2802e-02,  1.6785e+00,  ..., -2.6153e-01,\n",
       "           -2.9854e-01, -9.9181e-01],\n",
       "          [ 2.3984e-01,  9.3868e-02,  1.6231e+00,  ..., -3.6948e-01,\n",
       "           -1.9689e-01, -1.0245e+00],\n",
       "          ...,\n",
       "          [-1.7845e+00,  1.0320e+00, -6.5446e+00,  ...,  2.3300e+00,\n",
       "            3.6456e+00,  5.5592e+00],\n",
       "          [-1.6412e+00,  8.4344e-01, -8.3675e+00,  ...,  2.6527e+00,\n",
       "            2.9843e+00,  6.0396e+00],\n",
       "          [-2.5884e+00,  1.7247e+00, -7.4885e+00,  ...,  2.7149e+00,\n",
       "            2.4736e+00,  6.0002e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.1485e-02, -9.2072e-03, -8.2693e-02,  ...,  1.8361e-02,\n",
       "            3.4089e-02,  1.9138e-01],\n",
       "          [ 5.3865e-02, -3.8231e-02, -5.7398e-02,  ...,  6.2914e-02,\n",
       "           -1.2945e-02,  2.0803e-01],\n",
       "          [ 2.7502e-02, -7.8580e-02, -1.2297e-01,  ...,  3.9351e-02,\n",
       "           -1.0691e-02,  1.3585e-01],\n",
       "          ...,\n",
       "          [-3.8019e-02,  3.2126e-01,  9.2787e-03,  ...,  5.3001e-01,\n",
       "            5.2903e-01,  1.6514e+00],\n",
       "          [-1.1066e+00,  3.1936e-01,  2.2966e-01,  ..., -2.3059e-01,\n",
       "            1.1840e+00,  1.1495e+00],\n",
       "          [-4.8513e-01,  1.0402e+00,  5.9265e-01,  ..., -6.2600e-01,\n",
       "            1.2398e+00,  1.1782e+00]],\n",
       "\n",
       "         [[-2.6135e-02, -3.6179e-02,  8.7572e-02,  ..., -8.2996e-02,\n",
       "           -1.2024e-02, -9.7263e-02],\n",
       "          [ 2.6034e-02, -4.4309e-02,  9.8867e-02,  ..., -9.3440e-02,\n",
       "           -7.5331e-02,  3.7468e-02],\n",
       "          [ 6.1487e-02, -8.4139e-02,  1.6274e-01,  ..., -5.5315e-02,\n",
       "            5.2707e-02, -6.9478e-02],\n",
       "          ...,\n",
       "          [-5.3004e-01, -2.5905e-01,  3.8624e-01,  ..., -1.6827e-01,\n",
       "           -4.3325e-01, -3.7938e-01],\n",
       "          [-2.0048e-01, -8.9370e-01,  6.6534e-01,  ..., -2.6800e-01,\n",
       "           -1.1669e-01,  1.0339e+00],\n",
       "          [-1.4581e-01, -5.0692e-01,  6.0539e-01,  ..., -6.0516e-01,\n",
       "           -3.8134e-01,  7.9094e-01]],\n",
       "\n",
       "         [[ 3.3975e-02, -7.2478e-02, -1.0116e-01,  ..., -2.1623e-02,\n",
       "            8.9142e-02, -1.0009e-01],\n",
       "          [ 6.0302e-02, -1.2069e-01, -1.2685e-01,  ..., -1.6607e-02,\n",
       "            6.9798e-02, -1.3025e-01],\n",
       "          [ 3.8859e-02, -1.9800e-02, -1.4258e-01,  ..., -3.8421e-02,\n",
       "            1.4982e-01, -1.7178e-01],\n",
       "          ...,\n",
       "          [-1.0008e-02,  3.7328e-01,  5.2420e-01,  ...,  1.9785e-01,\n",
       "           -8.7445e-02, -3.4596e-01],\n",
       "          [ 3.7889e-01, -5.5893e-02,  1.2824e-01,  ...,  2.4421e-01,\n",
       "           -3.3443e-02,  4.0740e-01],\n",
       "          [ 1.6406e-01, -3.5064e-01,  2.2309e-01,  ...,  2.8343e-01,\n",
       "            8.3825e-03, -1.7795e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.8838e-02,  1.3421e-01, -2.8007e-02,  ..., -6.3179e-02,\n",
       "            1.0986e-01, -7.1088e-02],\n",
       "          [ 3.5792e-02,  4.9069e-02, -2.8272e-02,  ..., -1.3869e-02,\n",
       "            8.3428e-02, -5.5415e-02],\n",
       "          [ 9.4102e-02,  5.5815e-02,  7.8057e-02,  ...,  1.0483e-02,\n",
       "            3.0254e-02, -4.7523e-02],\n",
       "          ...,\n",
       "          [-2.7282e-01,  4.8804e-02,  4.5336e-01,  ...,  4.7668e-01,\n",
       "            9.5678e-02, -7.5674e-02],\n",
       "          [-5.1331e-02,  7.8128e-01, -1.8280e-01,  ..., -4.7621e-01,\n",
       "            3.7692e-01,  1.6765e-01],\n",
       "          [ 2.1245e-01,  1.6972e-01,  4.8349e-01,  ...,  2.5836e-02,\n",
       "           -2.6242e-01,  7.8363e-02]],\n",
       "\n",
       "         [[-1.5141e-01, -1.1761e-01, -6.0836e-02,  ..., -1.9484e-01,\n",
       "            4.1110e-02, -1.1479e-01],\n",
       "          [-1.3257e-01, -1.2713e-01,  1.6843e-02,  ..., -2.6679e-01,\n",
       "            2.0921e-02, -1.2494e-01],\n",
       "          [-7.4017e-02, -3.1522e-02, -2.9868e-02,  ..., -1.8599e-01,\n",
       "            1.1346e-02, -1.3528e-01],\n",
       "          ...,\n",
       "          [ 2.7428e-01, -9.8859e-02, -9.7123e-01,  ..., -1.4752e-01,\n",
       "            1.2340e+00,  4.4097e-01],\n",
       "          [-3.4806e-01, -4.2846e-01,  2.7540e-01,  ...,  5.7536e-01,\n",
       "            2.5233e+00,  1.2604e-01],\n",
       "          [ 2.4210e-01, -4.3396e-01, -1.9414e-01,  ...,  6.8504e-02,\n",
       "            1.6802e+00,  4.6660e-01]],\n",
       "\n",
       "         [[ 1.5207e-01, -4.5196e-02, -2.9094e-02,  ..., -3.5277e-02,\n",
       "           -1.0486e-01, -5.7238e-02],\n",
       "          [ 1.0585e-01, -4.2929e-02, -3.1422e-02,  ..., -3.2004e-03,\n",
       "           -7.0650e-02, -1.0356e-02],\n",
       "          [ 1.3868e-01, -1.1978e-01, -1.2356e-01,  ..., -9.4721e-02,\n",
       "           -1.1099e-01, -1.4762e-01],\n",
       "          ...,\n",
       "          [ 3.7741e-01, -2.6297e-01, -3.5375e-01,  ..., -1.7511e-01,\n",
       "           -2.1762e-01,  9.9573e-02],\n",
       "          [-2.8574e-01,  9.0722e-04,  2.5923e-01,  ..., -6.1785e-03,\n",
       "            3.6290e-01, -3.1209e-01],\n",
       "          [-1.2800e-01, -9.3026e-02, -4.1302e-01,  ...,  1.9383e-01,\n",
       "           -3.9103e-01, -7.2010e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-8.9406e-01, -2.0523e-01,  3.0060e-01,  ..., -7.5821e-01,\n",
       "            1.2754e-01, -2.7533e+00],\n",
       "          [-8.9332e-01, -2.1235e-01,  2.2199e-01,  ..., -7.0958e-01,\n",
       "            5.2104e-02, -2.6689e+00],\n",
       "          [-8.4074e-01, -2.5042e-01,  3.6905e-01,  ..., -6.6547e-01,\n",
       "            7.8482e-02, -2.5927e+00],\n",
       "          ...,\n",
       "          [ 6.2660e+00, -2.1418e+00, -1.8097e+00,  ...,  8.0648e-01,\n",
       "           -1.2708e+00,  5.7609e+00],\n",
       "          [ 5.0525e+00, -9.3330e-01, -3.5328e+00,  ...,  1.1110e+00,\n",
       "           -1.4260e+00,  7.2488e+00],\n",
       "          [ 5.6293e+00, -8.9411e-01, -3.0461e+00,  ...,  1.4768e+00,\n",
       "           -3.8617e-01,  7.8129e+00]],\n",
       "\n",
       "         [[ 3.7803e-01, -2.0806e-02,  3.3244e-01,  ...,  2.7638e-02,\n",
       "           -5.6725e-02, -2.1488e+00],\n",
       "          [ 4.0197e-01,  8.4760e-03,  2.6959e-01,  ..., -5.6406e-02,\n",
       "           -4.6942e-02, -2.1344e+00],\n",
       "          [ 3.8535e-01, -8.6328e-02,  1.5532e-01,  ..., -1.3687e-02,\n",
       "            3.1512e-02, -2.0446e+00],\n",
       "          ...,\n",
       "          [-1.4203e+00,  4.3140e-02,  1.6548e+00,  ..., -9.8463e-02,\n",
       "            1.3245e+00,  6.2467e+00],\n",
       "          [-2.0633e+00, -6.2272e-02,  1.3274e+00,  ...,  3.3887e-01,\n",
       "            1.2274e+00,  7.8986e+00],\n",
       "          [-1.5557e+00, -5.8115e-01,  6.7892e-01,  ..., -8.3101e-01,\n",
       "            1.9306e+00,  6.9415e+00]],\n",
       "\n",
       "         [[ 2.5004e-01, -6.5001e-01, -1.9420e-01,  ...,  1.8805e-01,\n",
       "            1.6794e-01, -4.1590e-03],\n",
       "          [ 9.3605e-02, -5.0450e-01, -2.4638e-01,  ...,  1.4263e-01,\n",
       "            9.4663e-02, -4.2967e-02],\n",
       "          [ 1.4145e-01, -6.6893e-01, -3.6935e-01,  ...,  2.3870e-01,\n",
       "            1.4972e-01,  3.5735e-02],\n",
       "          ...,\n",
       "          [ 3.7518e-01,  1.7154e+00, -1.8492e+00,  ...,  4.0338e-01,\n",
       "            2.9426e-02,  6.5703e-01],\n",
       "          [ 3.1563e-01,  2.9488e+00, -9.3646e-01,  ...,  1.8980e-01,\n",
       "            3.6774e-01, -1.9771e-03],\n",
       "          [ 7.5455e-01,  1.7727e+00, -1.2703e+00,  ...,  9.5454e-02,\n",
       "            1.0953e+00,  1.5165e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.4512e-01, -1.8632e-02, -1.7744e-02,  ...,  1.1626e+00,\n",
       "            4.3927e-02,  1.6449e+00],\n",
       "          [-2.5564e-01,  1.5019e-02,  4.1562e-02,  ...,  1.1032e+00,\n",
       "           -2.5601e-03,  1.6322e+00],\n",
       "          [-3.8872e-01, -3.0968e-02,  7.5063e-02,  ...,  1.0108e+00,\n",
       "            1.6816e-01,  1.6406e+00],\n",
       "          ...,\n",
       "          [-2.2366e+00, -4.4093e-01,  1.4687e+00,  ..., -7.0432e+00,\n",
       "           -2.3188e+00, -5.6635e+00],\n",
       "          [-1.7891e+00, -4.5107e-01,  6.1445e-01,  ..., -6.8015e+00,\n",
       "           -2.2762e+00, -5.3599e+00],\n",
       "          [-1.0965e+00, -2.3188e-01,  8.7312e-01,  ..., -6.9884e+00,\n",
       "           -2.1396e+00, -5.0259e+00]],\n",
       "\n",
       "         [[-4.3698e-01, -1.3706e-01,  2.4430e-01,  ...,  1.8940e-01,\n",
       "           -4.3987e-02,  5.8833e-02],\n",
       "          [-3.5398e-01,  7.8864e-02,  1.6684e-01,  ...,  1.9876e-01,\n",
       "           -1.0588e-01, -4.2647e-02],\n",
       "          [-4.1301e-01, -2.2162e-01,  2.6537e-01,  ...,  1.3451e-01,\n",
       "           -7.0701e-02,  5.5549e-02],\n",
       "          ...,\n",
       "          [-2.2770e-02, -5.8084e-02, -1.7046e+00,  ..., -4.5525e-01,\n",
       "           -7.5832e-01, -9.0207e-01],\n",
       "          [ 7.9969e-01, -7.5697e-01, -1.0766e+00,  ...,  5.0437e-01,\n",
       "           -5.6865e-01, -1.1924e+00],\n",
       "          [ 2.4313e+00, -6.6322e-01, -3.4546e-01,  ..., -7.5454e-02,\n",
       "           -3.7049e-01, -8.6310e-01]],\n",
       "\n",
       "         [[ 3.3474e+00,  1.8021e+00, -2.1277e+00,  ..., -2.3912e+00,\n",
       "           -3.4692e+00, -1.3132e+00],\n",
       "          [ 3.1444e+00,  1.6060e+00, -2.3091e+00,  ..., -2.4247e+00,\n",
       "           -3.4195e+00, -1.3071e+00],\n",
       "          [ 3.2344e+00,  1.6422e+00, -2.0820e+00,  ..., -2.4798e+00,\n",
       "           -3.3788e+00, -1.3311e+00],\n",
       "          ...,\n",
       "          [-8.2652e+00, -1.5507e+01, -9.0408e+00,  ...,  6.7595e-03,\n",
       "            8.5743e+00, -3.7981e+00],\n",
       "          [-7.2478e+00, -1.8471e+01, -9.9748e+00,  ..., -5.6551e-01,\n",
       "            6.6511e+00, -2.0517e+00],\n",
       "          [-7.0927e+00, -1.9824e+01, -8.4799e+00,  ..., -3.3568e-01,\n",
       "            4.7029e+00, -2.1963e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-5.2110e-02,  1.0080e-02,  8.4165e-03,  ...,  1.5843e-02,\n",
       "           -1.6291e-02, -7.6303e-03],\n",
       "          [-1.1696e-02, -6.1415e-02,  3.6344e-02,  ..., -2.1445e-03,\n",
       "           -8.3016e-02,  4.1968e-02],\n",
       "          [-3.3947e-02, -4.5631e-02,  9.5160e-04,  ..., -8.0274e-03,\n",
       "            4.4961e-02, -2.0647e-02],\n",
       "          ...,\n",
       "          [ 3.2163e-01, -2.4991e-01,  3.9302e-02,  ...,  5.7210e-01,\n",
       "           -5.7778e-01, -3.4119e-01],\n",
       "          [ 3.8867e-02, -4.9823e-01, -6.0020e-01,  ...,  3.0019e-01,\n",
       "            1.4417e-01,  9.4236e-03],\n",
       "          [ 4.1476e-01, -3.8131e-01, -8.3561e-02,  ..., -2.5963e-02,\n",
       "            1.3941e-01, -5.5803e-01]],\n",
       "\n",
       "         [[-2.0412e-02,  2.7527e-02, -9.8058e-02,  ...,  1.9029e-02,\n",
       "            9.3374e-02,  5.8530e-04],\n",
       "          [-3.7162e-02, -4.1502e-04, -6.2718e-02,  ..., -2.5058e-02,\n",
       "            8.7806e-02, -5.4501e-02],\n",
       "          [-4.8989e-02,  2.7906e-02, -5.5694e-02,  ..., -7.9824e-02,\n",
       "            2.7580e-02,  2.4791e-02],\n",
       "          ...,\n",
       "          [-2.4278e-02,  3.7901e-02,  6.3441e-02,  ..., -8.1319e-02,\n",
       "           -2.2371e-02,  3.6321e-01],\n",
       "          [ 1.1860e-01, -4.7773e-01, -2.8719e-01,  ...,  6.4342e-01,\n",
       "            1.3567e-01,  1.5320e-01],\n",
       "          [-2.2916e-01,  8.4679e-02, -2.3364e-01,  ...,  8.1471e-02,\n",
       "            9.7107e-02,  1.6834e-01]],\n",
       "\n",
       "         [[ 8.6707e-02,  2.3749e-01,  6.5693e-02,  ...,  4.7254e-02,\n",
       "           -1.1011e-01, -4.9433e-02],\n",
       "          [ 1.1560e-01,  1.3977e-01,  8.4876e-02,  ...,  1.0458e-01,\n",
       "            6.5689e-02, -6.2362e-02],\n",
       "          [ 9.8385e-02,  1.0768e-01,  4.6257e-02,  ...,  8.9628e-02,\n",
       "           -1.2323e-02, -5.3308e-02],\n",
       "          ...,\n",
       "          [ 8.3879e-01,  6.4599e-01, -2.6354e-01,  ...,  4.3727e-01,\n",
       "           -2.4813e-02,  7.2378e-01],\n",
       "          [ 8.2990e-01,  3.3623e-01, -3.1828e-01,  ...,  1.2291e+00,\n",
       "            4.5404e-01,  7.2525e-01],\n",
       "          [ 6.7110e-01, -5.2396e-01, -7.0471e-01,  ...,  5.9692e-01,\n",
       "            8.9044e-01,  1.2751e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1120e-02,  1.3837e-01, -1.0980e-01,  ...,  5.5738e-02,\n",
       "            6.4685e-02, -6.7648e-02],\n",
       "          [ 3.9333e-02,  7.4232e-02, -1.2818e-01,  ...,  9.1335e-03,\n",
       "            1.0079e-01, -8.7300e-02],\n",
       "          [ 4.0395e-02,  1.3837e-01, -1.0778e-01,  ...,  9.8192e-02,\n",
       "            7.5089e-02, -5.7860e-02],\n",
       "          ...,\n",
       "          [ 1.0581e-01,  3.7979e-01, -1.3106e-01,  ...,  8.4300e-01,\n",
       "           -9.0111e-01, -6.7957e-01],\n",
       "          [ 4.5486e-01, -7.1662e-01, -4.0016e-01,  ..., -1.2098e-01,\n",
       "            7.6506e-01, -5.7085e-01],\n",
       "          [-1.8397e-01, -5.7576e-01,  1.4233e-01,  ...,  6.7239e-02,\n",
       "            6.5927e-01, -6.1683e-01]],\n",
       "\n",
       "         [[-6.8389e-02, -2.7720e-02,  2.2350e-01,  ..., -4.1974e-02,\n",
       "            9.1938e-02, -1.0515e-01],\n",
       "          [-7.3230e-02, -1.7358e-01,  7.4397e-02,  ...,  2.7891e-03,\n",
       "            1.0193e-01, -1.3458e-01],\n",
       "          [-4.2907e-02, -5.8284e-02,  7.6536e-02,  ..., -1.3182e-01,\n",
       "            8.3099e-02, -1.2630e-01],\n",
       "          ...,\n",
       "          [-1.3016e+00,  1.7201e-01,  5.8260e-01,  ..., -1.5532e-01,\n",
       "           -7.9790e-02, -3.0932e-01],\n",
       "          [-1.2252e+00, -5.7229e-01, -5.4094e-01,  ..., -8.7802e-02,\n",
       "            3.7520e-01, -5.9587e-01],\n",
       "          [-8.6314e-01, -6.7784e-01,  5.3761e-01,  ..., -6.6615e-01,\n",
       "           -5.8115e-01, -1.4313e+00]],\n",
       "\n",
       "         [[-3.7583e-02, -2.5511e-02, -1.4136e-02,  ..., -6.6053e-02,\n",
       "           -1.7508e-02, -1.3374e-02],\n",
       "          [-3.8135e-02, -4.2578e-02, -4.3156e-03,  ..., -6.4999e-02,\n",
       "           -2.9676e-02, -6.2212e-03],\n",
       "          [-1.9707e-02, -1.9700e-02,  8.7139e-02,  ..., -4.9715e-02,\n",
       "           -1.6596e-02,  4.5022e-02],\n",
       "          ...,\n",
       "          [ 1.7148e-01, -2.3840e-01,  3.1645e-02,  ..., -5.7651e-02,\n",
       "            1.4468e-01,  5.7260e-01],\n",
       "          [ 1.7670e-01, -8.7609e-01,  6.8382e-02,  ..., -7.8759e-01,\n",
       "           -1.8720e-01, -1.7033e-01],\n",
       "          [ 1.4182e-01, -3.8005e-01,  1.0896e+00,  ..., -3.5340e-01,\n",
       "            1.4046e-02,  6.0053e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-4.7180e-02, -3.2070e-01,  3.4078e-01,  ...,  1.6165e+00,\n",
       "           -1.4138e-01, -1.2195e-01],\n",
       "          [-1.1668e-01, -2.2900e-01,  1.7684e-01,  ...,  1.5898e+00,\n",
       "           -6.7566e-02, -9.5247e-02],\n",
       "          [-1.3193e-01, -3.6052e-01,  4.0984e-01,  ...,  1.6039e+00,\n",
       "           -1.5460e-01, -2.0502e-01],\n",
       "          ...,\n",
       "          [-1.5656e-02, -2.0704e+00, -6.3535e-01,  ..., -6.1343e+00,\n",
       "           -1.0725e-01,  7.0077e-01],\n",
       "          [ 1.1049e+00, -2.1250e+00, -2.0241e+00,  ..., -7.1282e+00,\n",
       "            1.1351e-01,  2.1209e-01],\n",
       "          [-1.1653e-01, -2.2146e+00, -1.3940e+00,  ..., -6.1050e+00,\n",
       "            1.9030e+00, -2.3279e-02]],\n",
       "\n",
       "         [[ 1.3492e-01,  9.9082e-01, -1.1857e+00,  ..., -1.6766e-01,\n",
       "            1.9765e-01,  9.3392e-01],\n",
       "          [ 1.1989e-01,  1.0126e+00, -1.1185e+00,  ..., -9.6397e-02,\n",
       "            8.4991e-02,  9.2750e-01],\n",
       "          [ 1.8138e-01,  1.1577e+00, -1.0936e+00,  ..., -1.6694e-01,\n",
       "            3.7953e-02,  8.6959e-01],\n",
       "          ...,\n",
       "          [ 3.5397e+00, -5.4073e+00,  4.0782e+00,  ...,  1.0793e-01,\n",
       "            2.3037e+00, -3.8349e+00],\n",
       "          [ 4.0093e+00, -3.8992e+00,  1.3200e+00,  ...,  1.8871e+00,\n",
       "            1.3328e+00, -3.4506e+00],\n",
       "          [ 2.7035e+00, -5.5176e+00,  3.4923e+00,  ...,  1.4076e+00,\n",
       "            8.0935e-01, -4.7899e+00]],\n",
       "\n",
       "         [[-8.3077e-01,  4.6434e-01, -4.6019e-02,  ...,  2.3773e-01,\n",
       "            7.3391e-02, -3.0934e-01],\n",
       "          [-8.3770e-01,  3.4808e-01,  5.7679e-02,  ...,  2.7805e-01,\n",
       "            4.7059e-02, -2.6107e-01],\n",
       "          [-8.3108e-01,  3.3261e-01,  8.0133e-02,  ...,  2.5619e-01,\n",
       "            5.2228e-02, -2.6159e-01],\n",
       "          ...,\n",
       "          [-5.6048e-01,  2.1066e+00, -5.8299e-01,  ...,  1.4020e+00,\n",
       "           -4.0445e-02, -4.7864e-01],\n",
       "          [-1.2627e+00,  1.7236e+00, -8.2496e-01,  ...,  2.2606e+00,\n",
       "            1.2834e+00, -4.1434e-01],\n",
       "          [-9.6717e-01,  1.2203e+00,  2.2646e-02,  ...,  2.0125e+00,\n",
       "            1.3118e+00,  1.1129e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0321e-01,  2.4282e-01,  1.9190e-01,  ..., -1.6439e-01,\n",
       "            8.2811e-02,  6.2482e-02],\n",
       "          [-1.1993e-01,  1.5515e-01,  2.7116e-01,  ..., -1.7580e-01,\n",
       "            2.2365e-01,  1.0711e-01],\n",
       "          [-8.9500e-02,  1.6159e-01,  2.1411e-01,  ..., -1.9619e-01,\n",
       "            1.3975e-01,  1.0042e-01],\n",
       "          ...,\n",
       "          [-2.7829e+00, -1.3143e-01,  1.3982e+00,  ..., -1.5185e+00,\n",
       "            8.8656e-01,  4.7095e-01],\n",
       "          [-3.4152e+00,  1.0848e+00,  1.6401e+00,  ..., -6.6358e-01,\n",
       "            5.1488e-01, -2.2281e-01],\n",
       "          [-2.7295e+00, -4.1622e-02, -1.0829e-02,  ..., -8.9671e-01,\n",
       "            1.4415e+00,  2.5655e-01]],\n",
       "\n",
       "         [[-2.8845e+00,  5.1821e-01, -6.1214e-02,  ..., -4.0503e-01,\n",
       "           -5.0417e-01,  1.2596e+00],\n",
       "          [-2.7509e+00,  4.4690e-01, -1.9777e-02,  ..., -5.3147e-01,\n",
       "           -5.5034e-01,  1.0599e+00],\n",
       "          [-2.6863e+00,  3.8648e-01,  7.5897e-02,  ..., -3.0517e-01,\n",
       "           -4.5571e-01,  1.1877e+00],\n",
       "          ...,\n",
       "          [ 6.4919e+00,  9.9754e-01, -5.7621e-01,  ..., -4.3876e-01,\n",
       "            6.3037e-01, -2.0747e+00],\n",
       "          [ 7.6470e+00,  1.0217e-01, -4.2947e-02,  ..., -2.7452e-01,\n",
       "            2.9446e-02,  2.1458e-02],\n",
       "          [ 7.8787e+00, -1.0436e+00,  1.2588e-02,  ..., -3.6484e-01,\n",
       "            1.2045e+00, -8.7343e-01]],\n",
       "\n",
       "         [[-1.2258e-01, -7.6191e-02,  2.2240e-03,  ..., -9.7564e-02,\n",
       "            2.7901e-01,  1.5484e-01],\n",
       "          [-1.4313e-01, -1.4665e-01, -6.0222e-02,  ..., -2.6980e-02,\n",
       "            2.0945e-01,  1.4629e-01],\n",
       "          [-2.7109e-01, -7.8011e-02,  9.1840e-03,  ..., -1.1320e-01,\n",
       "            2.1899e-01,  1.6401e-01],\n",
       "          ...,\n",
       "          [ 6.8008e-01, -7.6546e-01, -1.0289e+00,  ...,  1.1480e+00,\n",
       "            1.2592e-02,  4.0447e-01],\n",
       "          [ 1.2858e+00, -3.9586e-01, -1.2312e+00,  ...,  1.9078e-01,\n",
       "           -9.0419e-01, -1.2184e+00],\n",
       "          [ 2.0563e-01,  4.3807e-01, -5.7896e-01,  ...,  1.5850e-01,\n",
       "            1.4936e-01, -2.2217e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-6.7339e-02, -1.3590e-01, -1.8550e-02,  ...,  9.3967e-02,\n",
       "           -8.7316e-02,  3.4483e-01],\n",
       "          [-7.2492e-02, -7.8105e-02,  6.8010e-02,  ...,  1.1805e-01,\n",
       "           -1.2383e-01,  3.2764e-01],\n",
       "          [-1.5664e-01, -1.8787e-01,  4.8914e-02,  ...,  3.2882e-02,\n",
       "           -8.1117e-02,  4.0710e-01],\n",
       "          ...,\n",
       "          [-7.7135e-01, -3.6580e-01, -1.3227e-01,  ...,  1.0815e+00,\n",
       "           -1.5738e-01,  6.2660e-01],\n",
       "          [-5.0422e-01, -4.0625e-01,  4.1568e-01,  ...,  9.6191e-01,\n",
       "           -1.2035e+00, -4.6207e-01],\n",
       "          [-8.6706e-01, -1.1261e+00,  4.4225e-01,  ...,  1.2188e+00,\n",
       "           -1.5162e+00, -3.5573e-01]],\n",
       "\n",
       "         [[ 4.4605e-02, -2.4329e-02,  7.6797e-02,  ..., -6.0430e-02,\n",
       "           -7.9475e-02,  4.1126e-03],\n",
       "          [ 1.0207e-01, -1.0721e-01,  9.9713e-02,  ...,  7.1548e-02,\n",
       "           -9.5008e-02, -3.3509e-02],\n",
       "          [ 1.0054e-01, -9.7080e-02,  5.5863e-02,  ..., -2.7842e-02,\n",
       "           -2.6944e-02,  5.3730e-02],\n",
       "          ...,\n",
       "          [-6.8715e-01, -1.0729e-01,  9.7918e-01,  ...,  2.1849e-01,\n",
       "           -1.0221e+00,  5.8819e-01],\n",
       "          [ 7.3510e-01, -7.6369e-01,  4.5887e-01,  ..., -2.8012e-01,\n",
       "           -1.5978e+00, -3.6766e-01],\n",
       "          [ 4.9671e-02, -5.4586e-01,  2.0544e+00,  ...,  2.2780e-01,\n",
       "           -1.2571e+00,  1.0962e+00]],\n",
       "\n",
       "         [[-6.9811e-02, -3.1022e-02,  5.9818e-03,  ..., -1.0210e-04,\n",
       "           -8.1284e-03, -3.5113e-02],\n",
       "          [-6.0707e-02, -1.7986e-03, -1.9783e-02,  ..., -1.1102e-02,\n",
       "            9.4711e-03, -6.0591e-02],\n",
       "          [-1.1177e-01,  2.0726e-02, -4.3944e-02,  ..., -8.3273e-03,\n",
       "           -1.1422e-02, -1.1344e-01],\n",
       "          ...,\n",
       "          [ 3.2913e-01, -5.7033e-01, -4.2640e-01,  ..., -3.2148e-01,\n",
       "           -6.0260e-01, -1.6502e-02],\n",
       "          [-1.1764e+00,  3.7533e-01,  5.7403e-01,  ..., -6.0163e-01,\n",
       "            6.1847e-01, -6.8916e-01],\n",
       "          [-8.7106e-02, -9.4833e-02,  8.1151e-01,  ..., -2.5824e-01,\n",
       "           -4.3033e-02, -3.1103e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.8473e-01, -2.2726e-01, -7.7579e-02,  ..., -4.2149e-01,\n",
       "            2.7418e-01,  1.5259e-01],\n",
       "          [-3.6244e-01, -1.4660e-01, -1.3359e-01,  ..., -5.1088e-01,\n",
       "            1.9377e-01,  1.0892e-01],\n",
       "          [-4.5824e-01, -2.9275e-02, -9.8369e-02,  ..., -4.1776e-01,\n",
       "            7.5678e-02, -3.9071e-02],\n",
       "          ...,\n",
       "          [-1.4022e-01,  4.8734e-01, -5.0450e-01,  ...,  7.9679e-01,\n",
       "           -5.8957e-01, -2.4657e+00],\n",
       "          [-2.1783e-02, -2.3571e-01,  3.6623e-02,  ...,  1.0010e+00,\n",
       "           -3.4668e-01,  1.1533e-01],\n",
       "          [-5.1143e-01,  6.7245e-01,  7.2354e-01,  ...,  1.0096e+00,\n",
       "           -1.6333e+00, -5.4689e-01]],\n",
       "\n",
       "         [[-6.8447e-02, -1.3948e-01, -5.0909e-02,  ..., -4.6781e-02,\n",
       "           -1.1206e-01,  8.3698e-02],\n",
       "          [-9.3776e-02, -1.7105e-01, -1.9154e-02,  ..., -9.4472e-02,\n",
       "           -7.6638e-02,  1.6261e-01],\n",
       "          [-7.2960e-02, -1.1857e-01, -1.0389e-01,  ..., -7.4924e-02,\n",
       "           -6.7459e-03,  1.8815e-01],\n",
       "          ...,\n",
       "          [ 1.8141e-01, -7.0140e-02, -2.2129e-01,  ...,  3.1648e-01,\n",
       "           -2.1852e-01,  4.6058e-01],\n",
       "          [ 6.2930e-01,  2.1632e-01, -3.8674e-01,  ...,  2.0151e-02,\n",
       "           -1.0750e+00,  6.3272e-01],\n",
       "          [ 1.8357e-01, -2.5175e-01, -1.6122e+00,  ...,  3.0636e-02,\n",
       "            1.8290e-01,  2.6938e-01]],\n",
       "\n",
       "         [[-8.2486e-03, -9.9348e-02,  9.2675e-02,  ...,  3.4789e-02,\n",
       "           -7.8284e-02, -2.9169e-02],\n",
       "          [ 8.4538e-03, -2.7728e-02,  3.5273e-02,  ...,  4.6024e-02,\n",
       "           -1.2318e-01,  2.6645e-02],\n",
       "          [ 5.2324e-02,  5.7104e-02,  3.1526e-02,  ...,  1.0998e-01,\n",
       "           -1.2967e-01,  1.2325e-01],\n",
       "          ...,\n",
       "          [ 6.7209e-01, -2.0902e+00, -1.9752e-01,  ...,  5.4615e-01,\n",
       "            3.0423e-02,  5.3827e-01],\n",
       "          [ 1.0081e+00, -1.6910e+00, -1.2237e+00,  ..., -1.2403e+00,\n",
       "           -7.8546e-01,  1.3901e+00],\n",
       "          [ 1.0050e-01, -5.1933e-01, -2.8909e-01,  ...,  1.7224e-01,\n",
       "            1.9918e-02,  1.6833e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-4.0050e-01,  1.0097e+00, -8.9396e-02,  ...,  1.2832e+00,\n",
       "           -1.2714e-01,  1.7388e-01],\n",
       "          [-4.3997e-01,  1.0006e+00, -8.5457e-02,  ...,  1.2402e+00,\n",
       "           -2.1031e-01,  3.9810e-02],\n",
       "          [-5.2438e-01,  1.0226e+00, -5.7329e-02,  ...,  1.2816e+00,\n",
       "           -1.1765e-01,  1.1434e-01],\n",
       "          ...,\n",
       "          [ 7.9466e-01, -5.7295e+00,  2.2801e+00,  ..., -7.6977e-02,\n",
       "           -9.7565e-01, -1.9545e-01],\n",
       "          [ 2.4032e-01, -5.0661e+00,  2.5453e+00,  ...,  1.2849e+00,\n",
       "           -1.4143e+00, -2.5908e-01],\n",
       "          [ 2.1386e-01, -5.6144e+00,  1.4590e+00,  ...,  6.5502e-01,\n",
       "           -1.3523e+00, -3.4583e-01]],\n",
       "\n",
       "         [[ 7.7587e-02,  8.0421e-01, -5.4396e-01,  ..., -1.2427e-01,\n",
       "            3.2104e-01,  4.1086e-02],\n",
       "          [ 7.9649e-02,  7.9984e-01, -5.9762e-01,  ..., -1.7429e-01,\n",
       "            4.3407e-01, -3.4069e-02],\n",
       "          [ 7.3289e-02,  8.4555e-01, -6.3540e-01,  ..., -2.2681e-01,\n",
       "            2.9125e-01,  4.5312e-02],\n",
       "          ...,\n",
       "          [-1.2718e+00,  1.1505e-01, -9.2909e-01,  ..., -1.0327e+00,\n",
       "            6.2827e-01,  1.7312e+00],\n",
       "          [-1.4508e+00,  1.7061e-01, -1.9307e+00,  ..., -1.5769e+00,\n",
       "            1.5034e+00,  1.8573e+00],\n",
       "          [-1.3811e+00, -9.7629e-01, -1.2510e-02,  ..., -1.6496e-01,\n",
       "            1.0756e+00,  5.8528e-01]],\n",
       "\n",
       "         [[-3.1934e-01,  5.6654e-02, -8.7941e-01,  ..., -3.4592e-01,\n",
       "            3.3331e-02, -1.3382e-01],\n",
       "          [-3.1588e-01, -1.9898e-03, -8.6819e-01,  ..., -3.0831e-01,\n",
       "           -1.4499e-01, -1.5526e-01],\n",
       "          [-3.8477e-01,  8.3191e-02, -7.7005e-01,  ..., -2.8715e-01,\n",
       "            4.1407e-02, -1.1156e-01],\n",
       "          ...,\n",
       "          [-7.1803e-01,  2.9697e-01,  3.5629e+00,  ..., -8.5051e-01,\n",
       "           -5.4751e-01, -9.9277e-01],\n",
       "          [-3.4132e-01,  1.1543e+00,  4.6996e+00,  ...,  2.4207e-01,\n",
       "           -2.2825e-01,  1.1611e-01],\n",
       "          [-5.6066e-01,  1.0950e+00,  4.1576e+00,  ...,  1.3342e-01,\n",
       "           -1.9627e-01, -3.5861e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.0425e-01,  1.3436e-01, -1.1882e-01,  ..., -7.9553e-02,\n",
       "            2.9320e-01, -3.6855e-03],\n",
       "          [ 3.0354e-01,  1.3499e-01, -1.2617e-01,  ...,  1.3611e-02,\n",
       "            3.0198e-01, -1.8306e-02],\n",
       "          [ 2.4172e-01,  1.1044e-01, -2.2636e-01,  ...,  5.1057e-02,\n",
       "            2.4189e-01, -4.5697e-02],\n",
       "          ...,\n",
       "          [-2.9703e+00,  5.8974e-01, -1.7564e+00,  ...,  2.7246e+00,\n",
       "           -1.3589e+00,  1.2290e+00],\n",
       "          [-2.5061e+00,  5.5060e-01, -1.5118e+00,  ...,  2.3002e+00,\n",
       "           -1.4116e-01,  1.2593e+00],\n",
       "          [-1.8369e+00,  3.1039e+00,  9.5906e-03,  ...,  5.8075e-01,\n",
       "           -2.2843e+00,  1.9188e+00]],\n",
       "\n",
       "         [[ 2.2902e-01,  3.9865e-02,  3.1877e-01,  ...,  4.9122e-01,\n",
       "            7.3942e-02,  1.5887e-01],\n",
       "          [ 2.5463e-01,  6.8092e-02,  2.4773e-01,  ...,  4.3605e-01,\n",
       "            1.4519e-01,  2.0535e-01],\n",
       "          [ 1.4760e-01, -1.1731e-01,  2.9210e-01,  ...,  4.3001e-01,\n",
       "            1.6881e-01,  1.4022e-01],\n",
       "          ...,\n",
       "          [-1.3284e+00, -8.0416e-01, -4.4965e-02,  ..., -8.1864e-01,\n",
       "            9.8067e-01, -3.2906e+00],\n",
       "          [-5.0210e-01, -9.2501e-01, -1.6124e+00,  ..., -8.9887e-01,\n",
       "            2.4526e-02, -1.4954e+00],\n",
       "          [-4.1842e-01,  7.4393e-01, -1.5448e+00,  ..., -5.4347e-01,\n",
       "            2.4820e-01, -1.2225e+00]],\n",
       "\n",
       "         [[-2.9102e+00,  5.5189e-01,  6.6227e-01,  ..., -1.0910e+00,\n",
       "            3.8114e-01,  2.6410e-01],\n",
       "          [-2.8436e+00,  6.1580e-01,  5.5984e-01,  ..., -1.0591e+00,\n",
       "            3.3743e-01,  3.9910e-01],\n",
       "          [-2.7855e+00,  5.1071e-01,  6.7102e-01,  ..., -1.0234e+00,\n",
       "            3.8241e-01,  4.0110e-01],\n",
       "          ...,\n",
       "          [ 8.1802e+00, -2.0724e+00,  3.2119e+00,  ...,  7.0620e+00,\n",
       "            1.7571e+00, -5.2427e-01],\n",
       "          [ 8.6718e+00, -2.0794e+00,  1.9456e+00,  ...,  5.9320e+00,\n",
       "            3.1833e+00, -8.7819e-01],\n",
       "          [ 8.4178e+00, -1.4573e+00,  1.8088e+00,  ...,  7.1529e+00,\n",
       "            2.7752e+00, -5.3611e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 5.7853e-02, -2.4642e-02, -1.5855e-02,  ..., -4.8961e-02,\n",
       "           -3.3722e-02, -1.5356e-01],\n",
       "          [ 2.4141e-02, -4.2802e-02, -4.5188e-03,  ..., -6.5156e-02,\n",
       "           -7.7255e-02, -5.7839e-02],\n",
       "          [ 9.0592e-02,  1.0308e-03,  5.6938e-02,  ..., -2.4534e-02,\n",
       "           -3.2714e-02, -1.1065e-01],\n",
       "          ...,\n",
       "          [-3.1328e-01, -5.4941e-01,  4.8695e-01,  ..., -5.5088e-01,\n",
       "            5.2186e-03, -1.7364e+00],\n",
       "          [-3.6417e-01, -4.7685e-01,  5.4125e-01,  ..., -3.0057e-01,\n",
       "            4.9254e-01, -7.4896e-01],\n",
       "          [ 7.5213e-02, -2.8568e-01,  3.5666e-01,  ..., -7.7972e-02,\n",
       "           -2.6055e-01, -5.5120e-01]],\n",
       "\n",
       "         [[ 6.0886e-02, -1.2365e-02, -9.5463e-02,  ...,  2.8240e-02,\n",
       "            5.5722e-03,  4.5333e-02],\n",
       "          [ 3.8407e-02, -6.4320e-02, -1.2860e-01,  ..., -6.6054e-02,\n",
       "           -7.7357e-03,  4.3613e-02],\n",
       "          [-1.6882e-02, -7.6039e-02, -1.4775e-01,  ...,  5.9050e-03,\n",
       "           -1.0020e-02,  1.8668e-04],\n",
       "          ...,\n",
       "          [-8.1884e-01, -7.2187e-01, -1.7038e+00,  ..., -1.8215e+00,\n",
       "           -2.0475e-01,  6.5958e-01],\n",
       "          [-1.3332e+00, -1.2798e+00, -6.2512e-01,  ..., -9.3885e-02,\n",
       "            6.8831e-01,  4.9631e-01],\n",
       "          [-1.4707e+00, -5.5623e-01,  1.3828e-01,  ..., -8.9210e-01,\n",
       "            8.8485e-01,  3.0120e-01]],\n",
       "\n",
       "         [[-1.1822e-02,  1.0482e-01, -8.8814e-02,  ...,  3.6839e-03,\n",
       "           -4.6069e-02,  1.3220e-02],\n",
       "          [ 7.9093e-02,  3.0830e-02, -8.4961e-02,  ...,  6.9950e-02,\n",
       "           -1.1965e-01,  6.5338e-03],\n",
       "          [-1.7603e-02,  1.3038e-01, -7.9220e-02,  ...,  1.1280e-01,\n",
       "           -1.3233e-01,  3.4548e-02],\n",
       "          ...,\n",
       "          [-7.9062e-01,  1.0091e+00,  2.3757e-01,  ..., -7.8424e-01,\n",
       "           -2.9142e-01, -2.6515e-01],\n",
       "          [ 4.3932e-01,  2.0856e+00,  5.7765e-01,  ..., -4.7868e-01,\n",
       "           -9.8892e-01, -3.7213e-01],\n",
       "          [-7.4208e-02,  1.4161e+00,  4.5203e-01,  ..., -3.7697e-01,\n",
       "            1.1020e+00, -1.1750e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.2080e-02, -2.4837e-04,  2.7276e-02,  ..., -9.3250e-02,\n",
       "           -5.9542e-02, -6.3491e-02],\n",
       "          [ 6.1710e-02, -2.9157e-02,  2.8283e-03,  ..., -1.5075e-01,\n",
       "           -5.2742e-02,  2.2761e-02],\n",
       "          [ 5.9137e-02, -3.9342e-02,  1.2503e-01,  ..., -9.1048e-02,\n",
       "           -1.0500e-02,  1.2587e-02],\n",
       "          ...,\n",
       "          [ 5.0682e-01,  1.4589e-01,  6.4514e-01,  ..., -1.8897e-01,\n",
       "            5.8946e-01,  1.8175e+00],\n",
       "          [-1.5699e-01,  6.6126e-01,  3.0902e-01,  ..., -6.7614e-01,\n",
       "           -2.6679e-02,  4.3189e-02],\n",
       "          [ 7.4099e-01,  1.1241e+00, -3.5271e-01,  ...,  3.5028e-01,\n",
       "            8.2131e-01,  1.6602e+00]],\n",
       "\n",
       "         [[-5.7933e-02, -6.0319e-02, -6.9611e-02,  ...,  6.6880e-02,\n",
       "            4.9421e-02, -1.2932e-02],\n",
       "          [-2.5366e-02,  1.7866e-02, -5.5216e-02,  ...,  2.2809e-02,\n",
       "            6.0442e-03,  6.3472e-02],\n",
       "          [ 9.1023e-02, -1.5088e-01,  4.6778e-02,  ...,  2.5710e-02,\n",
       "            8.7473e-02,  7.9632e-02],\n",
       "          ...,\n",
       "          [-1.0050e+00,  8.1491e-01, -5.9556e-02,  ...,  4.1350e-01,\n",
       "           -2.8129e-01,  1.1061e+00],\n",
       "          [-1.2593e-01,  7.9838e-02, -1.2008e-01,  ..., -6.8419e-01,\n",
       "           -8.0698e-01, -4.6149e-01],\n",
       "          [-1.0995e+00,  3.5728e-01, -8.6618e-02,  ...,  1.0717e+00,\n",
       "            8.4990e-01, -7.2818e-01]],\n",
       "\n",
       "         [[ 1.1575e-01, -2.0476e-01, -1.0883e-01,  ..., -4.3150e-02,\n",
       "            2.0312e-01, -1.7618e-02],\n",
       "          [ 1.8909e-01, -1.6144e-01, -7.7699e-02,  ...,  5.7711e-02,\n",
       "            1.8159e-01,  6.3820e-03],\n",
       "          [ 2.6300e-01, -8.6922e-02, -6.9086e-02,  ..., -1.6180e-02,\n",
       "            9.7077e-02, -1.3003e-01],\n",
       "          ...,\n",
       "          [-4.1763e-01,  4.4294e-01, -8.6901e-01,  ..., -5.6411e-01,\n",
       "            2.3317e-02,  4.4338e-01],\n",
       "          [-6.5708e-01,  1.4887e-01, -1.3116e-01,  ...,  4.5446e-01,\n",
       "           -1.3179e-01,  1.0217e+00],\n",
       "          [-1.5665e+00,  2.3995e-01, -3.0129e-01,  ...,  1.8275e-01,\n",
       "            4.4356e-01,  4.5250e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0065e+00, -2.4982e-01, -1.0235e-01,  ...,  5.8035e-01,\n",
       "            8.4039e-01, -2.0546e-01],\n",
       "          [ 9.4043e-01, -2.9921e-01, -4.5287e-02,  ...,  5.3159e-01,\n",
       "            8.0702e-01, -2.5228e-01],\n",
       "          [ 9.4514e-01, -2.4840e-01, -3.1692e-02,  ...,  5.7414e-01,\n",
       "            8.8160e-01, -1.6235e-01],\n",
       "          ...,\n",
       "          [-5.8070e+00, -1.5363e+00,  2.2635e+00,  ..., -5.2233e+00,\n",
       "           -2.6584e+00,  3.1257e-01],\n",
       "          [-8.1757e+00,  6.5138e-01,  2.1785e+00,  ..., -4.5452e+00,\n",
       "           -3.7548e+00,  1.2093e-01],\n",
       "          [-6.4924e+00,  6.3718e-01,  1.5655e+00,  ..., -5.3228e+00,\n",
       "           -4.5790e+00,  4.4682e-01]],\n",
       "\n",
       "         [[-1.3722e-01, -7.7258e-02,  8.4806e-02,  ..., -1.0555e-01,\n",
       "           -8.1340e-01, -1.5398e-01],\n",
       "          [-8.1428e-02, -4.4368e-02,  1.2121e-01,  ...,  3.3443e-03,\n",
       "           -8.1896e-01, -1.3859e-01],\n",
       "          [-2.1582e-01,  8.4549e-02,  3.3820e-02,  ..., -1.2157e-01,\n",
       "           -8.0689e-01, -6.2928e-02],\n",
       "          ...,\n",
       "          [ 5.1530e-01, -9.1441e-01,  5.9666e-01,  ..., -1.3146e+00,\n",
       "            1.2771e+00,  1.8753e-01],\n",
       "          [ 8.1075e-01,  1.8231e-02,  5.6360e-01,  ..., -9.7181e-01,\n",
       "            1.3606e+00,  7.2193e-01],\n",
       "          [-5.9642e-01, -5.5620e-01,  6.3959e-01,  ..., -1.8754e-01,\n",
       "            1.7988e+00,  7.6467e-01]],\n",
       "\n",
       "         [[ 1.9700e-01,  3.3372e-01,  1.1193e+00,  ..., -4.4966e-01,\n",
       "            4.7027e-01, -4.8243e-01],\n",
       "          [ 2.3347e-01,  2.2696e-01,  1.0866e+00,  ..., -4.1562e-01,\n",
       "            4.3753e-01, -4.3049e-01],\n",
       "          [ 1.1928e-01,  3.7818e-01,  1.1589e+00,  ..., -4.9586e-01,\n",
       "            4.4705e-01, -3.9669e-01],\n",
       "          ...,\n",
       "          [ 3.3855e+00,  5.9291e-01, -4.9890e+00,  ..., -1.5340e+00,\n",
       "            2.4920e+00, -9.8850e-01],\n",
       "          [ 3.0554e+00, -8.5166e-03, -4.0872e+00,  ..., -3.0360e+00,\n",
       "            2.4903e+00,  7.1560e-01],\n",
       "          [ 3.0001e+00, -1.7686e+00, -5.7056e+00,  ..., -2.5075e+00,\n",
       "            2.6801e+00,  1.8987e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.7899e-02,  7.3544e-02, -3.0051e-01,  ...,  5.0542e-02,\n",
       "            1.7675e-01,  6.5692e-02],\n",
       "          [-4.7994e-02,  5.0100e-02, -3.0770e-01,  ..., -2.7631e-02,\n",
       "            1.2659e-01,  6.2427e-02],\n",
       "          [-2.3603e-02,  1.9239e-02, -3.6093e-01,  ..., -8.5904e-04,\n",
       "            1.7762e-01,  1.2727e-01],\n",
       "          ...,\n",
       "          [-9.7028e-01, -5.4458e-01,  1.6386e+00,  ..., -1.6085e-01,\n",
       "           -8.7318e-01,  5.5958e-01],\n",
       "          [-1.3635e+00,  1.2899e+00, -7.8978e-02,  ..., -6.9319e-02,\n",
       "           -1.2552e-01, -8.7788e-02],\n",
       "          [-4.6347e-01,  1.0704e+00,  9.6914e-01,  ...,  4.3458e-01,\n",
       "           -6.2866e-01, -1.2810e-01]],\n",
       "\n",
       "         [[-3.9371e-01, -2.0613e+00,  1.2668e-01,  ..., -6.6736e-02,\n",
       "           -7.3476e-02,  9.3150e-01],\n",
       "          [-3.4788e-01, -2.0159e+00,  1.7018e-01,  ..., -1.5119e-01,\n",
       "           -4.3078e-02,  1.0660e+00],\n",
       "          [-4.0861e-01, -2.0413e+00,  5.6306e-02,  ..., -1.1090e-01,\n",
       "           -1.7695e-01,  9.3629e-01],\n",
       "          ...,\n",
       "          [ 5.5221e+00,  4.7421e+00, -2.8124e+00,  ...,  1.3792e+00,\n",
       "            2.5070e+00, -2.3948e+00],\n",
       "          [ 5.4164e+00,  4.1045e+00, -2.2214e+00,  ...,  1.1286e+00,\n",
       "            8.9649e-01, -1.2964e+00],\n",
       "          [ 4.2299e+00,  6.2223e+00, -2.4541e+00,  ..., -3.9225e-01,\n",
       "            1.5424e-01, -5.4246e-01]],\n",
       "\n",
       "         [[ 2.8073e-01,  1.1613e-01, -1.8267e-01,  ...,  7.3128e-01,\n",
       "            2.1132e-01,  3.2029e-01],\n",
       "          [ 2.8135e-01,  1.6677e-01, -2.7804e-01,  ...,  6.6007e-01,\n",
       "            2.0206e-01,  2.5911e-01],\n",
       "          [ 2.2795e-01,  9.1544e-02, -2.8091e-01,  ...,  6.8769e-01,\n",
       "            3.6602e-01,  3.5922e-01],\n",
       "          ...,\n",
       "          [-3.2654e-01,  1.1578e+00, -2.2181e+00,  ..., -3.1728e-01,\n",
       "            4.4488e-01, -1.5447e+00],\n",
       "          [-1.8892e-01,  6.9637e-01, -7.5129e-01,  ..., -2.9248e-02,\n",
       "           -8.5036e-01, -2.4178e-01],\n",
       "          [-2.7233e-01,  1.9800e+00, -1.3610e+00,  ..., -7.3045e-01,\n",
       "           -2.0121e-02, -4.1488e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.5574e-02,  6.5553e-02, -6.5054e-02,  ..., -9.2138e-02,\n",
       "           -1.9683e-02, -4.4395e-02],\n",
       "          [-1.1191e-02,  1.3297e-01, -1.2090e-01,  ...,  2.5934e-03,\n",
       "           -5.5480e-02,  4.8033e-02],\n",
       "          [-3.2782e-02,  8.6507e-02, -2.2573e-02,  ..., -1.2251e-02,\n",
       "           -1.2874e-02,  2.5211e-02],\n",
       "          ...,\n",
       "          [ 3.1217e-01,  4.6828e-01, -7.0294e-02,  ..., -4.1286e-01,\n",
       "           -1.9022e-01,  8.0256e-01],\n",
       "          [-3.8995e-01,  7.6221e-01, -4.1159e-01,  ...,  4.0514e-01,\n",
       "            4.6037e-01,  5.6676e-01],\n",
       "          [ 5.4426e-01,  1.8790e-02, -4.9069e-01,  ...,  4.9609e-01,\n",
       "            9.6944e-02,  6.8891e-01]],\n",
       "\n",
       "         [[ 2.9921e-02, -8.9406e-02,  2.8061e-02,  ...,  3.8785e-02,\n",
       "           -3.4401e-02,  2.3305e-02],\n",
       "          [-5.0139e-03,  4.3558e-02,  8.9953e-02,  ...,  6.8207e-02,\n",
       "           -5.3992e-02,  7.1749e-02],\n",
       "          [-1.2302e-01, -6.7482e-02,  7.6508e-02,  ...,  1.1557e-01,\n",
       "           -2.1547e-02,  7.3629e-02],\n",
       "          ...,\n",
       "          [ 1.3445e+00, -5.6628e-01, -1.9263e+00,  ...,  1.8482e-01,\n",
       "            1.0638e+00,  9.0404e-01],\n",
       "          [ 5.3753e-01, -4.1844e-01, -7.0825e-01,  ...,  1.0206e+00,\n",
       "            6.6565e-01,  8.6851e-01],\n",
       "          [ 1.2860e-01,  1.2774e-01, -2.1181e+00,  ...,  1.3112e+00,\n",
       "            1.1032e+00, -5.7197e-03]],\n",
       "\n",
       "         [[ 2.6704e-02,  1.6268e-02, -1.1432e-02,  ...,  8.1409e-02,\n",
       "           -8.2191e-02,  7.0993e-02],\n",
       "          [ 2.1534e-02,  2.6899e-02,  6.7594e-02,  ..., -5.4745e-02,\n",
       "           -6.8127e-02,  1.0758e-01],\n",
       "          [ 1.0035e-01,  2.0031e-03,  1.0910e-02,  ...,  1.3199e-01,\n",
       "           -1.5350e-01,  2.2354e-02],\n",
       "          ...,\n",
       "          [-1.1818e-02, -6.2813e-01, -1.3837e+00,  ..., -1.8063e+00,\n",
       "            1.8242e-01, -9.7880e-01],\n",
       "          [-1.0669e+00, -7.4617e-01,  7.0511e-01,  ..., -1.8995e+00,\n",
       "            6.2943e-01,  2.3573e+00],\n",
       "          [ 7.7249e-01, -1.3898e+00,  6.9383e-01,  ..., -2.5446e+00,\n",
       "            4.0979e-01, -6.4685e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7342e-01,  2.0055e-01,  5.7171e-04,  ...,  2.7655e-02,\n",
       "            6.1144e-02, -1.4388e-01],\n",
       "          [-3.0073e-01,  1.7751e-01,  6.5547e-03,  ...,  7.0813e-02,\n",
       "            5.3187e-03, -1.4316e-01],\n",
       "          [-1.9456e-01,  1.7414e-01,  8.8890e-02,  ...,  1.1542e-01,\n",
       "           -9.5960e-02, -2.3933e-01],\n",
       "          ...,\n",
       "          [-7.8896e-01,  8.0969e-01, -7.6048e-02,  ..., -7.8177e-01,\n",
       "            2.1956e-01, -5.2014e-01],\n",
       "          [ 5.6583e-01,  1.5036e+00,  4.0066e-01,  ..., -1.6955e-01,\n",
       "            3.6999e-01, -5.9757e-01],\n",
       "          [-4.1162e-01,  6.1528e-01, -1.1050e-01,  ..., -4.3567e-01,\n",
       "           -6.1172e-01, -9.2394e-01]],\n",
       "\n",
       "         [[-6.9643e-01, -3.2950e-02, -6.8221e-03,  ..., -7.2002e-02,\n",
       "           -6.1795e-03,  1.1578e-01],\n",
       "          [-7.2753e-01, -6.0240e-02,  9.8452e-02,  ...,  5.1793e-03,\n",
       "            5.3560e-02,  2.9021e-02],\n",
       "          [-6.5693e-01, -8.9535e-03, -1.3945e-01,  ...,  9.2815e-02,\n",
       "            1.2025e-01,  9.4434e-02],\n",
       "          ...,\n",
       "          [-2.0905e+00, -8.2685e-02,  1.2907e-01,  ..., -1.0842e-02,\n",
       "            5.7620e-01, -7.8632e-03],\n",
       "          [-2.7279e-01, -1.3641e-01,  1.2958e-01,  ...,  1.5761e+00,\n",
       "           -7.3023e-01,  4.5481e-01],\n",
       "          [-1.5198e+00, -1.1513e-01, -6.4311e-03,  ...,  5.1640e-01,\n",
       "            5.2213e-01,  7.1877e-01]],\n",
       "\n",
       "         [[ 6.6967e-02,  7.3060e-02, -9.5622e-02,  ...,  1.8318e-01,\n",
       "            3.8288e-02, -1.2736e-02],\n",
       "          [ 7.7911e-02,  9.2873e-02, -1.1009e-01,  ...,  1.3117e-01,\n",
       "           -7.4898e-03,  4.3349e-02],\n",
       "          [ 1.7480e-02, -8.1908e-02, -2.7433e-02,  ...,  2.1640e-01,\n",
       "            3.5441e-02, -4.5089e-02],\n",
       "          ...,\n",
       "          [ 2.3627e-01, -6.6410e-01, -4.5737e-01,  ..., -9.3303e-02,\n",
       "           -1.1355e+00,  1.0081e+00],\n",
       "          [-8.1407e-01, -4.6772e-01,  1.1959e+00,  ..., -3.8399e-01,\n",
       "           -1.9983e+00,  7.6188e-01],\n",
       "          [-5.0313e-01, -1.2388e+00,  1.3136e+00,  ..., -3.4753e-01,\n",
       "           -2.5031e+00, -2.4633e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-7.5902e-02, -2.2969e+00,  2.1307e-01,  ..., -1.5074e-01,\n",
       "           -1.5204e-01,  1.2219e-01],\n",
       "          [-1.3821e-01, -2.1520e+00,  1.8037e-01,  ..., -2.2684e-01,\n",
       "           -2.8007e-01,  1.2595e-01],\n",
       "          [-4.4907e-02, -2.1572e+00,  2.0275e-01,  ..., -2.0123e-01,\n",
       "           -1.9907e-01,  1.6327e-01],\n",
       "          ...,\n",
       "          [ 6.0036e-01,  4.0861e+00, -1.5551e-01,  ..., -7.3576e-01,\n",
       "           -1.3530e+00, -2.5897e-02],\n",
       "          [-4.0843e-01,  4.4795e+00, -6.4382e-01,  ..., -5.6078e-01,\n",
       "           -1.5442e-01,  5.0425e-01],\n",
       "          [-9.4588e-01,  5.2359e+00, -6.0603e-01,  ...,  1.1862e+00,\n",
       "            1.8334e-01, -3.2796e-01]],\n",
       "\n",
       "         [[-8.8620e-01,  2.2078e-01,  5.4190e-01,  ..., -4.9415e-01,\n",
       "            1.0606e+00,  1.1622e+00],\n",
       "          [-8.5433e-01,  1.5438e-01,  5.5929e-01,  ..., -5.3206e-01,\n",
       "            1.0650e+00,  1.2234e+00],\n",
       "          [-8.3740e-01,  1.0404e-01,  5.2959e-01,  ..., -4.8433e-01,\n",
       "            1.0072e+00,  1.0492e+00],\n",
       "          ...,\n",
       "          [-1.0683e-02,  2.3688e-01, -3.1150e+00,  ...,  2.4131e-02,\n",
       "            9.1617e-02, -7.7604e-01],\n",
       "          [ 5.6180e-01,  1.5941e+00, -6.6200e-01,  ..., -7.1343e-02,\n",
       "           -1.1750e+00, -7.2677e-01],\n",
       "          [ 1.0151e-01,  1.2923e-01, -1.5314e+00,  ...,  1.1597e+00,\n",
       "           -8.4173e-01, -9.2758e-01]],\n",
       "\n",
       "         [[-9.2710e-01,  2.8305e-01,  7.0869e-02,  ...,  5.8043e-01,\n",
       "           -2.2965e-01,  9.7441e-01],\n",
       "          [-9.1207e-01,  4.5507e-01,  1.1852e-01,  ...,  5.4215e-01,\n",
       "           -2.2121e-01,  1.0078e+00],\n",
       "          [-8.8431e-01,  4.3322e-01,  1.2132e-01,  ...,  5.8249e-01,\n",
       "           -4.0473e-01,  1.0119e+00],\n",
       "          ...,\n",
       "          [ 6.3650e-01,  6.4865e-01,  6.8646e-02,  ..., -1.5983e+00,\n",
       "            8.8444e-01, -1.5034e+00],\n",
       "          [ 9.3038e-01,  1.1334e+00,  3.2697e-01,  ..., -3.0892e-01,\n",
       "            1.6565e+00, -1.2585e+00],\n",
       "          [ 1.3349e+00,  2.9418e-01, -5.3920e-02,  ..., -1.7656e-01,\n",
       "           -6.2229e-01, -2.2923e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7681e-01, -1.7760e-01,  2.0424e-01,  ...,  2.4427e-01,\n",
       "            1.7321e+00, -2.7347e+00],\n",
       "          [-2.8887e-01, -1.5685e-01,  1.7080e-01,  ...,  2.5859e-01,\n",
       "            1.6408e+00, -2.6516e+00],\n",
       "          [-3.1173e-01, -1.5495e-01,  1.5661e-01,  ...,  2.7522e-01,\n",
       "            1.7049e+00, -2.5656e+00],\n",
       "          ...,\n",
       "          [-3.0486e-01, -1.3467e+00,  1.1344e+00,  ...,  9.1186e-01,\n",
       "           -2.8702e+00,  4.8122e+00],\n",
       "          [-5.7145e-01, -5.0965e-01,  9.6593e-01,  ...,  5.2247e-01,\n",
       "           -3.2041e+00,  5.7108e+00],\n",
       "          [-8.0197e-01, -1.4382e-01,  6.6820e-01,  ...,  1.0228e+00,\n",
       "           -3.7351e+00,  5.7248e+00]],\n",
       "\n",
       "         [[ 2.6631e-01,  3.9342e-01,  2.7521e-01,  ..., -2.9054e-01,\n",
       "           -5.2974e-02, -1.2712e-01],\n",
       "          [ 2.7136e-01,  3.7221e-01,  2.7553e-01,  ..., -2.8145e-01,\n",
       "            9.7235e-04, -1.2393e-01],\n",
       "          [ 3.0667e-01,  3.7200e-01,  2.2479e-01,  ..., -2.6956e-01,\n",
       "           -6.4356e-02,  7.4851e-02],\n",
       "          ...,\n",
       "          [-1.1762e-01,  7.0149e-01,  1.0264e-01,  ..., -1.2940e+00,\n",
       "           -1.2224e+00,  6.7835e-01],\n",
       "          [ 8.4931e-01,  3.9569e-01,  4.4113e-02,  ..., -2.4905e+00,\n",
       "           -1.1262e+00,  3.7748e-01],\n",
       "          [ 7.0490e-01,  1.1455e+00,  7.3149e-02,  ..., -1.7342e+00,\n",
       "           -1.7649e+00, -1.1142e+00]],\n",
       "\n",
       "         [[ 3.4803e-01,  1.4018e-01,  7.2108e-01,  ...,  4.9873e-01,\n",
       "            5.7724e-01, -3.5993e-01],\n",
       "          [ 3.1310e-01,  1.6400e-01,  7.0952e-01,  ...,  4.3969e-01,\n",
       "            5.0171e-01, -3.0722e-01],\n",
       "          [ 3.3600e-01,  1.5688e-01,  5.9795e-01,  ...,  4.4042e-01,\n",
       "            4.8102e-01, -2.8142e-01],\n",
       "          ...,\n",
       "          [-2.8376e-01,  7.2536e-01,  6.3574e-01,  ..., -1.7292e+00,\n",
       "           -3.5866e+00,  9.5165e-01],\n",
       "          [-1.0046e+00,  5.0861e-01,  1.6135e+00,  ..., -3.7349e+00,\n",
       "           -2.6735e+00,  1.4312e-01],\n",
       "          [ 7.4336e-01, -1.1230e-01,  2.4999e+00,  ..., -1.4966e+00,\n",
       "           -4.0082e+00,  2.6426e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.1181, -0.0400, -0.0423,  ...,  0.1543, -0.1189,  0.0161],\n",
       "          [ 0.2135, -0.0073,  0.0679,  ...,  0.2109, -0.0833,  0.0647],\n",
       "          [ 0.1990, -0.1537, -0.1224,  ...,  0.1546, -0.1541,  0.0115],\n",
       "          ...,\n",
       "          [ 0.0482,  0.0463,  1.3506,  ..., -1.2372,  1.9071,  1.1659],\n",
       "          [-0.5978,  0.8616,  1.6694,  ..., -0.4706,  0.4646,  1.0695],\n",
       "          [-0.4684,  0.3479,  1.8908,  ..., -1.2119,  0.0278,  0.4909]],\n",
       "\n",
       "         [[ 0.0740,  0.0919,  0.0186,  ..., -0.0091,  0.0049,  0.0512],\n",
       "          [ 0.0485,  0.0914,  0.0489,  ..., -0.0184, -0.0266, -0.0319],\n",
       "          [ 0.1072,  0.1839, -0.0570,  ...,  0.1629,  0.0152,  0.1021],\n",
       "          ...,\n",
       "          [ 0.8002,  0.5239, -0.3150,  ..., -1.7336,  1.3779,  0.2767],\n",
       "          [ 0.5821,  0.3632, -0.2416,  ..., -0.5362,  1.0172, -2.0144],\n",
       "          [ 0.8137,  1.5070, -0.1070,  ...,  0.8995,  1.4000, -1.5308]],\n",
       "\n",
       "         [[ 0.0295, -0.1771,  0.0262,  ...,  0.1483,  0.0389, -0.0773],\n",
       "          [ 0.0619, -0.1362,  0.0557,  ...,  0.0209, -0.0970, -0.0208],\n",
       "          [ 0.0175, -0.1098,  0.1053,  ...,  0.0360, -0.1484, -0.0160],\n",
       "          ...,\n",
       "          [ 0.7229,  0.1190,  0.3676,  ..., -0.6781, -0.2352,  1.0859],\n",
       "          [ 1.2776,  0.2037,  0.2120,  ..., -0.2628,  0.0812,  0.2549],\n",
       "          [ 1.0224, -0.3577,  1.0712,  ..., -1.4851, -1.4858,  1.9184]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0412, -0.0367, -0.0260,  ..., -0.0753,  0.0784,  0.0067],\n",
       "          [-0.0275, -0.1590, -0.0104,  ..., -0.0490,  0.0549,  0.0478],\n",
       "          [-0.1167, -0.1705, -0.0465,  ...,  0.0194, -0.0962, -0.0210],\n",
       "          ...,\n",
       "          [-0.5260,  0.1222,  0.0148,  ..., -1.1274, -1.2655, -1.3241],\n",
       "          [-0.5754,  0.5778,  1.0160,  ..., -0.1985,  0.3105, -1.5895],\n",
       "          [-0.9911,  0.4691,  0.4109,  ..., -0.8111, -0.9189, -1.6227]],\n",
       "\n",
       "         [[ 0.1167, -0.1318,  0.1862,  ...,  0.0585,  0.0132, -0.1694],\n",
       "          [ 0.0568, -0.2439,  0.1735,  ..., -0.0149, -0.0346, -0.1028],\n",
       "          [ 0.1597, -0.3780,  0.2312,  ...,  0.0565, -0.0671, -0.0077],\n",
       "          ...,\n",
       "          [ 0.9900,  0.4035,  0.8855,  ..., -1.4281,  0.2395, -1.9428],\n",
       "          [ 0.2458,  1.6843,  0.6422,  ..., -0.8176,  0.4452, -1.5686],\n",
       "          [-0.0935,  1.4191,  1.0132,  ..., -1.0833, -0.5761, -1.9852]],\n",
       "\n",
       "         [[ 0.1793, -0.1346,  0.0166,  ..., -0.0427,  0.1133,  0.0898],\n",
       "          [ 0.2479, -0.0441, -0.0890,  ...,  0.0337,  0.0286,  0.0219],\n",
       "          [ 0.2637, -0.0146, -0.1264,  ...,  0.0468,  0.0299, -0.1165],\n",
       "          ...,\n",
       "          [-1.5658, -0.6018,  0.1135,  ..., -0.3117,  0.7272,  0.6192],\n",
       "          [-0.6574, -0.4667, -1.3855,  ..., -0.0454,  1.2962,  0.5199],\n",
       "          [-1.1834, -1.5341, -2.0377,  ...,  0.2560, -0.7168, -0.9876]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0222, -0.3159, -0.3865,  ...,  0.2196,  0.2197,  0.2685],\n",
       "          [-0.0237, -0.3049, -0.3732,  ...,  0.2614,  0.0979,  0.3367],\n",
       "          [ 0.0343, -0.2378, -0.3274,  ...,  0.2004,  0.2568,  0.2504],\n",
       "          ...,\n",
       "          [-1.1978, -1.8017, -0.3189,  ..., -0.6421,  0.0284, -0.0984],\n",
       "          [ 0.6036, -1.0609,  0.2170,  ..., -0.2359,  0.7832, -0.5068],\n",
       "          [-0.1593, -1.7942, -0.3414,  ..., -0.0604,  0.4638,  1.6231]],\n",
       "\n",
       "         [[-0.3955,  0.2655,  0.0641,  ...,  0.0872, -1.1738, -0.2686],\n",
       "          [-0.3372,  0.2236, -0.0136,  ...,  0.0771, -1.1252, -0.2282],\n",
       "          [-0.2752,  0.1743,  0.1374,  ...,  0.1437, -0.9943, -0.2660],\n",
       "          ...,\n",
       "          [-0.2117,  0.2509, -1.9557,  ...,  0.3449, -1.3692, -1.3522],\n",
       "          [-0.7136, -0.7359, -1.9354,  ...,  0.3290, -2.3177, -0.0521],\n",
       "          [-1.4427,  0.2552, -1.2037,  ..., -0.5196, -0.8354,  0.0862]],\n",
       "\n",
       "         [[-1.2711, -0.1270,  0.6291,  ..., -0.7229,  0.4986, -0.2429],\n",
       "          [-1.2569, -0.0980,  0.6139,  ..., -0.7689,  0.4937, -0.2193],\n",
       "          [-1.1574, -0.1212,  0.5885,  ..., -0.7668,  0.4553, -0.1840],\n",
       "          ...,\n",
       "          [ 0.4032,  1.5884, -0.1657,  ..., -1.5122, -1.7174,  1.3869],\n",
       "          [ 1.9399,  1.2246,  0.4084,  ..., -0.4899, -0.7371,  0.9395],\n",
       "          [ 2.9431,  1.9687, -0.6992,  ..., -1.3460, -0.7208,  2.1314]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.7526, -0.9043, -0.3263,  ..., -1.1153, -0.4444,  0.4141],\n",
       "          [ 0.7346, -0.8352, -0.3751,  ..., -1.0656, -0.4650,  0.4157],\n",
       "          [ 0.8107, -0.9384, -0.2932,  ..., -0.9350, -0.4863,  0.2340],\n",
       "          ...,\n",
       "          [-1.6383,  0.0427, -0.0908,  ..., -0.5212, -0.5769, -1.8428],\n",
       "          [-3.0694, -0.3524, -1.1430,  ..., -0.8897, -1.7741, -2.9682],\n",
       "          [-3.2411,  0.7839, -0.0217,  ..., -2.2878, -0.9266, -2.9211]],\n",
       "\n",
       "         [[-0.9464,  2.1412,  0.2689,  ...,  0.4320,  1.7682, -0.6394],\n",
       "          [-0.9619,  2.0754,  0.3163,  ...,  0.3681,  1.7308, -0.6800],\n",
       "          [-0.9253,  2.1181,  0.1916,  ...,  0.4590,  1.6870, -0.7092],\n",
       "          ...,\n",
       "          [-1.2677, -3.5845, -2.1029,  ...,  0.2896, -4.2102,  1.0175],\n",
       "          [-0.0705, -2.7850, -1.7146,  ...,  1.0760, -4.9226, -0.6308],\n",
       "          [-2.3420, -3.5965, -2.2798,  ...,  0.6600, -5.8990, -1.2618]],\n",
       "\n",
       "         [[-1.8937, -0.2896, -1.1642,  ..., -0.4208,  0.0768,  0.2945],\n",
       "          [-1.8462, -0.2575, -1.1903,  ..., -0.4701,  0.1481,  0.2143],\n",
       "          [-1.8057, -0.2962, -1.2400,  ..., -0.4651,  0.1095,  0.3818],\n",
       "          ...,\n",
       "          [ 1.4258,  0.0701,  1.0037,  ...,  0.9214,  1.6596,  0.0485],\n",
       "          [ 1.6883, -0.4960,  3.0697,  ...,  1.4926,  1.5395,  0.5332],\n",
       "          [ 1.7320, -1.1426,  2.0586,  ...,  1.0799,  1.2399, -0.2060]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-1.9129e-02, -1.4341e-01,  9.0588e-03,  ...,  8.6774e-02,\n",
       "           -7.7708e-02,  1.2611e-02],\n",
       "          [-9.3530e-02, -6.8749e-03,  4.8997e-02,  ...,  4.8056e-02,\n",
       "           -6.0217e-02,  9.0133e-02],\n",
       "          [ 2.0448e-02, -1.5191e-01, -3.3445e-02,  ...,  3.9258e-02,\n",
       "           -8.8067e-02,  6.7009e-02],\n",
       "          ...,\n",
       "          [ 6.9728e-01,  2.3710e-01, -5.4304e-02,  ...,  2.0411e-02,\n",
       "            1.6471e-01,  9.9449e-01],\n",
       "          [ 1.6337e+00,  1.2118e+00, -4.9185e-01,  ...,  9.6250e-01,\n",
       "            7.5920e-01, -4.0294e-02],\n",
       "          [ 1.0361e+00,  1.0870e+00, -9.8145e-01,  ...,  2.3847e-01,\n",
       "           -1.1204e+00,  9.2173e-01]],\n",
       "\n",
       "         [[-3.2591e-02,  6.2324e-02, -2.8117e-02,  ...,  3.6760e-02,\n",
       "            6.4921e-02,  1.8478e-01],\n",
       "          [-1.2777e-01, -1.1495e-01, -7.6965e-02,  ..., -7.9648e-03,\n",
       "            1.3447e-01,  1.8248e-01],\n",
       "          [-3.2139e-01,  1.0942e-01, -3.1297e-02,  ...,  9.5149e-02,\n",
       "            2.0922e-01,  3.1586e-02],\n",
       "          ...,\n",
       "          [ 7.0347e-02,  8.9217e-01,  7.6847e-01,  ...,  1.6199e-01,\n",
       "           -1.1772e-01, -3.7208e-01],\n",
       "          [-2.5009e+00, -1.2093e+00, -2.1468e-01,  ..., -2.8417e-01,\n",
       "            1.4766e-01,  7.0776e-01],\n",
       "          [-5.8994e-01, -7.3957e-01, -1.3015e+00,  ..., -1.1279e+00,\n",
       "            1.2659e+00, -5.3118e-01]],\n",
       "\n",
       "         [[ 8.0453e-02,  1.4157e-04, -1.2594e-01,  ..., -3.1753e-02,\n",
       "            3.0943e-02,  1.3231e-02],\n",
       "          [ 2.2406e-01,  4.2511e-02, -1.1708e-01,  ..., -1.0907e-02,\n",
       "           -1.1232e-01, -3.5277e-02],\n",
       "          [ 2.0183e-01, -1.0831e-01, -1.5922e-01,  ..., -5.6653e-02,\n",
       "           -8.3683e-02, -1.5215e-01],\n",
       "          ...,\n",
       "          [-3.9319e-01, -4.3172e-01,  4.5723e-01,  ...,  5.4064e-01,\n",
       "            2.2744e-01,  4.7784e-01],\n",
       "          [ 2.9299e-01,  4.2049e-02, -2.1825e-01,  ..., -7.7056e-01,\n",
       "           -1.9892e-01,  8.4675e-01],\n",
       "          [ 3.9290e-01, -1.0421e+00, -1.4984e+00,  ..., -1.6167e+00,\n",
       "            6.5594e-01, -3.4066e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.0785e-02, -7.6876e-02, -8.6129e-02,  ...,  3.7545e-02,\n",
       "           -5.8094e-02,  5.2578e-02],\n",
       "          [ 3.4380e-02, -6.9303e-02,  5.6113e-02,  ..., -4.3722e-02,\n",
       "           -2.4220e-01, -1.7082e-01],\n",
       "          [-5.2646e-02, -1.9272e-01,  1.2077e-01,  ..., -2.5443e-02,\n",
       "            1.8597e-01,  1.0894e-01],\n",
       "          ...,\n",
       "          [-3.9659e-01,  1.5536e+00, -6.7576e-03,  ...,  6.8797e-01,\n",
       "            3.6552e-01,  1.0460e-01],\n",
       "          [ 3.0577e-02, -8.2128e-01,  3.5231e-01,  ..., -1.5621e+00,\n",
       "            1.6771e+00,  1.5382e-02],\n",
       "          [ 1.9446e-01,  6.1852e-01, -6.2060e-01,  ..., -2.5367e+00,\n",
       "            6.3600e-01, -4.6704e-01]],\n",
       "\n",
       "         [[-6.0489e-02,  7.4228e-02,  9.3286e-02,  ...,  5.7134e-02,\n",
       "            8.8688e-02, -6.6835e-02],\n",
       "          [ 5.1837e-02, -1.6932e-02,  3.0649e-02,  ...,  1.9822e-02,\n",
       "            7.4690e-02, -1.3037e-01],\n",
       "          [ 1.7061e-01, -2.8988e-04,  1.9030e-01,  ...,  2.4721e-02,\n",
       "            5.9774e-02, -1.7006e-01],\n",
       "          ...,\n",
       "          [-6.2766e-01,  8.0834e-02,  4.4419e-01,  ..., -1.0712e+00,\n",
       "           -8.1822e-01,  2.3537e-01],\n",
       "          [-6.2570e-02,  1.1702e+00, -8.2612e-02,  ...,  5.6278e-01,\n",
       "           -1.1682e+00, -1.2789e-01],\n",
       "          [ 8.7371e-01, -9.4751e-01, -8.0386e-01,  ...,  4.3756e-01,\n",
       "            1.5439e-01,  1.4368e-01]],\n",
       "\n",
       "         [[ 2.4180e-02,  5.8450e-02, -1.0861e-02,  ...,  8.1953e-02,\n",
       "           -5.2088e-03,  7.4028e-04],\n",
       "          [-8.3663e-02,  9.1557e-02, -1.6821e-01,  ...,  2.1691e-01,\n",
       "           -1.6135e-01,  2.9153e-02],\n",
       "          [-9.1064e-02, -1.9692e-01, -2.1317e-02,  ...,  1.0062e-01,\n",
       "           -1.5628e-01,  8.6707e-02],\n",
       "          ...,\n",
       "          [ 5.4974e-01, -5.4262e-01,  4.2960e-01,  ...,  9.6331e-01,\n",
       "            6.2307e-01,  3.3210e-01],\n",
       "          [-6.3514e-01,  1.6538e+00,  4.3957e-01,  ...,  6.0346e-01,\n",
       "            2.4834e-02, -2.0517e-01],\n",
       "          [-3.6138e-01, -1.5429e-01,  2.1179e-01,  ...,  7.6827e-01,\n",
       "            1.0484e+00,  6.6212e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-4.5189e-01,  5.1575e-01, -7.0459e-01,  ..., -8.9059e-01,\n",
       "           -1.2286e+00,  1.9153e-01],\n",
       "          [-3.9819e-01,  3.7609e-01, -7.1407e-01,  ..., -9.0597e-01,\n",
       "           -1.2133e+00,  1.8582e-01],\n",
       "          [-2.6073e-01,  5.8438e-01, -6.9096e-01,  ..., -1.0384e+00,\n",
       "           -1.2521e+00,  1.5192e-01],\n",
       "          ...,\n",
       "          [ 1.5350e+00, -1.0097e+00,  1.1259e+00,  ...,  2.2855e+00,\n",
       "            1.1055e+00,  1.3490e+00],\n",
       "          [-3.4695e-01, -1.7374e+00,  1.0855e+00,  ...,  6.5870e-01,\n",
       "            2.8834e-01,  1.7080e+00],\n",
       "          [ 1.0104e+00, -1.8729e+00, -1.5058e-01,  ...,  1.5932e+00,\n",
       "            1.5273e+00,  9.6922e-01]],\n",
       "\n",
       "         [[ 7.4326e-01, -1.9412e+00,  1.0822e-01,  ...,  2.0067e-01,\n",
       "           -2.2737e+00, -4.3049e-01],\n",
       "          [ 6.5091e-01, -1.7650e+00,  1.0486e-01,  ...,  1.2351e-01,\n",
       "           -2.0988e+00, -4.4448e-01],\n",
       "          [ 7.1042e-01, -1.8259e+00,  2.1543e-01,  ...,  1.7704e-01,\n",
       "           -2.1217e+00, -3.6397e-01],\n",
       "          ...,\n",
       "          [ 1.4505e-01,  2.7393e+00,  2.0443e+00,  ..., -2.6528e-01,\n",
       "            3.8919e+00, -2.5200e-01],\n",
       "          [-1.1570e+00,  6.8637e-01,  1.7953e+00,  ...,  9.9813e-01,\n",
       "           -3.0881e-01, -5.7226e-01],\n",
       "          [-1.1256e+00,  3.6073e+00,  8.1204e-01,  ..., -2.9534e-01,\n",
       "            4.3145e+00, -1.7969e+00]],\n",
       "\n",
       "         [[ 1.1099e+00,  3.9180e-01, -2.2090e-01,  ..., -7.1599e-01,\n",
       "           -1.4184e+00, -4.1113e-01],\n",
       "          [ 1.1497e+00,  3.2848e-01, -2.6859e-01,  ..., -6.4687e-01,\n",
       "           -1.3787e+00, -3.7213e-01],\n",
       "          [ 1.1217e+00,  3.2949e-01, -1.9132e-01,  ..., -7.0860e-01,\n",
       "           -1.4098e+00, -3.0551e-01],\n",
       "          ...,\n",
       "          [-3.0881e-01, -4.7338e-01,  5.5751e-01,  ..., -4.3328e-01,\n",
       "            4.9385e-01, -4.6271e-01],\n",
       "          [-1.9039e-01,  3.9334e-01,  5.7711e-01,  ..., -3.3443e-01,\n",
       "           -3.1387e-01, -9.0506e-01],\n",
       "          [ 1.4580e+00, -1.0691e+00, -2.4045e-01,  ..., -1.5993e+00,\n",
       "           -1.9951e-01,  2.4213e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.9478e-01, -7.7628e-01,  5.3524e-01,  ..., -5.4547e-01,\n",
       "            1.1461e+00,  3.1401e-01],\n",
       "          [ 3.6019e-01, -6.7915e-01,  5.0595e-01,  ..., -5.0428e-01,\n",
       "            1.2321e+00,  2.8493e-01],\n",
       "          [ 2.4362e-01, -7.5217e-01,  6.0305e-01,  ..., -5.9088e-01,\n",
       "            1.2015e+00,  2.7287e-01],\n",
       "          ...,\n",
       "          [ 4.1400e+00, -3.3214e-02, -2.9023e+00,  ...,  9.9304e-01,\n",
       "            9.6661e-01, -7.6663e-01],\n",
       "          [ 5.9300e+00, -4.4840e-01, -5.2551e+00,  ...,  2.4809e+00,\n",
       "            9.7253e-01,  8.7218e-01],\n",
       "          [ 6.6594e+00,  8.2141e-01, -5.7258e+00,  ...,  2.5373e+00,\n",
       "           -1.2718e+00,  4.2817e-01]],\n",
       "\n",
       "         [[ 3.5440e-01,  5.8291e-01,  5.9867e-01,  ...,  7.9965e-01,\n",
       "            8.6385e-02,  8.8446e-01],\n",
       "          [ 3.8334e-01,  4.7516e-01,  5.7734e-01,  ...,  7.6188e-01,\n",
       "            6.2670e-02,  8.7839e-01],\n",
       "          [ 3.8590e-01,  5.4629e-01,  6.6635e-01,  ...,  7.3167e-01,\n",
       "            5.3182e-02,  8.6265e-01],\n",
       "          ...,\n",
       "          [ 8.7054e-01,  3.3189e-01,  3.9306e-01,  ..., -1.1491e+00,\n",
       "           -1.5359e+00, -7.3206e-01],\n",
       "          [-2.1793e+00,  2.4962e-01, -1.1225e+00,  ..., -5.0484e-01,\n",
       "           -7.7237e-01,  2.7091e-01],\n",
       "          [-6.6042e-01, -1.2773e+00, -1.2097e+00,  ...,  3.8239e-02,\n",
       "           -1.0069e+00,  2.6395e-01]],\n",
       "\n",
       "         [[-7.0076e-01,  4.3208e-01, -1.5818e+00,  ..., -2.8246e-01,\n",
       "            1.8031e-01, -1.2619e+00],\n",
       "          [-7.7340e-01,  1.9435e-01, -1.5935e+00,  ..., -2.2487e-01,\n",
       "            9.4389e-02, -1.3140e+00],\n",
       "          [-7.4495e-01,  3.9381e-01, -1.4966e+00,  ..., -3.3335e-01,\n",
       "           -3.2290e-03, -1.2682e+00],\n",
       "          ...,\n",
       "          [ 2.0029e-02, -5.3322e-01,  1.8312e+00,  ...,  2.2840e+00,\n",
       "            1.0422e+00,  1.0449e+00],\n",
       "          [ 8.2782e-01, -1.7703e+00,  2.7875e+00,  ...,  2.3853e+00,\n",
       "           -2.5209e-02,  7.4770e-01],\n",
       "          [ 2.0086e+00, -2.5216e+00,  3.7790e+00,  ...,  2.5200e+00,\n",
       "           -1.1734e+00,  2.3214e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 7.9705e-02, -3.0300e-02, -1.3340e-01,  ...,  1.9086e-02,\n",
       "            9.7794e-02, -1.7617e-01],\n",
       "          [-2.4381e-03,  8.1059e-02,  3.5696e-02,  ...,  2.5313e-02,\n",
       "           -1.0111e-01, -3.1827e-01],\n",
       "          [-1.1143e-01, -4.7946e-02, -2.0797e-02,  ..., -4.6508e-02,\n",
       "           -2.7185e-01, -3.6959e-01],\n",
       "          ...,\n",
       "          [ 8.6984e-01, -4.3527e-01, -1.0011e+00,  ...,  8.3614e-01,\n",
       "            1.0836e-01, -4.3828e-01],\n",
       "          [-1.9492e+00, -1.0289e+00, -1.1938e+00,  ...,  1.2344e+00,\n",
       "            2.0194e+00, -4.4703e-02],\n",
       "          [ 7.6644e-01,  2.1317e-01, -5.6874e-02,  ...,  8.2829e-01,\n",
       "            6.8622e-01, -3.7133e-01]],\n",
       "\n",
       "         [[ 3.4748e-02, -1.4578e-01,  5.1592e-02,  ...,  3.2523e-02,\n",
       "           -6.1480e-02,  2.1003e-02],\n",
       "          [ 5.5676e-02, -8.6346e-02,  6.7871e-02,  ...,  3.5135e-02,\n",
       "            1.0826e-01, -6.6333e-02],\n",
       "          [-1.9827e-02, -6.2329e-02, -2.5362e-02,  ...,  3.3803e-02,\n",
       "            2.8758e-02,  6.6630e-02],\n",
       "          ...,\n",
       "          [ 3.8071e-02,  9.2392e-01, -9.7508e-01,  ...,  3.4176e-01,\n",
       "            5.8500e-01, -5.6342e-01],\n",
       "          [-1.1297e+00, -8.7581e-01, -8.2009e-01,  ...,  1.4234e+00,\n",
       "           -2.1820e+00,  1.9904e+00],\n",
       "          [-1.0705e+00, -1.2520e+00, -1.8928e-01,  ..., -4.9950e-01,\n",
       "            1.2434e+00,  8.3978e-01]],\n",
       "\n",
       "         [[-2.0202e-01,  1.6664e-01,  1.0720e-01,  ...,  7.6170e-02,\n",
       "            8.6529e-03,  6.5661e-02],\n",
       "          [-8.6370e-02,  1.1029e-01, -1.5717e-01,  ...,  1.1967e-01,\n",
       "            2.6380e-02,  6.3820e-02],\n",
       "          [-9.0964e-02,  1.0217e-01, -7.5252e-02,  ..., -4.3961e-02,\n",
       "            1.5958e-01,  1.6300e-02],\n",
       "          ...,\n",
       "          [-1.1457e+00, -9.8841e-01,  8.9392e-01,  ...,  5.7973e-01,\n",
       "            4.2544e-02,  1.3689e+00],\n",
       "          [-9.1339e-02,  1.7095e+00,  1.5842e+00,  ..., -4.3059e-01,\n",
       "           -2.8089e-01, -2.0397e+00],\n",
       "          [ 1.5045e+00, -4.2418e-01, -1.8929e-01,  ..., -2.2881e+00,\n",
       "            2.3554e+00,  7.1454e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7462e-02,  3.5433e-02,  4.6469e-03,  ..., -7.8031e-02,\n",
       "           -2.0351e-02, -5.6891e-02],\n",
       "          [-1.1623e-01,  6.1324e-02, -3.7416e-02,  ..., -1.4966e-01,\n",
       "           -7.7268e-02, -1.0384e-01],\n",
       "          [ 1.8045e-02,  9.3783e-02,  6.8858e-02,  ..., -1.2907e-01,\n",
       "           -2.1637e-01,  9.1495e-03],\n",
       "          ...,\n",
       "          [ 6.5088e-01,  4.1873e-02, -7.2510e-01,  ...,  6.8679e-01,\n",
       "           -1.2865e+00, -2.3541e-01],\n",
       "          [-4.5290e-01,  6.3426e-01, -5.5182e-01,  ...,  1.9403e+00,\n",
       "           -3.5551e-01, -2.4905e-01],\n",
       "          [ 2.1288e-01, -1.8589e+00, -1.4235e+00,  ...,  2.8224e-01,\n",
       "            6.7400e-01,  1.3599e+00]],\n",
       "\n",
       "         [[ 1.8533e-02,  1.1101e-01,  2.0423e-01,  ...,  1.0304e-01,\n",
       "            8.4876e-02,  9.8563e-02],\n",
       "          [ 3.0498e-02,  1.7126e-01,  2.5318e-01,  ...,  6.7495e-02,\n",
       "            2.1935e-02,  1.3430e-01],\n",
       "          [ 1.1752e-01, -8.1606e-02,  3.8073e-01,  ...,  1.1731e-02,\n",
       "           -1.6963e-01,  2.4607e-01],\n",
       "          ...,\n",
       "          [ 7.9757e-02,  1.3469e+00,  2.7865e+00,  ..., -1.0015e+00,\n",
       "            1.7793e+00,  5.8851e-02],\n",
       "          [ 2.7081e-01, -1.2495e+00, -9.0541e-01,  ...,  5.6182e-01,\n",
       "            2.0323e+00, -4.7933e-01],\n",
       "          [-1.1998e+00,  4.5707e-01, -3.4090e-01,  ...,  1.3888e-03,\n",
       "            1.9255e+00,  3.3189e-01]],\n",
       "\n",
       "         [[-1.1547e-01, -5.0219e-02, -1.4452e-01,  ...,  5.0286e-02,\n",
       "            6.6558e-02, -1.8759e-03],\n",
       "          [-7.7289e-02,  3.4985e-02, -1.3488e-01,  ..., -5.7187e-02,\n",
       "            1.4817e-01, -7.7741e-02],\n",
       "          [-1.3303e-02,  9.0304e-02, -1.4704e-01,  ...,  7.0032e-02,\n",
       "            2.9423e-01,  7.2970e-02],\n",
       "          ...,\n",
       "          [-4.2056e-01,  3.0166e-01, -1.1120e+00,  ..., -5.8127e-01,\n",
       "            2.6476e-01, -1.6269e-01],\n",
       "          [-7.6476e-01, -8.6573e-01, -1.3923e+00,  ...,  8.5466e-01,\n",
       "            2.4422e+00, -3.1225e-02],\n",
       "          [-4.2751e-01,  2.6629e+00, -6.7360e-01,  ..., -9.4934e-01,\n",
       "            1.2625e+00, -4.4446e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.5846e+00, -4.4114e-01, -2.8785e-01,  ...,  8.9017e-02,\n",
       "            3.0474e-01, -4.2875e-01],\n",
       "          [-1.5904e+00, -4.1173e-01, -3.3459e-01,  ...,  9.3176e-02,\n",
       "            3.6441e-01, -3.4328e-01],\n",
       "          [-1.5635e+00, -4.0937e-01, -1.3001e-01,  ...,  2.1239e-01,\n",
       "            3.6184e-01, -3.7184e-01],\n",
       "          ...,\n",
       "          [ 6.5321e-01,  3.3275e-01, -3.0405e-02,  ..., -1.1962e+00,\n",
       "           -1.7006e-01,  3.2706e-01],\n",
       "          [ 2.7843e+00,  1.2459e+00, -1.8291e-01,  ..., -1.1967e+00,\n",
       "           -1.5394e+00, -3.1338e-02],\n",
       "          [ 2.9284e+00,  1.0225e+00, -7.4549e-01,  ..., -9.1285e-01,\n",
       "           -3.2113e+00, -7.4516e-01]],\n",
       "\n",
       "         [[ 8.9769e-03, -6.1320e-02,  2.1961e+00,  ...,  3.1935e-01,\n",
       "            1.3991e-01, -1.7628e-01],\n",
       "          [-6.4653e-02, -5.2548e-02,  2.0728e+00,  ...,  2.7789e-01,\n",
       "            2.0462e-01, -2.0930e-01],\n",
       "          [ 8.8221e-02, -4.5567e-02,  2.2049e+00,  ...,  2.6238e-01,\n",
       "            2.1872e-01, -1.6734e-01],\n",
       "          ...,\n",
       "          [-3.2858e-01, -4.9220e-01, -2.3496e+00,  ...,  5.8566e-01,\n",
       "           -1.4773e+00, -5.6426e-01],\n",
       "          [-8.7950e-01, -5.2878e-01, -1.6371e+00,  ...,  1.0401e+00,\n",
       "           -9.2249e-02, -1.8338e-01],\n",
       "          [-3.4234e-01,  6.0935e-02, -1.9236e+00,  ...,  1.8908e+00,\n",
       "           -6.3772e-01, -8.3511e-01]],\n",
       "\n",
       "         [[-1.4484e-01,  9.3407e-01,  3.4281e-01,  ..., -5.5283e-01,\n",
       "            2.1171e-01, -1.0600e-01],\n",
       "          [-2.2150e-01,  9.1505e-01,  3.0303e-01,  ..., -4.0251e-01,\n",
       "            1.7956e-01,  4.8703e-02],\n",
       "          [-3.0725e-01,  9.0965e-01,  3.9780e-01,  ..., -3.5246e-01,\n",
       "            3.5669e-01,  4.0096e-02],\n",
       "          ...,\n",
       "          [ 6.8857e-01,  8.9555e-01, -1.0456e+00,  ...,  8.2110e-01,\n",
       "            1.8724e-01, -3.7399e-02],\n",
       "          [-6.7914e-01, -1.6564e-01, -3.9777e-02,  ...,  1.8730e-01,\n",
       "           -5.9981e-01, -2.6979e-01],\n",
       "          [ 2.8299e-01, -2.4579e-01, -3.3424e-01,  ...,  1.5160e+00,\n",
       "            1.2835e-01, -1.4697e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.5040e-01,  8.6428e-01, -7.1534e-01,  ..., -6.3294e-01,\n",
       "            6.5084e-01,  8.6147e-01],\n",
       "          [ 4.4171e-01,  7.9422e-01, -5.6689e-01,  ..., -6.2410e-01,\n",
       "            6.5351e-01,  8.6963e-01],\n",
       "          [ 4.5627e-01,  7.0805e-01, -6.2023e-01,  ..., -6.9171e-01,\n",
       "            6.2692e-01,  8.6008e-01],\n",
       "          ...,\n",
       "          [-1.6890e+00, -1.9041e+00,  1.6605e+00,  ..., -1.1356e+00,\n",
       "           -1.3783e+00, -9.3934e-01],\n",
       "          [-9.1879e-01, -1.9181e+00,  1.8178e+00,  ..., -9.8274e-01,\n",
       "           -1.0394e+00, -6.7748e-02],\n",
       "          [-1.7750e+00, -4.3560e-01,  5.2299e-01,  ..., -8.0034e-02,\n",
       "           -9.5868e-01, -9.7334e-01]],\n",
       "\n",
       "         [[-3.7142e-01,  4.3818e-01,  3.4124e-01,  ...,  6.0191e-01,\n",
       "            8.0191e-02, -8.4620e-02],\n",
       "          [-4.0977e-01,  4.9019e-01,  3.2924e-01,  ...,  6.9537e-01,\n",
       "            6.7167e-02, -3.4583e-02],\n",
       "          [-5.6474e-01,  4.7995e-01,  1.8671e-01,  ...,  3.8229e-01,\n",
       "            2.8238e-01, -1.8401e-01],\n",
       "          ...,\n",
       "          [-3.4130e-01, -1.0347e-01, -9.1012e-01,  ..., -1.3658e+00,\n",
       "           -4.5149e-01, -5.8811e-01],\n",
       "          [-6.7633e-01, -3.4154e+00, -1.4997e+00,  ..., -1.2525e+00,\n",
       "           -8.9139e-01,  6.4011e-01],\n",
       "          [-8.6520e-01, -1.5257e+00, -8.1966e-01,  ..., -7.1274e-01,\n",
       "           -1.0350e+00,  5.4637e-01]],\n",
       "\n",
       "         [[-8.9742e-01, -1.0698e-01,  4.7853e-01,  ...,  1.1121e-01,\n",
       "            1.6244e-01,  6.1143e-02],\n",
       "          [-9.2645e-01,  1.6156e-03,  4.0551e-01,  ...,  1.1105e-01,\n",
       "            1.8365e-01,  1.6930e-01],\n",
       "          [-1.0740e+00, -1.1506e-01,  4.9409e-01,  ...,  2.9284e-01,\n",
       "            3.2799e-01, -3.8375e-03],\n",
       "          ...,\n",
       "          [ 1.1177e+00, -1.1018e+00, -8.5405e-01,  ...,  3.1836e-01,\n",
       "            7.0054e-01,  9.1347e-01],\n",
       "          [ 3.9098e-01, -1.4763e+00, -5.8363e-01,  ...,  7.7698e-01,\n",
       "            3.9830e-01,  1.3249e+00],\n",
       "          [-4.3330e-01, -1.2123e+00, -2.5434e-01,  ...,  1.4076e+00,\n",
       "            8.9947e-01,  5.4536e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.1250e-01, -1.1254e-01, -1.0370e-01,  ..., -4.1246e-01,\n",
       "           -4.7890e-02, -2.1121e-01],\n",
       "          [ 1.8409e-01, -5.5825e-02, -4.9921e-02,  ..., -4.9713e-01,\n",
       "           -4.4836e-02, -4.7701e-02],\n",
       "          [ 7.1470e-02, -8.2343e-02,  5.7812e-03,  ..., -3.1748e-01,\n",
       "           -2.4217e-01,  2.1661e-01],\n",
       "          ...,\n",
       "          [ 6.1303e-01,  2.0522e+00,  1.8393e+00,  ...,  1.6024e+00,\n",
       "            5.1778e-01,  1.9248e+00],\n",
       "          [-1.2779e+00,  6.1831e-02,  9.2094e-01,  ...,  2.2381e+00,\n",
       "            1.6397e+00,  2.3052e+00],\n",
       "          [-8.3901e-01, -2.8221e-01,  5.0947e-01,  ...,  1.3583e+00,\n",
       "            4.6206e-01,  3.1119e+00]],\n",
       "\n",
       "         [[ 9.3963e-02,  3.3356e-02,  1.2081e-01,  ..., -5.8449e-02,\n",
       "           -2.2173e-01,  2.9188e-01],\n",
       "          [ 1.7160e-01, -2.2772e-02,  6.1095e-02,  ..., -1.7263e-02,\n",
       "           -3.1094e-01,  9.0343e-02],\n",
       "          [ 2.1001e-01, -1.8184e-02, -9.6955e-02,  ...,  3.8297e-02,\n",
       "           -3.6917e-01,  4.5891e-01],\n",
       "          ...,\n",
       "          [-8.1164e-01,  1.0653e+00,  2.0096e-01,  ...,  3.5399e-02,\n",
       "            8.0531e-01,  1.6106e+00],\n",
       "          [ 1.5045e+00,  3.5615e+00, -1.3869e+00,  ..., -6.5008e-01,\n",
       "            1.6169e-01, -2.2589e-01],\n",
       "          [ 8.3717e-02,  6.2462e-01, -1.3517e+00,  ...,  1.5982e+00,\n",
       "            1.2195e+00,  1.6228e+00]],\n",
       "\n",
       "         [[-4.6763e-02, -6.1215e-02,  6.8244e-02,  ...,  1.9776e-02,\n",
       "           -1.3391e-01, -3.5823e-02],\n",
       "          [-1.1930e-01,  6.2276e-02, -6.3967e-02,  ...,  1.3009e-01,\n",
       "           -7.2578e-02,  4.0818e-03],\n",
       "          [-4.2147e-01,  2.3536e-01, -1.3311e-01,  ..., -2.3478e-01,\n",
       "            1.6730e-01, -2.5955e-01],\n",
       "          ...,\n",
       "          [ 1.0520e+00,  3.2658e-01, -3.3460e-01,  ..., -2.9865e-01,\n",
       "           -7.2634e-01, -5.1497e-02],\n",
       "          [-4.3020e+00,  2.2056e+00, -7.0785e-01,  ..., -2.3638e-01,\n",
       "            3.6086e-02, -6.6901e-01],\n",
       "          [-6.0365e-01, -9.8467e-01,  4.2457e-01,  ...,  1.2292e+00,\n",
       "           -9.6456e-01, -1.7314e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.8973e-01, -1.0955e-01, -2.3672e-02,  ...,  1.7145e-01,\n",
       "           -6.6491e-02,  2.8758e-02],\n",
       "          [ 3.8910e-02, -9.9004e-02,  2.0725e-01,  ..., -8.0339e-02,\n",
       "            2.2189e-01,  6.9365e-02],\n",
       "          [-3.8548e-01, -1.9781e-01,  7.1801e-02,  ..., -1.4020e-01,\n",
       "            1.2128e-01, -1.7169e-01],\n",
       "          ...,\n",
       "          [ 2.0548e+00,  2.6173e-01,  2.2928e+00,  ...,  2.8067e-01,\n",
       "           -3.3825e-02,  1.0890e+00],\n",
       "          [-1.0205e+00,  1.3905e+00, -2.1344e-01,  ...,  5.9804e-01,\n",
       "            4.4544e-01, -2.3480e+00],\n",
       "          [ 8.2792e-01,  9.4713e-01,  1.7329e+00,  ..., -4.3321e-02,\n",
       "            8.3468e-01, -8.7587e-01]],\n",
       "\n",
       "         [[-2.5013e-01, -6.6256e-02,  1.4201e-01,  ...,  4.8743e-02,\n",
       "            4.7427e-02, -2.7213e-01],\n",
       "          [-2.5519e-01,  3.5433e-02,  1.1614e-02,  ...,  9.3536e-02,\n",
       "           -1.8797e-02, -3.2062e-01],\n",
       "          [-2.9584e-01, -2.0254e-01, -7.2109e-02,  ..., -1.6756e-01,\n",
       "            2.4284e-02, -4.9046e-01],\n",
       "          ...,\n",
       "          [-3.7893e-01,  5.9704e-01, -1.3953e+00,  ..., -2.5510e-01,\n",
       "            7.3124e-01, -3.4163e-01],\n",
       "          [ 1.3040e-01, -2.1206e+00,  2.4120e+00,  ..., -4.0610e-01,\n",
       "           -1.2326e+00, -9.4751e-01],\n",
       "          [-3.3190e+00, -1.2255e+00,  3.8022e-01,  ..., -2.0979e+00,\n",
       "           -2.2796e+00,  5.6772e-01]],\n",
       "\n",
       "         [[ 1.6444e-01,  1.1251e-01,  1.6669e-01,  ..., -9.9347e-02,\n",
       "           -3.8279e-02, -4.6995e-02],\n",
       "          [-1.9921e-02, -2.9681e-01,  1.0562e-01,  ..., -1.2211e-02,\n",
       "           -2.6744e-01, -4.2433e-01],\n",
       "          [-9.8309e-03,  2.2196e-02,  9.9082e-02,  ..., -1.3696e-01,\n",
       "           -2.0221e-01,  6.9135e-03],\n",
       "          ...,\n",
       "          [-9.9182e-02,  5.1681e-01,  2.4010e-01,  ..., -5.8783e-01,\n",
       "            1.2849e-01, -9.8441e-02],\n",
       "          [-1.4058e-01, -7.4925e-01,  7.1151e-01,  ...,  7.9351e-01,\n",
       "           -4.8208e-01, -4.2554e-01],\n",
       "          [ 4.6111e-01, -6.2778e-01,  1.0429e+00,  ...,  2.4919e+00,\n",
       "            7.5462e-02, -2.3000e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.loss)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-3)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db31390efb234414a04ecb5c9f31c3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step: 0 | Loss: 112.278938\n",
      "Diff in soft embs:  0.0\n",
      "Epoch: 0 | Step: 1000 | Loss: 98.665703\n",
      "Diff in soft embs:  0.0\n",
      "Epoch: 0 | Step: 2000 | Loss: 108.133545\n",
      "Diff in soft embs:  0.0\n",
      "Epoch: 0 | Step: 3000 | Loss: 101.336281\n",
      "Diff in soft embs:  0.0\n",
      "Epoch: 0 | Step: 4000 | Loss: 103.810539\n",
      "Diff in soft embs:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     10\u001b[0m     batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[39m=\u001b[39m ptgpt2(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     12\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     13\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[63], line 48\u001b[0m, in \u001b[0;36mPrefixTunedGPT2.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     40\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(((\n\u001b[1;32m     41\u001b[0m     torch\u001b[39m.\u001b[39mones((batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoft_prompt_length))\u001b[39m.\u001b[39mlong() \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m \n\u001b[1;32m     42\u001b[0m                     )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), input_ids), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[39m# print(\"Shapes: \", input_embs.shape, attention_mask.shape, labels.shape, \" | \", input_ids.shape)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# print(\"nan check (should be false): \", input_embs.isnan().any(), attention_mask.isnan().any(), labels.isnan().any())\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# print(\"attention mask:\", attention_mask, attention_mask.dtype)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels, inputs_embeds\u001b[39m=\u001b[39;49minput_embs)\n\u001b[1;32m     49\u001b[0m \u001b[39m# return outputs, soft_prompt, input_embs, k\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1075\u001b[0m     input_ids,\n\u001b[1;32m   1076\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1077\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1079\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1080\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1082\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1084\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1085\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1087\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[1;32m    890\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    891\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    892\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    893\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    894\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    895\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    896\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:356\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    354\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m    355\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[0;32m--> 356\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_proj(hidden_states)\n\u001b[1;32m    357\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    358\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/transformers/pytorch_utils.py:107\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    106\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 107\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    108\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "lastprompt = ptgpt2.check_in()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = torch.tensor([0], requires_grad=False).float().cuda()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = ptgpt2(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # if(i%10 == 0):\n",
    "        #     print(ptgpt2.soft_prompt.weight.grad)\n",
    "\n",
    "        if(i % 500 == 0):\n",
    "            print(f\"Epoch: %d | Step: %d | Loss: %f\" % (epoch, i, loss))\n",
    "            newprompt = ptgpt2.check_in()\n",
    "            print(\"Diff in soft embs: \", torch.sum(newprompt-lastprompt).item())\n",
    "            lastprompt = newprompt\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(ptgpt2.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "    print(avg_loss/len(train_dataloader))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droid-slam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
