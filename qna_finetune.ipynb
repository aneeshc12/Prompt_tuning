{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import kaggle\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,  DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, trim train and val for easier computation\n",
    "dataset = load_dataset(\"squad\")\n",
    "traindata = dataset[\"train\"].shuffle().select(range(10000))\n",
    "valdata = dataset[\"validation\"].shuffle().select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50263, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add extra special characters\n",
    "\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<|bos|>\",\n",
    "                              \"eos_token\": \"<|eos|>\",\n",
    "                              \"unk_token\": \"<|unk|>\",\n",
    "                              \"sep_token\": \"<|sep|>\",\n",
    "                              \"pad_token\": \"<|pad|>\",\n",
    "                              \"cls_token\": \"<|cls|>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 8711.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for t in tqdm(traindata):\n",
    "    if len(t['answers']['text']) > 1:\n",
    "        print(t)\n",
    "# traindata[0]['answers'][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50263, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = tokenizer(traindata[0][\"context\"] + \"<|bos|>\" + traindata[0][\"question\"] + \"<|sep|>\" + traindata[0][\"answers\"][\"text\"][0] + \"<|eos|>\",\n",
    "                           padding=True, truncation=True)\n",
    "default_label = tokenizer(\"<|pad|>\" * (len(traindata[0][\"context\"] + \"<|bos|>\" + traindata[0][\"question\"]) - 1)\n",
    "                          + \"<|sep|>\" + traindata[0][\"answers\"][\"text\"][0] + \"<|eos|>\" \n",
    "                          + \"<|pad|>\",\n",
    "                          padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all examples in the dataset w/ huggingfaces dataset.map()\n",
    "\n",
    "def tokenize_function(item):\n",
    "       lengths = [len(tokenizer(context  + \"<|bos|>\" + q + \"<|sep|>\",\n",
    "                  truncation=True)['input_ids']) \\\n",
    "              for (context, q) in zip(item[\"context\"], item[\"question\"])]\n",
    "\n",
    "\n",
    "       default_prompt = [tokenizer(context + \"<|bos|>\" + q + \"<|sep|>\" + a,\n",
    "                            truncation=True) \\\n",
    "                     for (context, q, a) in zip(item[\"context\"], item[\"question\"], [k[\"text\"][0] for k in item[\"answers\"]])]\n",
    "       # default_label = [tokenizer(\"<|pad|> \" * (len(context + \"<|bos|>\" + q) - 1)\n",
    "\n",
    "       default_label = [tokenizer(context + \"<|bos|>\" + q + \"<|sep|>\" + a + \"<|eos|>\",\n",
    "                            truncation=True) \\\n",
    "                     for (context, q, a) in zip(item[\"context\"], item[\"question\"], [k[\"text\"][0] for k in item[\"answers\"]])]\n",
    "\n",
    "       for i, l in enumerate(lengths):\n",
    "              default_label[i][\"input_ids\"][:l] = [-100 for i in range(l)]\n",
    "\n",
    "       ret = {\"input_ids\": [i[\"input_ids\"] for i in default_prompt],\n",
    "              \"attention_mask\": [a[\"attention_mask\"] for a in default_prompt],\n",
    "              \"labels\": [i[\"input_ids\"][1:] for i in default_label]}         # shift labels by one\n",
    "\n",
    "       return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211bc20706834e108716b22dcc1e6d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = valdata.map(tokenize_function, batched=True, remove_columns=valdata.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bb21d9d2464848bf014f64266f0074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_traindata = traindata.map(tokenize_function, batched=True, remove_columns=traindata.column_names)\n",
    "tokenized_valdata = valdata.map(tokenize_function, batched=True, remove_columns=valdata.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_valdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator =  DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8332, 262, 4238, 4633, 1803, 11, 1811, 9293, 423, 1813, 262, 1080, 845, 922, 8088, 4632, 5115, 663, 6890, 13, 327, 12884, 1578, 7526, 15342, 262, 1080, 2282, 11, 366, 1169, 6599, 18, 318, 257, 21362, 290, 8036, 3704, 286, 1363, 12, 298, 1425, 434, 5112, 326, 3160, 510, 284, 262, 20606, 26894, 262, 6599, 18, 318, 880, 2861, 663, 29784, 2756, 7621, 526, 327, 12884, 11343, 340, 257, 4776, 286, 807, 13, 23, 503, 286, 838, 290, 7052, 340, 355, 663, 1271, 530, 366, 27238, 12, 14150, 1, 42892, 11, 33557, 663, 12373, 27831, 9889, 290, 30511, 20897, 1486, 981, 26816, 663, 3614, 6356, 286, 1695, 1830, 13, 554, 3090, 11, 1111, 5995, 22207, 11175, 290, 11165, 14661, 423, 1813, 262, 1080, 338, 12391, 12, 2433, 16388, 845, 17070, 8088, 11, 12316, 326, 262, 3081, 286, 16388, 21695, 326, 286, 867, 1459, 27669, 12391, 12, 2433, 8444, 1938, 13, 50257, 2061, 3052, 531, 262, 14047, 513, 366, 75, 1083, 510, 284, 262, 20606, 13984, 220, 50260, 34, 12884, 1578, 7526], [24472, 31982, 11, 257, 1964, 3356, 4166, 287, 262, 1578, 1829, 284, 14598, 2839, 30632, 13, 12168, 2585, 17814, 3657, 543, 2672, 30632, 284, 307, 5952, 1626, 257, 366, 11930, 1, 393, 366, 268, 17966, 1, 284, 366, 1069, 9152, 1171, 1570, 526, 1114, 1672, 11, 287, 30992, 11, 262, 11565, 15928, 8197, 257, 14195, 357, 43, 13, 1129, 1129, 11, 279, 13, 767, 6659, 8, 543, 2672, 11, 366, 1169, 6827, 286, 1918, 815, 307, 10945, 1626, 262, 7968, 7356, 11, 611, 11282, 11, 290, 4306, 1626, 281, 30685, 1474, 262, 7356, 526, 383, 11565, 1099, 10431, 262, 1957, 16570, 284, 14983, 8318, 284, 3925, 357, 23073, 1957, 4290, 8, 4150, 339, 4762, 815, 4973, 262, 10938, 11, 475, 262, 15059, 10203, 784, 329, 2972, 3840, 784, 3360, 6699, 8318, 284, 3925, 508, 2227, 284, 2342, 13, 11565, 30632, 5952, 706, 30992, 547, 407, 366, 11377, 1, 780, 484, 547, 5952, 2157, 4838, 7714, 11, 290, 262, 2276, 1171, 373, 407, 10431, 284, 5262, 13, 50257, 818, 262, 11565, 286, 30992, 11, 508, 10158, 503, 8318, 284, 4973, 30632, 30, 50260, 1169, 1957, 16570], [818, 262, 1290, 10183, 636, 286, 262, 1181, 262, 609, 48406, 993, 7258, 18692, 38777, 2233, 284, 1877, 32025, 290, 4457, 1029, 10101, 26, 617, 3006, 286, 262, 10183, 636, 286, 262, 1181, 389, 523, 5894, 645, 28459, 318, 1043, 588, 262, 3837, 360, 4015, 286, 3409, 282, 323, 43120, 13, 1318, 389, 734, 18778, 4258, 14123, 1043, 287, 262, 10183, 636, 286, 262, 1181, 25, 6964, 18692, 357, 33, 1199, 8, 290, 15226, 18692, 357, 48802, 74, 8, 543, 389, 47543, 416, 2811, 5079, 5951, 2233, 284, 5400, 287, 22910, 13, 1318, 318, 257, 6801, 6516, 287, 262, 3504, 286, 262, 1181, 1022, 262, 734, 4457, 1180, 45225, 422, 262, 7627, 290, 7421, 26, 428, 6516, 318, 262, 2441, 27768, 16264, 416, 257, 13110, 1022, 45273, 1335, 4258, 14123, 13, 50257, 464, 6801, 6516, 1022, 7627, 290, 7421, 318, 1444, 644, 30, 50260, 1169, 2441, 27768], [464, 14884, 3615, 286, 4486, 8096, 2092, 14479, 13, 554, 6733, 370, 2120, 11, 257, 20555, 7533, 3199, 287, 29679, 58, 68, 60, 262, 38702, 20555, 3554, 15819, 16322, 1671, 30830, 6886, 262, 7681, 357, 37114, 10200, 366, 1169, 749, 35578, 2700, 287, 262, 995, 4943, 290, 7189, 25, 366, 464, 2679, 1230, 6875, 2346, 3492, 329, 8030, 2316, 351, 262, 7570, 4479, 11, 9472, 262, 3594, 1906, 24111, 1175, 24003, 15997, 257, 1175, 1028, 262, 15889, 7570, 4479, 13, 383, 7570, 661, 290, 262, 1762, 661, 286, 4486, 423, 281, 1393, 287, 12174, 262, 3594, 1175, 1410, 526, 50257, 8241, 2227, 1175, 351, 262, 7570, 4479, 30, 50260, 15823, 1906, 24111, 1175, 24003], [50, 11736, 67, 4997, 271, 1275, 417, 571, 7240, 357, 49022, 329, 366, 5189, 262, 18725, 571, 378, 33060, 12340, 44532, 515, 319, 1987, 2795, 15904, 11, 33446, 262, 7835, 4564, 338, 6761, 286, 11503, 306, 18725, 571, 1590, 287, 262, 2688, 13, 770, 551, 15539, 605, 373, 3194, 287, 262, 7765, 286, 19057, 2873, 11, 618, 262, 7835, 4564, 373, 14085, 290, 2710, 1710, 867, 890, 12, 10217, 6593, 13, 20876, 306, 18725, 571, 1590, 318, 3177, 257, 12883, 2138, 621, 40937, 11, 290, 617, 550, 2938, 326, 340, 1244, 307, 18397, 13, 554, 2882, 284, 777, 2683, 11, 262, 13258, 30384, 8789, 262, 12883, 355, 257, 890, 12, 10217, 3357, 351, 2041, 6817, 287, 262, 7835, 4564, 13, 383, 551, 15539, 605, 9295, 45744, 4997, 271, 1275, 417, 571, 7240, 422, 1987, 2795, 15904, 11, 19623, 262, 4569, 4564, 7743, 11, 326, 18725, 571, 1590, 318, 281, 7306, 1181, 290, 4477, 284, 307, 13677, 329, 7993, 7835, 17345, 13, 15248, 571, 1590, 6194, 4340, 262, 3950, 286, 262, 13239, 286, 1793, 10371, 3660, 3592, 13, 383, 11503, 306, 18725, 571, 1590, 318, 7173, 6692, 284, 262, 5360, 859, 2470, 33060, 13, 2102, 11, 1141, 465, 45443, 22460, 3362, 13889, 373, 3177, 14431, 287, 29256, 27538, 284, 7264, 8591, 291, 1634, 286, 17345, 508, 2227, 284, 2666, 262, 5360, 45744, 4997, 1181, 11, 257, 2292, 543, 373, 22188, 17687, 416, 1757, 3362, 2873, 287, 7169, 290, 20534, 276, 287, 262, 13540, 19507, 3854, 326, 691, 262, 26850, 460, 287, 15313, 5917, 7264, 8591, 291, 1634, 13, 50257, 2061, 3188, 220, 286, 15904, 21068, 262, 4928, 338, 12046, 286, 18725, 571, 1590, 287, 262, 33060, 30, 50260, 50, 11736, 67, 4997, 271, 1275, 417, 571, 7240], [464, 31953, 3071, 2497, 5433, 7179, 14952, 7018, 284, 262, 2097, 286, 13815, 11, 257, 2383, 5373, 1201, 11, 257, 614, 878, 262, 3071, 11, 262, 2097, 286, 18651, 550, 3804, 262, 29667, 8492, 6922, 326, 833, 2367, 791, 507, 287, 262, 1578, 7526, 714, 645, 2392, 16565, 1637, 284, 1814, 262, 3071, 9964, 290, 9400, 286, 7179, 14952, 13, 383, 15030, 20162, 547, 19084, 284, 14634, 428, 13657, 2551, 351, 4165, 5520, 13, 383, 6001, 286, 12266, 13110, 373, 284, 10400, 257, 7699, 329, 12688, 286, 8411, 284, 4781, 262, 761, 284, 6211, 262, 9601, 791, 507, 13, 2750, 35145, 11, 7452, 351, 262, 5471, 286, 262, 4387, 833, 2367, 791, 507, 11, 262, 12266, 1230, 3804, 262, 9601, 36060, 1769, 2191, 284, 1249, 9601, 791, 507, 284, 1814, 7179, 14952, 1752, 517, 13, 50257, 8241, 3804, 262, 29667, 8492, 30, 50260, 35965, 36060, 1769, 2191], [21009, 286, 10140, 1703, 372, 301, 4837, 6108, 326, 422, 8069, 284, 8235, 11, 3139, 5474, 422, 1542, 3834, 12, 40461, 2678, 39398, 720, 23451, 9374, 11, 23353, 883, 7027, 6, 7097, 20250, 13, 357, 464, 2482, 11, 6241, 287, 34445, 393, 25822, 2478, 11, 423, 587, 29563, 287, 4583, 416, 17646, 337, 1192, 333, 31530, 2014, 554, 262, 1339, 286, 5478, 11, 530, 286, 262, 5087, 329, 428, 4069, 373, 1964, 24842, 11, 290, 262, 1109, 326, 649, 6905, 1690, 34012, 2180, 1230, 338, 10622, 306, 6492, 6798, 13, 770, 10085, 2828, 284, 38305, 511, 5129, 10522, 11, 503, 286, 3151, 286, 597, 2003, 1033, 9219, 341, 13, 554, 6273, 11, 7740, 34790, 884, 355, 1778, 18647, 78, 338, 968, 8284, 1690, 1718, 257, 2005, 319, 1597, 8945, 393, 2810, 3403, 329, 2478, 11, 832, 6884, 4896, 11, 1099, 290, 1502, 11, 3503, 13, 50257, 13828, 4837, 9713, 9253, 422, 3834, 12, 40461, 2678, 422, 8069, 284, 8235, 30, 50260, 21009, 286, 10140, 1703, 372, 301], [46667, 4894, 14807, 357, 52, 25374, 8, 389, 27953, 3006, 351, 2440, 10101, 621, 326, 286, 262, 7346, 2858, 13, 383, 2440, 10101, 389, 257, 1255, 286, 3220, 24774, 286, 262, 12347, 1657, 416, 7876, 5696, 884, 355, 48292, 290, 10017, 11, 543, 423, 2793, 435, 3077, 418, 290, 2440, 4894, 32484, 621, 883, 287, 262, 3288, 2858, 13, 317, 15836, 2446, 286, 3753, 27362, 262, 471, 25374, 1245, 318, 284, 7521, 6832, 290, 9725, 2330, 290, 4618, 7150, 13, 8554, 777, 5050, 11, 257, 25345, 366, 24494, 5348, 1, 1430, 287, 5401, 5652, 468, 13301, 326, 7876, 10101, 714, 307, 5322, 416, 6702, 513, 22074, 34, 379, 281, 6108, 1575, 286, 1294, 3, 16, 2997, 11, 3501, 6108, 2472, 5079, 4034, 286, 1294, 3, 38612, 1510, 422, 5322, 1633, 12, 31448, 278, 3484, 290, 11409, 10653, 13, 50257, 2061, 318, 257, 835, 284, 4646, 262, 1029, 10101, 2727, 287, 7876, 4894, 14807, 30, 50260, 79, 2913, 6832, 290, 9725, 2330, 290, 4618, 7150]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[-100, 4238, 4633, 1803, 11, 1811, 9293, 423, 1813, 262, 1080, 845, 922, 8088, 4632, 5115, 663, 6890, 13, 327, 12884, 1578, 7526, 15342, 262, 1080, 2282, 11, 366, 1169, 6599, 18, 318, 257, 21362, 290, 8036, 3704, 286, 1363, 12, 298, 1425, 434, 5112, 326, 3160, 510, 284, 262, 20606, 26894, 262, 6599, 18, 318, 880, 2861, 663, 29784, 2756, 7621, 526, 327, 12884, 11343, 340, 257, 4776, 286, 807, 13, 23, 503, 286, 838, 290, 7052, 340, 355, 663, 1271, 530, 366, 27238, 12, 14150, 1, 42892, 11, 33557, 663, 12373, 27831, 9889, 290, 30511, 20897, 1486, 981, 26816, 663, 3614, 6356, 286, 1695, 1830, 13, 554, 3090, 11, 1111, 5995, 22207, 11175, 290, 11165, 14661, 423, 1813, 262, 1080, 338, 12391, 12, 2433, 16388, 845, 17070, 8088, 11, 12316, 326, 262, 3081, 286, 16388, 21695, 326, 286, 867, 1459, 27669, 12391, 12, 2433, 8444, 1938, 13, 50257, 2061, 3052, 531, 262, 14047, 513, 366, 75, 1083, 510, 284, 262, 20606, 13984, 220, 50260, 34, 12884, 1578, 7526, 50258], [-100, 11, 257, 1964, 3356, 4166, 287, 262, 1578, 1829, 284, 14598, 2839, 30632, 13, 12168, 2585, 17814, 3657, 543, 2672, 30632, 284, 307, 5952, 1626, 257, 366, 11930, 1, 393, 366, 268, 17966, 1, 284, 366, 1069, 9152, 1171, 1570, 526, 1114, 1672, 11, 287, 30992, 11, 262, 11565, 15928, 8197, 257, 14195, 357, 43, 13, 1129, 1129, 11, 279, 13, 767, 6659, 8, 543, 2672, 11, 366, 1169, 6827, 286, 1918, 815, 307, 10945, 1626, 262, 7968, 7356, 11, 611, 11282, 11, 290, 4306, 1626, 281, 30685, 1474, 262, 7356, 526, 383, 11565, 1099, 10431, 262, 1957, 16570, 284, 14983, 8318, 284, 3925, 357, 23073, 1957, 4290, 8, 4150, 339, 4762, 815, 4973, 262, 10938, 11, 475, 262, 15059, 10203, 784, 329, 2972, 3840, 784, 3360, 6699, 8318, 284, 3925, 508, 2227, 284, 2342, 13, 11565, 30632, 5952, 706, 30992, 547, 407, 366, 11377, 1, 780, 484, 547, 5952, 2157, 4838, 7714, 11, 290, 262, 2276, 1171, 373, 407, 10431, 284, 5262, 13, 50257, 818, 262, 11565, 286, 30992, 11, 508, 10158, 503, 8318, 284, 4973, 30632, 30, 50260, 1169, 1957, 16570, 50258], [-100, 1290, 10183, 636, 286, 262, 1181, 262, 609, 48406, 993, 7258, 18692, 38777, 2233, 284, 1877, 32025, 290, 4457, 1029, 10101, 26, 617, 3006, 286, 262, 10183, 636, 286, 262, 1181, 389, 523, 5894, 645, 28459, 318, 1043, 588, 262, 3837, 360, 4015, 286, 3409, 282, 323, 43120, 13, 1318, 389, 734, 18778, 4258, 14123, 1043, 287, 262, 10183, 636, 286, 262, 1181, 25, 6964, 18692, 357, 33, 1199, 8, 290, 15226, 18692, 357, 48802, 74, 8, 543, 389, 47543, 416, 2811, 5079, 5951, 2233, 284, 5400, 287, 22910, 13, 1318, 318, 257, 6801, 6516, 287, 262, 3504, 286, 262, 1181, 1022, 262, 734, 4457, 1180, 45225, 422, 262, 7627, 290, 7421, 26, 428, 6516, 318, 262, 2441, 27768, 16264, 416, 257, 13110, 1022, 45273, 1335, 4258, 14123, 13, 50257, 464, 6801, 6516, 1022, 7627, 290, 7421, 318, 1444, 644, 30, 50260, 1169, 2441, 27768, 50258], [-100, 3615, 286, 4486, 8096, 2092, 14479, 13, 554, 6733, 370, 2120, 11, 257, 20555, 7533, 3199, 287, 29679, 58, 68, 60, 262, 38702, 20555, 3554, 15819, 16322, 1671, 30830, 6886, 262, 7681, 357, 37114, 10200, 366, 1169, 749, 35578, 2700, 287, 262, 995, 4943, 290, 7189, 25, 366, 464, 2679, 1230, 6875, 2346, 3492, 329, 8030, 2316, 351, 262, 7570, 4479, 11, 9472, 262, 3594, 1906, 24111, 1175, 24003, 15997, 257, 1175, 1028, 262, 15889, 7570, 4479, 13, 383, 7570, 661, 290, 262, 1762, 661, 286, 4486, 423, 281, 1393, 287, 12174, 262, 3594, 1175, 1410, 526, 50257, 8241, 2227, 1175, 351, 262, 7570, 4479, 30, 50260, 15823, 1906, 24111, 1175, 24003, 50258], [-100, 67, 4997, 271, 1275, 417, 571, 7240, 357, 49022, 329, 366, 5189, 262, 18725, 571, 378, 33060, 12340, 44532, 515, 319, 1987, 2795, 15904, 11, 33446, 262, 7835, 4564, 338, 6761, 286, 11503, 306, 18725, 571, 1590, 287, 262, 2688, 13, 770, 551, 15539, 605, 373, 3194, 287, 262, 7765, 286, 19057, 2873, 11, 618, 262, 7835, 4564, 373, 14085, 290, 2710, 1710, 867, 890, 12, 10217, 6593, 13, 20876, 306, 18725, 571, 1590, 318, 3177, 257, 12883, 2138, 621, 40937, 11, 290, 617, 550, 2938, 326, 340, 1244, 307, 18397, 13, 554, 2882, 284, 777, 2683, 11, 262, 13258, 30384, 8789, 262, 12883, 355, 257, 890, 12, 10217, 3357, 351, 2041, 6817, 287, 262, 7835, 4564, 13, 383, 551, 15539, 605, 9295, 45744, 4997, 271, 1275, 417, 571, 7240, 422, 1987, 2795, 15904, 11, 19623, 262, 4569, 4564, 7743, 11, 326, 18725, 571, 1590, 318, 281, 7306, 1181, 290, 4477, 284, 307, 13677, 329, 7993, 7835, 17345, 13, 15248, 571, 1590, 6194, 4340, 262, 3950, 286, 262, 13239, 286, 1793, 10371, 3660, 3592, 13, 383, 11503, 306, 18725, 571, 1590, 318, 7173, 6692, 284, 262, 5360, 859, 2470, 33060, 13, 2102, 11, 1141, 465, 45443, 22460, 3362, 13889, 373, 3177, 14431, 287, 29256, 27538, 284, 7264, 8591, 291, 1634, 286, 17345, 508, 2227, 284, 2666, 262, 5360, 45744, 4997, 1181, 11, 257, 2292, 543, 373, 22188, 17687, 416, 1757, 3362, 2873, 287, 7169, 290, 20534, 276, 287, 262, 13540, 19507, 3854, 326, 691, 262, 26850, 460, 287, 15313, 5917, 7264, 8591, 291, 1634, 13, 50257, 2061, 3188, 220, 286, 15904, 21068, 262, 4928, 338, 12046, 286, 18725, 571, 1590, 287, 262, 33060, 30, 50260, 50, 11736, 67, 4997, 271, 1275, 417, 571, 7240, 50258], [-100, 3071, 2497, 5433, 7179, 14952, 7018, 284, 262, 2097, 286, 13815, 11, 257, 2383, 5373, 1201, 11, 257, 614, 878, 262, 3071, 11, 262, 2097, 286, 18651, 550, 3804, 262, 29667, 8492, 6922, 326, 833, 2367, 791, 507, 287, 262, 1578, 7526, 714, 645, 2392, 16565, 1637, 284, 1814, 262, 3071, 9964, 290, 9400, 286, 7179, 14952, 13, 383, 15030, 20162, 547, 19084, 284, 14634, 428, 13657, 2551, 351, 4165, 5520, 13, 383, 6001, 286, 12266, 13110, 373, 284, 10400, 257, 7699, 329, 12688, 286, 8411, 284, 4781, 262, 761, 284, 6211, 262, 9601, 791, 507, 13, 2750, 35145, 11, 7452, 351, 262, 5471, 286, 262, 4387, 833, 2367, 791, 507, 11, 262, 12266, 1230, 3804, 262, 9601, 36060, 1769, 2191, 284, 1249, 9601, 791, 507, 284, 1814, 7179, 14952, 1752, 517, 13, 50257, 8241, 3804, 262, 29667, 8492, 30, 50260, 35965, 36060, 1769, 2191, 50258], [-100, 10140, 1703, 372, 301, 4837, 6108, 326, 422, 8069, 284, 8235, 11, 3139, 5474, 422, 1542, 3834, 12, 40461, 2678, 39398, 720, 23451, 9374, 11, 23353, 883, 7027, 6, 7097, 20250, 13, 357, 464, 2482, 11, 6241, 287, 34445, 393, 25822, 2478, 11, 423, 587, 29563, 287, 4583, 416, 17646, 337, 1192, 333, 31530, 2014, 554, 262, 1339, 286, 5478, 11, 530, 286, 262, 5087, 329, 428, 4069, 373, 1964, 24842, 11, 290, 262, 1109, 326, 649, 6905, 1690, 34012, 2180, 1230, 338, 10622, 306, 6492, 6798, 13, 770, 10085, 2828, 284, 38305, 511, 5129, 10522, 11, 503, 286, 3151, 286, 597, 2003, 1033, 9219, 341, 13, 554, 6273, 11, 7740, 34790, 884, 355, 1778, 18647, 78, 338, 968, 8284, 1690, 1718, 257, 2005, 319, 1597, 8945, 393, 2810, 3403, 329, 2478, 11, 832, 6884, 4896, 11, 1099, 290, 1502, 11, 3503, 13, 50257, 13828, 4837, 9713, 9253, 422, 3834, 12, 40461, 2678, 422, 8069, 284, 8235, 30, 50260, 21009, 286, 10140, 1703, 372, 301, 50258], [-100, 14807, 357, 52, 25374, 8, 389, 27953, 3006, 351, 2440, 10101, 621, 326, 286, 262, 7346, 2858, 13, 383, 2440, 10101, 389, 257, 1255, 286, 3220, 24774, 286, 262, 12347, 1657, 416, 7876, 5696, 884, 355, 48292, 290, 10017, 11, 543, 423, 2793, 435, 3077, 418, 290, 2440, 4894, 32484, 621, 883, 287, 262, 3288, 2858, 13, 317, 15836, 2446, 286, 3753, 27362, 262, 471, 25374, 1245, 318, 284, 7521, 6832, 290, 9725, 2330, 290, 4618, 7150, 13, 8554, 777, 5050, 11, 257, 25345, 366, 24494, 5348, 1, 1430, 287, 5401, 5652, 468, 13301, 326, 7876, 10101, 714, 307, 5322, 416, 6702, 513, 22074, 34, 379, 281, 6108, 1575, 286, 1294, 3, 16, 2997, 11, 3501, 6108, 2472, 5079, 4034, 286, 1294, 3, 38612, 1510, 422, 5322, 1633, 12, 31448, 278, 3484, 290, 11409, 10653, 13, 50257, 2061, 318, 257, 835, 284, 4646, 262, 1029, 10101, 2727, 287, 7876, 4894, 14807, 30, 50260, 79, 2913, 6832, 290, 9725, 2330, 290, 4618, 7150, 50258]]\n"
     ]
    }
   ],
   "source": [
    "samples = {k: v for k,v in tokenized_traindata[:8].items()}\n",
    "\n",
    "for i in ['input_ids', 'attention_mask', 'labels']:\n",
    "    print([x for x in samples[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 267]),\n",
       " 'attention_mask': torch.Size([8, 267]),\n",
       " 'labels': torch.Size([8, 267])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_valdata[i] for i in range(8)])\n",
    "batch\n",
    "{k: v.shape for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for training\n",
    "\n",
    "tokenized_traindata.set_format(\"torch\")\n",
    "tokenized_valdata.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_traindata, shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_valdata, batch_size=1, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([1, 169]),\n",
       " 'attention_mask': torch.Size([1, 169]),\n",
       " 'labels': torch.Size([1, 169])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 15 15:53:33 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   27C    P2    56W / 250W |   2109MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8     8W / 250W |      2MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     11380      C   ...nvs/droid-slam/bin/python     2107MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66.8771, device='cuda:0', grad_fn=<NllLossBackward0>) torch.Size([1, 169, 50263])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=tensor(66.8771, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-62.4476, -56.4331, -61.8573,  ...,  -9.1667, -11.4215,  -7.9637],\n",
       "         [-57.2493, -53.6497, -62.5995,  ...,  -9.8462, -12.4266,  -8.5500],\n",
       "         [-67.1257, -61.1422, -66.2666,  ..., -11.9947, -12.4417,  -8.8907],\n",
       "         ...,\n",
       "         [-23.9955, -14.2964, -26.4468,  ...,  -4.5412,  -7.2286,  -4.1709],\n",
       "         [-20.6872,  -9.9268, -24.3683,  ...,  -4.8183,  -6.7345,  -3.0585],\n",
       "         [-62.0287, -59.1233, -65.1496,  ...,  -8.5997, -15.0292,  -8.7982]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-0.6044,  1.5019,  0.8428,  ..., -1.1459, -0.3331,  1.1480],\n",
       "          [-1.5426,  1.5870,  0.8776,  ..., -1.2772, -0.3419,  0.6149],\n",
       "          [-2.1703,  1.4136,  2.3538,  ..., -0.3721, -1.1331,  2.1192],\n",
       "          ...,\n",
       "          [-0.6799,  1.6874, -0.1866,  ..., -1.3821,  0.8103, -0.0079],\n",
       "          [-0.0137,  1.5672,  1.5908,  ..., -1.6083,  0.6818, -0.9684],\n",
       "          [-0.2486,  1.8216,  2.9931,  ..., -1.2143,  0.0871,  1.1772]],\n",
       "\n",
       "         [[ 0.3921,  0.6642, -0.9662,  ...,  0.0411,  1.5787, -0.2748],\n",
       "          [-0.3900, -1.1281, -1.3679,  ...,  0.2045,  4.2222,  0.3017],\n",
       "          [-0.4060, -1.2603, -0.5230,  ...,  0.1024,  5.4077,  0.7001],\n",
       "          ...,\n",
       "          [ 0.4174, -0.4559,  1.7340,  ...,  1.0660,  2.2716,  2.0939],\n",
       "          [ 1.7079,  1.1360, -0.1145,  ...,  2.0462,  4.5481, -1.6683],\n",
       "          [-0.9067, -0.6763, -0.4998,  ...,  0.0463,  3.7569,  0.5333]],\n",
       "\n",
       "         [[ 0.0314, -0.3949,  0.7873,  ..., -0.9859, -1.6137,  0.6351],\n",
       "          [ 0.8590,  0.4325,  0.7100,  ..., -2.2531,  0.0591,  1.3262],\n",
       "          [ 0.5421,  0.5561,  1.0178,  ..., -2.4622,  0.9040,  1.4748],\n",
       "          ...,\n",
       "          [-1.4941, -0.9274, -0.4082,  ..., -0.5420,  1.1510,  0.6783],\n",
       "          [ 0.1253, -0.2097,  1.0154,  ..., -0.2864,  0.8038,  0.6638],\n",
       "          [ 0.8978,  0.2423,  1.1179,  ..., -1.5263,  2.7119,  1.8510]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.6611, -0.1594, -0.3926,  ...,  0.0088,  0.8673,  0.5375],\n",
       "          [-0.0462,  0.2001, -0.0597,  ...,  1.3706,  0.2484,  0.4612],\n",
       "          [-0.0344,  0.1022, -0.0235,  ...,  1.3289,  0.3845,  0.5162],\n",
       "          ...,\n",
       "          [-0.9263,  0.3329,  0.1816,  ...,  0.3863, -0.8269,  0.2738],\n",
       "          [ 0.2336,  0.2130, -0.2530,  ...,  0.0395, -0.6580,  0.4002],\n",
       "          [-0.2093,  0.1034, -0.1868,  ...,  0.5699, -0.0578, -0.2251]],\n",
       "\n",
       "         [[ 1.3586,  1.2025, -0.4263,  ...,  0.0761,  0.9604, -1.4965],\n",
       "          [ 0.3505,  0.2962,  0.7667,  ..., -0.8449,  1.2059, -0.7798],\n",
       "          [ 0.4399,  0.8562, -0.8415,  ..., -0.5808,  0.8329, -0.2993],\n",
       "          ...,\n",
       "          [-1.1082, -0.9490,  0.6056,  ..., -1.8707,  1.1209, -1.4571],\n",
       "          [-0.7278, -0.2416,  1.3647,  ..., -1.4144,  0.8004, -1.0854],\n",
       "          [-0.2669,  0.3310,  0.5053,  ..., -1.8900,  0.9077, -0.6787]],\n",
       "\n",
       "         [[ 0.6989,  0.2026,  0.3234,  ..., -0.6773,  0.2175,  1.5425],\n",
       "          [ 0.2304, -0.0930,  0.2565,  ...,  0.3056, -0.4916,  0.9721],\n",
       "          [-0.0461,  0.2046, -0.5600,  ...,  0.1917, -0.3308,  1.3291],\n",
       "          ...,\n",
       "          [-0.9560, -0.9923,  0.1431,  ...,  0.0826,  0.0616,  0.2086],\n",
       "          [ 0.5488, -0.8945, -0.1387,  ..., -0.2771,  0.3911,  0.3354],\n",
       "          [-0.1025, -0.3882, -0.3123,  ..., -1.0089,  0.2796, -0.0802]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-1.7120e-02, -5.8425e-03,  1.8310e-01,  ..., -2.3781e-02,\n",
       "            8.7113e-02,  3.1480e-02],\n",
       "          [ 2.1206e-01, -1.3838e-01,  1.5111e-01,  ..., -7.6420e-02,\n",
       "            3.8613e-01, -3.2399e-02],\n",
       "          [ 4.7318e-01, -4.3247e-02, -1.2047e-01,  ..., -1.8218e-01,\n",
       "            3.2037e-01, -5.0764e-02],\n",
       "          ...,\n",
       "          [ 3.1542e-01,  8.3607e-02,  7.9668e-02,  ..., -2.7388e-01,\n",
       "           -2.2329e-01,  8.4649e-02],\n",
       "          [ 2.2929e-02, -6.0206e-02,  2.7621e-01,  ..., -2.6379e-01,\n",
       "            4.6428e-02, -7.8241e-02],\n",
       "          [ 1.9343e-01,  7.2531e-02,  5.8552e-02,  ..., -1.4767e-01,\n",
       "            2.3148e-01,  1.3644e-01]],\n",
       "\n",
       "         [[ 5.7544e-01,  1.1739e-01, -2.2699e-01,  ..., -6.9929e-01,\n",
       "           -2.5952e-01,  2.0289e-01],\n",
       "          [ 3.9519e-01, -1.5400e-02,  4.6904e-03,  ...,  2.4353e-01,\n",
       "           -2.1579e-01, -8.8423e-03],\n",
       "          [ 6.4741e-01, -1.3277e-01,  3.4973e-01,  ..., -4.1474e-02,\n",
       "            6.1248e-03, -2.8148e-02],\n",
       "          ...,\n",
       "          [ 7.4152e-01, -1.7299e-01,  2.3301e-01,  ...,  2.6888e-01,\n",
       "            1.4353e-01,  4.0230e-01],\n",
       "          [ 5.6743e-01,  2.5084e-02, -2.5268e-01,  ...,  9.0299e-02,\n",
       "            2.9403e-01, -1.4967e-01],\n",
       "          [ 8.3230e-01,  6.6123e-02,  2.6269e-02,  ...,  5.6119e-02,\n",
       "            5.5980e-01,  1.2144e-01]],\n",
       "\n",
       "         [[-2.3429e-02, -1.1108e-01, -3.4485e-02,  ..., -1.8433e-02,\n",
       "            2.9200e-02, -5.4200e-02],\n",
       "          [-2.9548e-02, -3.2282e-01, -1.9013e-01,  ..., -3.6135e-01,\n",
       "           -8.2020e-02,  9.3719e-02],\n",
       "          [-4.2433e-01,  1.5064e-01,  9.8618e-02,  ..., -1.7020e-01,\n",
       "            1.9756e-01,  2.5444e-01],\n",
       "          ...,\n",
       "          [ 8.2270e-02, -1.4822e-01,  1.2756e-01,  ...,  1.2569e-02,\n",
       "            3.1933e-01,  2.2300e-01],\n",
       "          [-4.0995e-01, -3.6975e-01,  4.1027e-01,  ..., -4.6431e-02,\n",
       "            1.3862e-02,  1.8867e-01],\n",
       "          [-1.1691e-01,  2.2840e-01, -5.5567e-02,  ..., -2.0095e-01,\n",
       "            4.0924e-01,  5.7923e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3673e-01, -1.8679e-01, -3.3080e-02,  ..., -1.4099e-01,\n",
       "           -2.6678e-02, -9.4937e-02],\n",
       "          [-4.8737e-02,  4.1481e-01, -1.9859e-01,  ...,  2.3720e-01,\n",
       "            1.5084e-01, -2.4383e-01],\n",
       "          [ 2.6774e-01,  1.1180e-01,  4.0863e-03,  ..., -2.9020e-03,\n",
       "           -1.8925e-01,  3.0844e-01],\n",
       "          ...,\n",
       "          [ 4.8635e-01, -4.0239e-01,  1.2333e-01,  ...,  1.5640e-01,\n",
       "           -2.9326e-01, -2.4867e-01],\n",
       "          [ 1.5092e-01, -2.3225e-02,  1.8600e-01,  ..., -1.5743e-01,\n",
       "            2.1366e-01, -9.2591e-02],\n",
       "          [ 6.5913e-01,  4.2366e-01,  1.3140e-01,  ..., -7.3730e-02,\n",
       "           -9.8899e-02, -1.3169e-01]],\n",
       "\n",
       "         [[ 5.1450e-02, -1.0378e-01, -2.1035e-01,  ...,  2.0653e-01,\n",
       "            1.6017e-01, -3.1673e-02],\n",
       "          [-3.6347e-01, -1.0946e-01,  1.6017e-01,  ..., -4.4158e-01,\n",
       "            5.9410e-02,  2.5566e-01],\n",
       "          [-1.0510e-01,  1.0287e-01, -3.5718e-02,  ...,  7.5950e-02,\n",
       "            1.5177e-01,  9.8971e-02],\n",
       "          ...,\n",
       "          [ 6.2312e-02,  4.2077e-01, -2.8464e-01,  ..., -3.0343e-02,\n",
       "           -8.6323e-02, -2.2536e-01],\n",
       "          [ 2.4218e-01, -4.8163e-01, -3.7766e-02,  ...,  3.3465e-02,\n",
       "           -1.3824e-01, -9.9405e-02],\n",
       "          [-3.3943e-02,  5.2377e-02,  1.0676e-01,  ..., -1.6867e-02,\n",
       "            8.8642e-02, -6.6987e-02]],\n",
       "\n",
       "         [[-7.9845e-04, -3.6068e-01,  9.5070e-02,  ...,  6.7973e-02,\n",
       "           -2.6418e-01, -9.6287e-02],\n",
       "          [ 1.2641e-01,  1.3625e-01, -5.5577e-02,  ..., -1.3828e-02,\n",
       "            1.6495e-01,  3.5011e-01],\n",
       "          [ 6.8586e-02, -1.2762e-01, -1.0237e-01,  ...,  2.0326e-01,\n",
       "           -2.5097e-02, -3.2505e-02],\n",
       "          ...,\n",
       "          [-2.6875e-01,  1.3998e-01,  1.9530e-01,  ..., -1.0958e-01,\n",
       "           -3.0298e-01,  1.7740e-01],\n",
       "          [-9.5879e-02,  3.1330e-01, -5.7403e-02,  ...,  4.0923e-02,\n",
       "            1.6018e-01,  6.0392e-02],\n",
       "          [-1.4052e-01, -3.9660e-02,  2.4607e-01,  ...,  2.2313e-01,\n",
       "           -1.8728e-01, -1.3525e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-4.5821e-01,  1.6526e+00, -1.4548e+00,  ...,  1.3777e+00,\n",
       "           -7.3163e-01,  1.0253e+00],\n",
       "          [ 1.2836e+00,  1.2337e+00, -7.9822e-01,  ..., -6.1648e-01,\n",
       "           -1.6441e+00, -7.2417e-01],\n",
       "          [ 3.7847e-01,  8.7550e-01, -1.9927e+00,  ..., -6.7525e-01,\n",
       "           -6.1790e-01,  2.0118e+00],\n",
       "          ...,\n",
       "          [ 3.4986e+00, -1.1903e+00,  1.9568e+00,  ..., -2.9783e+00,\n",
       "           -1.6734e+00,  9.7103e-01],\n",
       "          [ 4.5090e+00, -2.1715e+00,  3.1999e+00,  ..., -5.1578e+00,\n",
       "           -2.9951e+00,  1.1726e+00],\n",
       "          [ 3.4045e+00, -1.4302e+00,  2.8374e+00,  ..., -3.7543e+00,\n",
       "           -1.8834e+00,  2.2539e+00]],\n",
       "\n",
       "         [[-9.5329e-01, -3.5872e-01, -5.1764e-01,  ..., -2.1456e-01,\n",
       "            1.0632e+00, -5.1818e-01],\n",
       "          [-8.9726e-01, -3.7589e-02, -1.6372e+00,  ..., -1.0179e+00,\n",
       "            1.7360e-02,  5.8101e-01],\n",
       "          [ 6.0372e-01,  4.1285e-01, -1.5108e+00,  ..., -6.7043e-01,\n",
       "            6.5812e-01, -1.9318e-02],\n",
       "          ...,\n",
       "          [-1.1374e+00, -5.4066e-01,  3.2243e+00,  ..., -3.8515e-02,\n",
       "           -8.8978e-01, -8.5630e-01],\n",
       "          [-1.0330e+00,  4.0347e-01,  4.5423e+00,  ..., -4.1268e-01,\n",
       "           -2.2889e+00, -3.0191e-01],\n",
       "          [-1.9995e-01,  5.3371e-01,  4.1559e+00,  ...,  5.7674e-03,\n",
       "           -1.6681e+00, -8.1579e-02]],\n",
       "\n",
       "         [[ 3.4305e-01,  7.0297e-02,  6.7381e-02,  ..., -1.2599e+00,\n",
       "            3.0633e-01, -1.9506e-01],\n",
       "          [-1.8379e-01,  3.4291e-01, -5.2131e-02,  ..., -1.1388e+00,\n",
       "            4.6205e-02,  7.6270e-01],\n",
       "          [-9.9028e-03, -1.1286e-01, -8.9777e-01,  ..., -1.0690e+00,\n",
       "            1.9703e-01,  5.0086e-01],\n",
       "          ...,\n",
       "          [-2.2533e-01,  8.4997e-02,  1.3819e-01,  ...,  1.0683e+00,\n",
       "           -1.2188e-02,  1.7413e-02],\n",
       "          [-5.4431e-02,  3.9538e-02,  3.5311e-01,  ...,  1.9807e+00,\n",
       "           -5.9462e-04, -5.0773e-02],\n",
       "          [-2.5634e-01, -3.6247e-02, -1.6740e-02,  ...,  1.2547e+00,\n",
       "           -5.6105e-02,  1.5918e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.4166e-01, -1.6598e-01, -8.0250e-01,  ..., -9.7245e-01,\n",
       "            4.3853e-01, -3.9228e-01],\n",
       "          [-7.6747e-01,  8.2173e-01,  1.7445e+00,  ...,  6.4203e-01,\n",
       "           -8.3869e-02, -1.0633e+00],\n",
       "          [-8.6021e-01,  1.3677e+00,  2.9249e+00,  ...,  6.6893e-01,\n",
       "           -7.7026e-01,  1.3800e+00],\n",
       "          ...,\n",
       "          [-2.7178e+00, -2.0224e-01,  1.5990e+00,  ...,  8.3888e-01,\n",
       "            1.6564e+00, -5.7107e-01],\n",
       "          [ 6.7331e-03, -2.1169e+00,  1.2970e+00,  ..., -6.9968e-01,\n",
       "            3.3805e+00,  1.1529e+00],\n",
       "          [ 2.8787e-01, -6.3027e-01,  9.4376e-01,  ..., -2.4305e-01,\n",
       "            2.0129e+00,  5.0402e-01]],\n",
       "\n",
       "         [[-1.1430e+00, -2.8059e+00,  1.1589e-01,  ...,  1.7502e+00,\n",
       "            1.5417e+00, -1.5467e+00],\n",
       "          [ 9.3499e-02,  7.0608e-01, -3.6827e-01,  ..., -7.6516e-01,\n",
       "            6.8587e-01, -2.1616e-01],\n",
       "          [ 5.2473e-01,  6.8613e-01, -9.3351e-02,  ..., -8.1436e-01,\n",
       "           -1.1128e-02,  2.9739e-01],\n",
       "          ...,\n",
       "          [-4.2501e-01,  1.8095e-02,  2.1707e-01,  ...,  1.9120e-02,\n",
       "           -5.6150e-01,  5.4751e-01],\n",
       "          [-9.2754e-01,  2.8140e-01,  6.0939e-01,  ...,  1.0503e-01,\n",
       "           -1.1614e-01,  2.5399e-01],\n",
       "          [-7.2129e-01,  3.7009e-01,  4.4509e-01,  ..., -3.4285e-01,\n",
       "           -3.1591e-01,  4.3075e-02]],\n",
       "\n",
       "         [[ 8.6506e-01,  1.8695e+00,  7.0206e-01,  ..., -8.6349e-01,\n",
       "            1.7327e-01,  6.8515e-01],\n",
       "          [-6.4279e-01,  2.4436e+00,  1.1555e+00,  ...,  1.5331e+00,\n",
       "           -4.9618e-01, -4.9914e-01],\n",
       "          [-2.5658e+00, -4.3055e-01,  3.8250e-01,  ...,  1.5482e+00,\n",
       "           -1.4692e+00,  1.1219e-01],\n",
       "          ...,\n",
       "          [-1.9835e-01,  1.8763e+00, -5.0807e-01,  ...,  2.9419e-01,\n",
       "           -6.2520e-01,  1.2494e+00],\n",
       "          [-2.6654e-02, -4.2329e-01, -3.2103e-01,  ...,  3.2277e+00,\n",
       "           -6.9071e-01,  2.3051e+00],\n",
       "          [-1.1295e-01, -1.3827e-01, -5.0655e-01,  ..., -2.0273e-02,\n",
       "           -7.3320e-01, -3.2976e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 2.6328e-01, -9.2846e-02, -2.6152e-02,  ...,  1.8476e-01,\n",
       "           -1.2180e-01, -4.9941e-03],\n",
       "          [ 4.6383e-01, -2.5483e-01, -2.7556e-01,  ..., -3.4014e-01,\n",
       "            1.6787e-01, -3.5067e-01],\n",
       "          [ 6.7266e-01,  2.0086e-01, -5.1418e-01,  ..., -3.7211e-01,\n",
       "           -2.1362e-02, -2.3600e-01],\n",
       "          ...,\n",
       "          [-3.4963e-01,  2.5565e-01, -7.9279e-02,  ..., -3.9785e-01,\n",
       "           -5.5067e-01,  2.4130e-01],\n",
       "          [-7.3907e-01,  5.0599e-01, -2.3182e-01,  ..., -6.6000e-01,\n",
       "           -4.6380e-01,  2.7523e-01],\n",
       "          [ 2.2681e-01, -1.3295e-01, -1.8580e-01,  ..., -1.6746e-01,\n",
       "           -2.7205e-01, -1.1274e-01]],\n",
       "\n",
       "         [[ 1.7551e-01, -6.3037e-02, -7.6310e-02,  ...,  2.8363e-02,\n",
       "           -4.1731e-01,  4.5290e-02],\n",
       "          [ 2.3349e-01, -1.1028e-01,  9.9313e-02,  ..., -3.9475e-01,\n",
       "           -3.8794e-02, -3.0637e-01],\n",
       "          [ 5.6487e-01, -4.4010e-04, -7.3187e-02,  ..., -2.2312e-01,\n",
       "           -8.4583e-02, -6.5441e-01],\n",
       "          ...,\n",
       "          [ 3.6727e-02, -6.1612e-01, -1.9295e-01,  ...,  1.0477e-01,\n",
       "           -1.1297e+00, -1.9933e-01],\n",
       "          [ 1.6797e-01,  9.5369e-01, -6.0195e-01,  ...,  5.1780e-01,\n",
       "           -5.2859e-01,  4.4380e-01],\n",
       "          [-2.8070e-01, -5.4846e-01,  1.8991e-01,  ...,  1.4320e-01,\n",
       "           -7.3957e-02, -7.2099e-01]],\n",
       "\n",
       "         [[ 1.7188e-02, -2.2161e-01, -5.0347e-02,  ..., -5.6473e-01,\n",
       "            2.1071e-04, -5.8281e-02],\n",
       "          [ 4.6034e-01,  9.5429e-02,  2.6091e-01,  ..., -4.9354e-01,\n",
       "           -1.5389e-01,  1.3450e-01],\n",
       "          [ 2.9942e-01, -1.9354e-01,  6.9511e-01,  ..., -5.0458e-01,\n",
       "           -2.5146e-01, -1.6988e-01],\n",
       "          ...,\n",
       "          [-4.5632e-01,  1.4911e-01, -3.8015e-01,  ..., -1.4193e+00,\n",
       "           -2.4739e-01, -9.5004e-02],\n",
       "          [-5.2564e-01, -1.7731e-01,  9.2832e-01,  ..., -2.1101e+00,\n",
       "            2.4239e-01,  8.0378e-02],\n",
       "          [-7.9666e-01, -7.5687e-02, -3.2878e-01,  ..., -1.3331e+00,\n",
       "           -2.2414e-01, -4.7079e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.3299e-01,  5.1877e-01, -3.0177e-04,  ...,  8.8135e-02,\n",
       "           -1.1473e+00, -8.2604e-02],\n",
       "          [-9.1250e-02, -5.5960e-02,  7.6260e-01,  ...,  2.1672e-01,\n",
       "           -9.2764e-01, -5.3218e-01],\n",
       "          [-7.8328e-01, -6.9851e-01, -4.5177e-01,  ...,  5.2004e-01,\n",
       "           -7.0260e-01, -2.8598e-01],\n",
       "          ...,\n",
       "          [ 1.8221e-01, -3.0118e-01,  3.1927e-01,  ..., -1.0522e-01,\n",
       "           -1.0127e-01,  1.7792e-02],\n",
       "          [ 7.0730e-01,  2.1909e-01, -1.7761e-01,  ..., -1.0134e+00,\n",
       "            5.0266e-01,  8.2432e-02],\n",
       "          [ 2.6057e-01, -7.7300e-02,  5.4476e-01,  ..., -2.2082e-01,\n",
       "            6.4296e-01, -3.2547e-01]],\n",
       "\n",
       "         [[ 3.2507e-01, -2.0029e-01, -1.5448e-01,  ...,  3.1514e-01,\n",
       "           -3.5603e+00, -1.2597e-01],\n",
       "          [-1.9311e-03, -2.7791e-01,  5.6199e-02,  ...,  6.8449e-02,\n",
       "            7.7431e-02, -2.4553e-02],\n",
       "          [-2.1254e-01,  2.8943e-01,  9.7597e-01,  ..., -8.0462e-02,\n",
       "            1.9884e-01,  7.3717e-02],\n",
       "          ...,\n",
       "          [-4.7842e-02, -1.4280e-01, -4.0941e-01,  ..., -2.4980e-01,\n",
       "            1.2591e-01,  9.7007e-02],\n",
       "          [-4.7626e-01, -2.2897e-01,  1.3201e-01,  ..., -1.6539e-01,\n",
       "           -2.8334e-02,  4.1749e-01],\n",
       "          [-4.0086e-01,  5.2328e-02, -1.6688e-01,  ..., -3.4471e-02,\n",
       "            1.7421e-01, -1.8156e-01]],\n",
       "\n",
       "         [[ 1.1657e-01, -8.2152e-02,  5.9751e-02,  ..., -2.1544e-01,\n",
       "            1.4300e-01,  6.9702e-02],\n",
       "          [-3.4976e-01,  3.9881e-01,  2.3310e-02,  ..., -2.0612e-01,\n",
       "            2.8132e-01,  2.6080e-01],\n",
       "          [ 2.6938e-02, -2.3890e-01,  4.3054e-01,  ...,  5.9104e-01,\n",
       "           -2.0109e-01, -7.2251e-02],\n",
       "          ...,\n",
       "          [-1.8877e-02, -4.0566e-01, -2.1649e-01,  ..., -2.6057e-01,\n",
       "            2.2475e-01, -3.2368e-01],\n",
       "          [-1.1393e-04, -6.7054e-02,  1.2008e-02,  ...,  9.2262e-02,\n",
       "            2.8331e-01, -3.0570e-02],\n",
       "          [ 2.5962e-01, -3.3610e-02,  3.4064e-01,  ...,  9.4056e-02,\n",
       "           -2.0828e-01, -2.0059e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.3224e-01, -1.1073e+00,  2.7167e-01,  ..., -5.9555e-01,\n",
       "           -1.8258e-01, -8.1383e-04],\n",
       "          [ 1.0294e+00, -2.9889e+00, -7.9997e-01,  ..., -6.6354e-01,\n",
       "           -5.3817e-01,  4.2808e-03],\n",
       "          [ 6.7955e-01, -2.1791e+00, -3.8799e-01,  ..., -1.6888e-01,\n",
       "            7.1909e-02, -4.1668e-01],\n",
       "          ...,\n",
       "          [-7.0704e-01,  1.5233e-01,  2.6368e-02,  ..., -1.0399e+00,\n",
       "            6.1138e-01, -5.1330e-01],\n",
       "          [ 5.1252e-01,  1.4529e+00, -2.0484e-01,  ..., -1.3876e+00,\n",
       "           -6.1244e-01, -7.2113e-01],\n",
       "          [ 6.9769e-02,  1.5583e+00, -9.3112e-01,  ...,  1.3921e-01,\n",
       "            6.1667e-01, -3.2273e-01]],\n",
       "\n",
       "         [[-5.4655e-01,  4.0567e-01, -3.4861e-01,  ...,  1.1353e+00,\n",
       "           -5.5978e-01, -4.1473e-01],\n",
       "          [-1.5243e+00, -1.7320e+00, -9.3374e-01,  ...,  3.4474e-01,\n",
       "           -3.6423e-01, -1.1851e+00],\n",
       "          [-1.3070e+00, -7.5025e-01, -5.6577e-01,  ...,  2.5237e+00,\n",
       "            1.2265e+00,  5.9879e-01],\n",
       "          ...,\n",
       "          [ 4.2286e-02, -1.0002e+00, -1.5394e+00,  ..., -2.2983e+00,\n",
       "           -4.0955e-01, -1.9515e-01],\n",
       "          [-1.2110e+00, -1.5839e+00,  2.8658e-01,  ..., -1.3915e+00,\n",
       "            8.0877e-01, -7.4996e-03],\n",
       "          [-8.1528e-01, -1.6956e+00, -5.7405e-01,  ..., -2.2736e-02,\n",
       "            1.6774e+00, -8.9955e-01]],\n",
       "\n",
       "         [[ 1.1715e+00,  2.9965e+00,  3.6423e+00,  ...,  6.5617e-01,\n",
       "            1.6715e+00, -7.2916e-01],\n",
       "          [-2.2921e+00,  1.1581e+00, -3.2684e+00,  ..., -2.7891e+00,\n",
       "            2.6555e+00,  4.3335e-01],\n",
       "          [-2.9200e+00,  1.0206e+00, -2.7552e+00,  ..., -2.1681e+00,\n",
       "            3.1143e+00,  1.3140e+00],\n",
       "          ...,\n",
       "          [-6.3862e+00, -2.0238e+00, -5.7583e+00,  ...,  3.0041e+00,\n",
       "           -1.3630e+00, -2.5708e+00],\n",
       "          [-7.0979e+00, -1.7627e+00, -6.9923e+00,  ...,  4.3306e+00,\n",
       "           -2.5523e+00, -4.2838e+00],\n",
       "          [-6.8426e+00, -2.1727e+00, -6.1370e+00,  ...,  3.5972e+00,\n",
       "           -2.6003e+00, -3.2427e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.2636e+00, -2.6828e+00, -2.7163e+00,  ...,  9.1606e-01,\n",
       "            4.0675e-01,  2.6233e+00],\n",
       "          [-1.8601e+00,  1.2998e+00, -7.3158e-01,  ...,  8.4449e-02,\n",
       "           -1.3922e+00, -7.9095e-01],\n",
       "          [-3.3661e+00,  1.5380e+00, -3.8271e-01,  ..., -5.5504e-01,\n",
       "           -1.4297e+00, -1.0013e+00],\n",
       "          ...,\n",
       "          [ 2.9080e+00,  4.7514e+00,  3.6519e+00,  ..., -3.4316e+00,\n",
       "            1.2318e+00, -2.8622e+00],\n",
       "          [ 2.6561e+00,  5.4910e+00,  5.4356e+00,  ..., -3.5242e+00,\n",
       "            1.6430e+00, -4.4890e+00],\n",
       "          [ 2.2474e+00,  4.8339e+00,  4.2790e+00,  ..., -2.8992e+00,\n",
       "            1.5482e+00, -2.8967e+00]],\n",
       "\n",
       "         [[ 1.7490e+00,  4.2128e-01,  8.6183e-01,  ...,  6.3470e-02,\n",
       "           -1.0515e+00, -3.0004e-01],\n",
       "          [ 2.3589e+00,  1.1676e+00,  9.3771e-01,  ...,  3.0807e-01,\n",
       "           -1.4422e+00, -1.2974e+00],\n",
       "          [ 2.7581e+00,  3.7763e-01,  7.8334e-01,  ...,  8.9568e-01,\n",
       "           -1.9760e+00, -9.8269e-01],\n",
       "          ...,\n",
       "          [-2.2512e+00, -7.1131e-01, -4.4140e-02,  ..., -4.4241e-01,\n",
       "            2.5675e+00,  4.2220e-01],\n",
       "          [-2.6016e+00, -7.1906e-01,  3.7389e-01,  ..., -1.8158e-01,\n",
       "            2.8241e+00, -2.3726e-01],\n",
       "          [-2.5328e+00, -8.7881e-01,  3.4880e-02,  ...,  3.4698e-01,\n",
       "            2.1167e+00,  2.4294e-01]],\n",
       "\n",
       "         [[-2.4405e-01,  1.5964e-01, -5.2728e-01,  ...,  2.7290e-01,\n",
       "            2.7945e-01,  1.8378e-01],\n",
       "          [-1.1161e+00,  1.4087e+00, -2.3181e-01,  ...,  1.3619e-01,\n",
       "            3.2425e-01, -5.7346e-01],\n",
       "          [ 3.9605e-01,  1.9042e-01, -1.4326e-01,  ...,  2.0561e-01,\n",
       "            1.4811e-01,  8.3291e-01],\n",
       "          ...,\n",
       "          [ 1.2471e-01,  9.3538e-01, -8.4398e-01,  ...,  4.9932e-01,\n",
       "            1.7640e-01,  4.6854e-01],\n",
       "          [ 3.0754e-01,  3.7122e-01, -4.9498e-01,  ..., -1.9736e-01,\n",
       "            7.8916e-01, -3.7274e-01],\n",
       "          [ 1.2564e-01,  5.3023e-02, -7.0388e-01,  ...,  5.4274e-01,\n",
       "            1.2497e+00,  1.4397e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.3091e-02,  2.8700e-02, -1.1814e-01,  ...,  1.8804e-02,\n",
       "            2.8452e-02, -5.2383e-01],\n",
       "          [-2.1425e-01, -8.9110e-01,  2.3496e-01,  ..., -1.0463e+00,\n",
       "            2.0202e-01,  8.3694e-01],\n",
       "          [ 3.3495e-01,  3.8399e-01, -8.1857e-01,  ...,  2.9594e-01,\n",
       "            9.0578e-03,  4.2867e-01],\n",
       "          ...,\n",
       "          [-1.0483e+00, -5.9567e-02,  8.3539e-01,  ..., -1.3660e-01,\n",
       "            8.9813e-03, -9.2369e-02],\n",
       "          [-1.1324e-01,  1.6244e-02,  4.5189e-01,  ...,  8.9781e-04,\n",
       "            8.0987e-02, -8.1415e-02],\n",
       "          [-2.9784e-02,  4.0644e-01, -4.9385e-01,  ...,  4.2193e-01,\n",
       "           -3.3672e-01, -4.5193e-01]],\n",
       "\n",
       "         [[ 1.0166e-02, -1.5681e-02,  5.3387e-02,  ..., -5.5638e-02,\n",
       "            1.3048e-02,  3.2088e-02],\n",
       "          [ 1.0151e+00,  5.5152e-01,  7.6061e-02,  ..., -6.1064e-01,\n",
       "           -3.0856e-01, -9.0702e-01],\n",
       "          [ 1.3623e-01, -2.7074e-01,  5.6517e-01,  ..., -2.5614e-01,\n",
       "           -1.0115e-02, -3.0013e-01],\n",
       "          ...,\n",
       "          [-1.9502e-01, -7.4030e-01,  1.0009e+00,  ...,  9.0451e-01,\n",
       "           -2.4962e-01, -5.7805e-01],\n",
       "          [ 5.9895e-02, -7.5916e-02,  8.9584e-01,  ..., -4.0520e-01,\n",
       "            2.2377e-01, -1.7557e-01],\n",
       "          [ 7.9911e-01, -7.7830e-02,  6.0515e-02,  ..., -2.0409e-01,\n",
       "            2.9867e-01, -5.8236e-01]],\n",
       "\n",
       "         [[ 1.8459e-02, -8.1356e-01, -2.9270e-02,  ...,  7.8567e-02,\n",
       "            3.0799e-03, -3.1376e-02],\n",
       "          [-1.5397e-01, -1.7032e+00, -3.7210e-02,  ..., -4.0525e-01,\n",
       "            7.1971e-02,  5.6523e-01],\n",
       "          [ 6.1951e-02, -1.6899e+00,  8.0399e-02,  ..., -2.7019e-01,\n",
       "           -4.4886e-02, -1.5198e-01],\n",
       "          ...,\n",
       "          [-4.0308e-01, -1.8496e+00,  6.1439e-02,  ...,  3.4037e-01,\n",
       "           -5.2854e-01, -3.8310e-01],\n",
       "          [-2.2747e-01, -1.4603e+00, -2.4012e-01,  ...,  8.0536e-02,\n",
       "            1.1013e-02, -4.7352e-01],\n",
       "          [-2.7053e-01, -1.4778e+00, -1.7157e-04,  ...,  2.3808e-02,\n",
       "           -1.9671e-03,  4.8867e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.8009e-02, -6.9106e-02,  1.2782e+00,  ..., -7.0118e-02,\n",
       "            1.9843e-01, -2.9528e-02],\n",
       "          [ 3.6540e-02, -9.7749e-01,  1.9270e+00,  ...,  1.1348e-01,\n",
       "           -5.3669e-01,  2.5594e-01],\n",
       "          [-3.4538e-01, -1.4580e-01,  2.4840e+00,  ...,  3.0269e-01,\n",
       "            2.4007e-01,  3.1596e-01],\n",
       "          ...,\n",
       "          [-5.4614e-01, -2.8136e-01,  9.1104e-01,  ...,  5.6475e-01,\n",
       "            4.5240e-01, -3.5136e-01],\n",
       "          [-8.3108e-02, -1.2998e-01,  1.3841e+00,  ...,  8.5104e-03,\n",
       "            3.7539e-01,  1.1760e-01],\n",
       "          [ 2.8794e-03,  1.8175e-01,  2.0217e+00,  ...,  1.2916e-01,\n",
       "            5.1096e-01,  6.3678e-02]],\n",
       "\n",
       "         [[-1.9321e-02, -1.0876e-01, -1.6662e-01,  ...,  1.2427e-01,\n",
       "            1.4078e-01,  2.2612e-01],\n",
       "          [ 7.2486e-01, -3.4344e-01, -1.0278e-02,  ...,  5.0311e-01,\n",
       "           -3.6491e-01, -8.6530e-01],\n",
       "          [-1.0526e-01,  6.0949e-01,  2.6594e-01,  ..., -2.0638e-01,\n",
       "            5.4666e-01, -2.8830e-01],\n",
       "          ...,\n",
       "          [ 4.7904e-01,  3.3104e-01, -2.5212e-02,  ..., -3.3986e-01,\n",
       "            4.1155e-01, -8.7742e-02],\n",
       "          [ 5.4428e-03,  5.0778e-01, -1.9647e-01,  ..., -3.1397e-01,\n",
       "           -4.2318e-01, -2.4695e-01],\n",
       "          [-1.7847e-01,  1.7476e+00, -6.6482e-02,  ..., -4.0266e-01,\n",
       "           -6.2089e-01,  1.6058e-01]],\n",
       "\n",
       "         [[ 1.7523e-02, -8.5219e-03,  2.1139e-02,  ..., -5.7303e-03,\n",
       "            2.0654e-01,  2.5783e-02],\n",
       "          [ 2.1652e-01, -4.8247e-01, -5.6098e-02,  ...,  6.3879e-02,\n",
       "           -1.8386e+00,  1.5806e+00],\n",
       "          [ 3.4434e-01, -2.9230e-01, -1.1687e-01,  ...,  2.9378e-01,\n",
       "           -1.5159e+00, -6.5657e-02],\n",
       "          ...,\n",
       "          [-4.1178e-01, -6.5472e-01, -5.0753e-01,  ..., -2.4665e-01,\n",
       "            5.5977e-01,  1.5193e-01],\n",
       "          [-1.1813e-01, -3.2247e-02,  3.9099e-01,  ..., -4.4291e-01,\n",
       "            5.2629e-01,  2.2647e-01],\n",
       "          [ 2.5686e-01, -3.7328e-01, -1.2254e-01,  ..., -2.5191e-02,\n",
       "            5.9545e-01,  2.7462e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.7595e-02, -2.1940e-01,  1.5185e-01,  ..., -8.8828e-01,\n",
       "            7.4700e-01, -1.1821e+00],\n",
       "          [ 1.1453e-01, -1.7328e-01,  2.5252e-01,  ...,  5.4106e-02,\n",
       "            5.1365e-01, -2.5873e-01],\n",
       "          [-5.0940e-02,  1.6164e-02,  8.1418e-01,  ..., -3.2973e-01,\n",
       "           -6.6279e-01,  7.3583e-01],\n",
       "          ...,\n",
       "          [-1.8099e-01, -5.1942e-01,  8.4573e-02,  ...,  2.4258e+00,\n",
       "            6.6644e-01,  2.8883e+00],\n",
       "          [-6.4715e-01, -1.2895e-01,  1.5251e-01,  ...,  2.7550e+00,\n",
       "           -2.4541e-01,  2.5146e+00],\n",
       "          [-5.7164e-02,  3.7565e-02,  4.0834e-01,  ...,  1.0067e+00,\n",
       "            3.5760e-01,  2.6548e+00]],\n",
       "\n",
       "         [[ 8.1353e-01,  1.8227e-01, -1.3752e-02,  ..., -1.6057e-01,\n",
       "           -1.0946e+00, -1.9014e-01],\n",
       "          [ 6.0266e-01, -1.0889e+00, -7.8439e-01,  ..., -1.1748e+00,\n",
       "            4.2523e+00,  3.1231e-01],\n",
       "          [-5.9760e-01,  4.5347e-01,  4.5679e-01,  ..., -7.6284e-01,\n",
       "            3.6689e+00,  3.0711e+00],\n",
       "          ...,\n",
       "          [-1.3344e+00,  1.5332e+00,  4.8611e-01,  ..., -1.3257e+00,\n",
       "            4.6737e+00,  4.1225e+00],\n",
       "          [-3.4493e+00,  9.1317e-01,  3.1126e-01,  ..., -8.7216e-01,\n",
       "            6.1726e+00,  5.0328e+00],\n",
       "          [-4.2669e+00,  1.9936e+00,  6.3538e-01,  ..., -2.5289e+00,\n",
       "            3.2445e+00,  4.7796e+00]],\n",
       "\n",
       "         [[ 3.4345e-01, -3.6786e-01, -3.1283e-01,  ...,  3.5582e-01,\n",
       "            1.4256e+00,  2.6629e-01],\n",
       "          [ 8.5627e-01, -5.1161e+00, -4.6217e-02,  ..., -2.7685e+00,\n",
       "           -1.8667e+00, -5.3279e+00],\n",
       "          [ 1.5756e-02, -6.8063e+00,  8.5312e-02,  ..., -3.5779e+00,\n",
       "           -5.3421e+00, -6.8485e+00],\n",
       "          ...,\n",
       "          [ 6.8042e+00,  4.6852e+00,  8.8488e+00,  ...,  2.3347e+00,\n",
       "           -1.1128e+01, -2.5018e+00],\n",
       "          [ 8.0064e+00,  5.9272e+00,  1.0733e+01,  ...,  1.9810e+00,\n",
       "           -1.2480e+01, -4.0659e+00],\n",
       "          [ 7.0290e+00,  4.4055e+00,  1.0124e+01,  ...,  1.6387e+00,\n",
       "           -1.2119e+01, -2.8380e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2473e-01,  1.7684e+00,  5.3023e-01,  ...,  2.5734e-01,\n",
       "            4.6143e-01, -1.6659e+00],\n",
       "          [-1.6662e-01, -4.2094e+00,  7.8764e-01,  ..., -1.9876e+00,\n",
       "            1.5076e+00,  5.5005e+00],\n",
       "          [ 3.2556e-01, -7.1669e+00, -3.0995e-01,  ..., -2.9134e+00,\n",
       "           -2.4692e+00,  8.4665e+00],\n",
       "          ...,\n",
       "          [-1.1236e-01, -2.0098e+00, -5.4651e+00,  ..., -3.2812e+00,\n",
       "           -1.8244e-01,  5.7148e+00],\n",
       "          [-8.1750e-01, -2.8271e+00, -6.6891e+00,  ..., -5.1085e+00,\n",
       "           -1.0928e+00,  6.0101e+00],\n",
       "          [-6.9311e-01, -3.6958e+00, -5.7240e+00,  ..., -3.6332e+00,\n",
       "           -6.1003e-02,  3.8876e+00]],\n",
       "\n",
       "         [[ 6.0162e-02, -2.3614e-02,  1.3587e-01,  ..., -8.7250e-02,\n",
       "           -8.1524e-02, -1.2992e-01],\n",
       "          [ 8.3783e-02, -4.4566e-02, -4.4174e-01,  ..., -2.3203e+00,\n",
       "           -6.6358e-01, -6.8798e-01],\n",
       "          [-7.9666e-01, -9.9658e-01, -8.3194e-01,  ..., -1.0807e+00,\n",
       "            5.7171e-01,  1.4142e-01],\n",
       "          ...,\n",
       "          [ 1.0483e-01,  1.1314e+00, -5.4886e-01,  ...,  1.2051e+00,\n",
       "            8.1737e-01,  5.6809e-01],\n",
       "          [-4.2102e-01,  4.6206e-01,  8.1936e-01,  ...,  6.6617e-01,\n",
       "            5.7633e-01, -7.0061e-01],\n",
       "          [-1.0126e+00, -2.0753e-01, -2.8871e-01,  ..., -9.3674e-02,\n",
       "            8.2679e-01,  1.0753e-02]],\n",
       "\n",
       "         [[ 3.9063e-01, -6.8947e-02,  1.8937e+00,  ..., -2.3168e-01,\n",
       "           -2.1753e-01, -1.0081e+00],\n",
       "          [ 2.3952e+00,  1.7042e+00, -2.0217e+00,  ...,  4.3797e-01,\n",
       "            5.9534e-01,  2.7217e+00],\n",
       "          [ 3.6918e+00,  1.5648e+00, -3.5633e+00,  ..., -2.7638e-01,\n",
       "            7.2989e-01,  4.4998e+00],\n",
       "          ...,\n",
       "          [-6.2854e+00,  1.2109e+00, -8.8034e+00,  ..., -4.2330e+00,\n",
       "            1.9374e+00,  5.3356e+00],\n",
       "          [-6.9608e+00,  1.0011e+00, -1.0260e+01,  ..., -4.5560e+00,\n",
       "            1.9907e+00,  4.9698e+00],\n",
       "          [-5.8113e+00,  1.3239e+00, -8.7711e+00,  ..., -5.1987e+00,\n",
       "            2.5053e+00,  4.7026e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0432,  0.0634, -0.0113,  ...,  0.0198,  0.1041,  0.0348],\n",
       "          [-0.1480, -1.2067, -0.2711,  ...,  0.0124,  0.3116, -0.8324],\n",
       "          [ 0.8863, -0.8607,  0.1729,  ..., -0.3498, -0.8838, -0.3621],\n",
       "          ...,\n",
       "          [-0.5274, -0.1464,  1.2164,  ...,  0.1916,  0.9412, -0.8356],\n",
       "          [ 0.4261,  0.9192,  0.5510,  ..., -0.3838,  0.4818,  0.1641],\n",
       "          [ 0.9835,  0.2400,  0.7892,  ..., -0.3250,  0.1739,  0.7361]],\n",
       "\n",
       "         [[-0.0368, -0.0063,  0.0779,  ..., -0.0457, -0.0341, -0.0446],\n",
       "          [ 0.3008, -0.3141, -0.9911,  ...,  1.1243,  0.0960,  0.1417],\n",
       "          [-0.1345,  0.3842, -1.1807,  ...,  0.1917,  0.2765,  0.0976],\n",
       "          ...,\n",
       "          [-0.4531,  0.1961,  0.5893,  ..., -0.6966,  0.3403, -0.1481],\n",
       "          [-0.5473, -0.4011,  0.4169,  ..., -0.2099, -0.5245,  0.0638],\n",
       "          [ 0.1046, -0.1582,  0.1731,  ...,  0.0483, -0.5662,  0.3398]],\n",
       "\n",
       "         [[ 0.0341, -0.1003, -0.0431,  ..., -0.0243,  0.0842, -0.1480],\n",
       "          [ 0.2461, -0.9376,  0.3031,  ...,  0.0930,  0.5727, -0.4078],\n",
       "          [-0.3832,  0.0187, -0.5270,  ..., -0.2109, -0.2754,  0.7977],\n",
       "          ...,\n",
       "          [ 0.2625,  0.5010,  0.2748,  ...,  0.2104,  0.0299,  0.0536],\n",
       "          [-0.3540,  0.2099,  0.1109,  ...,  0.4947, -0.0434,  0.6388],\n",
       "          [ 0.3433,  0.4028, -0.3456,  ..., -0.1876,  0.3466,  0.1420]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0233,  0.1183, -0.0081,  ..., -0.0232,  0.0580, -0.0381],\n",
       "          [ 0.6246, -1.0089,  0.2073,  ...,  0.6000, -0.5782,  0.1365],\n",
       "          [ 0.7049,  0.1339,  0.2296,  ..., -0.3607, -0.9103, -0.4588],\n",
       "          ...,\n",
       "          [ 0.0622,  0.2464,  0.4011,  ...,  0.0630, -0.2032, -0.0949],\n",
       "          [-0.0351,  0.5733, -0.0426,  ..., -0.0199, -0.0294,  0.4099],\n",
       "          [ 0.0717,  0.4727,  0.2571,  ..., -0.3367, -0.2301, -0.2044]],\n",
       "\n",
       "         [[-0.1503, -0.1214, -0.0652,  ..., -0.2352, -0.0141, -0.0431],\n",
       "          [ 0.5631, -0.1348,  0.7548,  ...,  0.6246, -0.5106,  0.9756],\n",
       "          [ 0.9989, -0.0030, -0.2798,  ...,  0.5357,  0.0250, -0.4544],\n",
       "          ...,\n",
       "          [ 0.2772, -0.1264,  0.3332,  ...,  0.9551,  0.1961, -0.1604],\n",
       "          [-0.0969, -0.6638,  0.0137,  ...,  0.5393,  0.6808, -0.0681],\n",
       "          [ 0.3541, -0.0064, -0.2626,  ...,  1.2030,  0.3729, -0.9194]],\n",
       "\n",
       "         [[ 0.1133, -0.0820, -0.0301,  ..., -0.0175, -0.0881, -0.0952],\n",
       "          [-0.9128,  0.5428, -0.7071,  ...,  0.3143,  0.3366,  0.6922],\n",
       "          [ 0.2389,  0.1967, -0.3173,  ...,  0.3218, -0.1777, -0.0384],\n",
       "          ...,\n",
       "          [-0.5659,  0.0562,  0.4541,  ...,  0.3806,  0.5207, -1.3138],\n",
       "          [-0.0595,  0.1739,  0.5818,  ...,  0.2164,  0.8508,  0.1419],\n",
       "          [ 0.5320,  0.4526, -0.4667,  ...,  0.3627, -0.4281,  0.0394]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[-8.7219e-01, -1.4097e-01,  3.3535e-01,  ..., -9.7384e-01,\n",
       "            1.7120e-02, -2.9394e+00],\n",
       "          [ 6.8185e-01,  1.2834e+00, -3.6201e+00,  ..., -2.0611e+00,\n",
       "           -1.7651e+00,  7.5198e+00],\n",
       "          [ 2.6754e+00,  9.7206e-01, -2.0291e+00,  ..., -2.3692e+00,\n",
       "           -3.2991e+00,  9.2442e+00],\n",
       "          ...,\n",
       "          [ 5.5315e+00, -1.7256e-01,  1.7784e+00,  ...,  4.6802e+00,\n",
       "           -4.5573e-01,  6.1285e+00],\n",
       "          [ 7.0465e+00, -6.7706e-02,  7.9871e-01,  ...,  5.1784e+00,\n",
       "            2.2573e-01,  7.4158e+00],\n",
       "          [ 8.0618e+00,  1.4976e-02,  5.1940e-01,  ...,  4.9411e+00,\n",
       "            3.4587e-01,  6.8910e+00]],\n",
       "\n",
       "         [[ 3.6381e-01, -6.2048e-02,  4.8148e-01,  ..., -1.3239e-01,\n",
       "           -6.5631e-02, -2.2119e+00],\n",
       "          [-4.3362e-01,  1.1199e+00,  2.5553e+00,  ..., -5.2028e-01,\n",
       "           -1.6449e+00,  5.3031e+00],\n",
       "          [-2.5053e-01,  8.6736e-01,  3.5052e+00,  ..., -8.5443e-01,\n",
       "           -2.1608e+00,  7.1182e+00],\n",
       "          ...,\n",
       "          [ 4.1822e-01,  7.1740e-01,  5.5920e-01,  ..., -4.3816e+00,\n",
       "            1.5650e+00,  8.7650e+00],\n",
       "          [-1.0134e-01,  1.8778e+00,  8.8194e-01,  ..., -3.7326e+00,\n",
       "            1.0407e+00,  9.8312e+00],\n",
       "          [ 2.5580e+00,  1.1964e+00,  1.3321e+00,  ..., -1.4662e+00,\n",
       "            5.1050e-01,  1.0006e+01]],\n",
       "\n",
       "         [[ 1.1765e-01, -6.4750e-01, -2.1731e-01,  ...,  1.4896e-01,\n",
       "            2.7050e-01, -1.6662e-01],\n",
       "          [-6.5738e-01,  1.6819e+00,  1.5315e+00,  ...,  1.0191e+00,\n",
       "            1.3892e+00, -9.0406e-04],\n",
       "          [-2.6021e-01,  2.8068e+00,  1.6927e+00,  ..., -1.0152e+00,\n",
       "           -1.8044e-01, -1.0798e+00],\n",
       "          ...,\n",
       "          [ 7.6686e-01,  2.1566e+00, -8.4995e-01,  ...,  2.0779e-01,\n",
       "           -4.9765e-01,  1.7669e+00],\n",
       "          [ 1.5822e+00,  3.1690e+00, -7.2941e-01,  ...,  9.8527e-01,\n",
       "           -5.4492e-01,  6.1825e-01],\n",
       "          [ 1.7492e-01,  1.3617e+00, -1.2006e+00,  ..., -4.4924e-02,\n",
       "            3.6815e-01, -1.0960e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.8923e-01,  2.6247e-02,  8.4157e-03,  ...,  1.2489e+00,\n",
       "            6.6181e-02,  1.7649e+00],\n",
       "          [ 7.6977e-01, -1.6048e+00, -6.0455e-01,  ..., -2.1760e+00,\n",
       "           -6.6488e-01, -1.2259e+00],\n",
       "          [ 5.2968e-01, -6.2717e-01, -2.1855e+00,  ..., -3.3733e+00,\n",
       "           -1.4427e+00, -9.0210e-01],\n",
       "          ...,\n",
       "          [-2.8724e+00, -9.3903e-01, -1.5492e-01,  ..., -4.1153e+00,\n",
       "           -9.3698e-01, -6.8386e+00],\n",
       "          [-1.1390e+00,  3.7457e-01, -4.2176e-01,  ..., -3.8977e+00,\n",
       "           -1.5700e+00, -7.1235e+00],\n",
       "          [-1.1007e+00, -7.2860e-01, -1.7989e+00,  ..., -5.3576e+00,\n",
       "           -7.4031e-01, -8.2736e+00]],\n",
       "\n",
       "         [[-3.3144e-01, -1.1842e-01,  2.2151e-01,  ...,  2.5297e-01,\n",
       "           -3.2933e-02,  1.4042e-02],\n",
       "          [ 7.8925e-01, -3.5116e-01,  1.3084e+00,  ...,  1.4838e+00,\n",
       "            1.2783e+00, -1.6413e-01],\n",
       "          [ 4.1027e-01, -2.9857e-01,  7.5710e-01,  ..., -9.9812e-01,\n",
       "            6.9559e-01,  5.2879e-01],\n",
       "          ...,\n",
       "          [-5.5300e-01, -8.0379e-01, -7.6925e-01,  ...,  5.5535e-01,\n",
       "           -1.6013e+00, -4.5937e-01],\n",
       "          [ 6.3660e-01, -4.5094e-01, -3.5225e-01,  ..., -1.0822e+00,\n",
       "            1.2513e-01,  2.2078e-02],\n",
       "          [ 7.8446e-01,  1.0118e+00, -8.8035e-02,  ..., -5.0668e-01,\n",
       "            2.1992e-02,  7.7098e-01]],\n",
       "\n",
       "         [[ 3.3961e+00,  2.1107e+00, -2.1578e+00,  ..., -2.8350e+00,\n",
       "           -3.9167e+00, -1.1819e+00],\n",
       "          [-1.7131e+00,  9.7868e-01,  6.7479e+00,  ..., -2.0930e+00,\n",
       "            7.6766e+00, -9.0573e-01],\n",
       "          [-3.6817e+00,  6.1935e-01,  6.5683e+00,  ..., -5.2100e+00,\n",
       "            1.4133e+01, -1.2133e+00],\n",
       "          ...,\n",
       "          [-2.1356e+00, -9.1639e+00,  3.8805e+00,  ...,  7.5615e+00,\n",
       "           -1.1474e+00,  9.6350e+00],\n",
       "          [-1.8698e+00, -1.1699e+01,  3.7073e+00,  ...,  1.0061e+01,\n",
       "            8.2297e-01,  9.8738e+00],\n",
       "          [-3.2334e+00, -1.1182e+01,  1.1956e+00,  ...,  6.5800e+00,\n",
       "            4.8334e+00,  6.9558e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.9979e-05, -4.2062e-02,  2.0988e-02,  ...,  5.9412e-02,\n",
       "            3.1133e-02,  7.2696e-02],\n",
       "          [ 3.4678e-01, -3.7832e-01,  4.5906e-01,  ..., -3.0458e-01,\n",
       "           -7.4555e-02, -3.8666e-01],\n",
       "          [ 1.6278e-01, -2.0741e-02,  6.5342e-01,  ..., -2.4351e-01,\n",
       "           -1.6658e-01, -1.3214e-01],\n",
       "          ...,\n",
       "          [-6.7431e-01, -3.9630e-01,  5.8392e-01,  ..., -2.6894e-01,\n",
       "            1.9387e-01, -4.3132e-01],\n",
       "          [ 2.5397e-01, -5.6112e-01, -1.3306e-01,  ..., -1.3670e-01,\n",
       "           -1.7561e-01, -4.9618e-01],\n",
       "          [ 5.7068e-01, -6.8389e-01,  5.9369e-01,  ..., -3.1977e-01,\n",
       "           -6.8977e-01, -5.2455e-02]],\n",
       "\n",
       "         [[-6.6587e-02, -2.0593e-02, -1.4013e-01,  ..., -4.7419e-02,\n",
       "            4.1925e-02, -1.6505e-02],\n",
       "          [-6.1500e-01, -2.1203e-01,  7.3746e-01,  ...,  1.6674e-01,\n",
       "           -9.7900e-02, -2.5439e-01],\n",
       "          [ 1.1308e-01,  4.5767e-01,  4.3956e-01,  ..., -1.0462e-01,\n",
       "           -2.8892e-01,  1.6829e-01],\n",
       "          ...,\n",
       "          [-3.9827e-01, -2.0281e-01,  6.2460e-01,  ...,  1.8959e-01,\n",
       "           -8.7016e-02, -2.5861e-01],\n",
       "          [-3.7149e-01, -6.4281e-01, -9.0833e-02,  ...,  2.5971e-02,\n",
       "            5.4698e-02, -2.6713e-01],\n",
       "          [ 5.0360e-01,  2.6260e-02,  4.1996e-02,  ..., -2.9456e-01,\n",
       "           -2.3887e-01, -1.1656e-01]],\n",
       "\n",
       "         [[ 6.9917e-02,  8.7795e-02,  8.1410e-02,  ...,  1.9220e-02,\n",
       "           -8.7865e-02, -2.4641e-03],\n",
       "          [-2.6309e-01,  3.5780e-01,  2.6656e-01,  ...,  1.1173e-01,\n",
       "            1.3535e+00,  3.1381e-01],\n",
       "          [ 1.3144e+00,  4.0258e-01, -1.1868e+00,  ..., -1.3956e+00,\n",
       "           -6.1454e-01,  1.1848e-01],\n",
       "          ...,\n",
       "          [ 5.7447e-02, -4.6393e-01, -4.8713e-01,  ...,  1.2301e+00,\n",
       "            3.2727e-01, -9.3358e-01],\n",
       "          [-3.3179e-01, -6.8308e-01,  5.4813e-01,  ...,  2.3227e-01,\n",
       "           -7.5907e-02, -1.7953e-01],\n",
       "          [-1.2050e-01,  1.1231e-01, -9.4805e-02,  ..., -2.1929e-01,\n",
       "           -1.0144e-01, -5.2189e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.3838e-03,  7.6653e-02, -7.8059e-02,  ...,  4.5003e-02,\n",
       "            4.6333e-02, -1.3531e-01],\n",
       "          [ 5.5919e-02,  7.9710e-01,  4.9820e-01,  ..., -5.0301e-01,\n",
       "            6.3585e-02,  1.5530e-01],\n",
       "          [-2.4933e-01,  6.0207e-01,  6.1526e-01,  ...,  4.8247e-01,\n",
       "            4.0131e-01,  2.1906e-01],\n",
       "          ...,\n",
       "          [-7.4447e-01,  2.0670e-01, -1.4176e-01,  ..., -4.7955e-01,\n",
       "            4.0917e-01, -2.7991e-01],\n",
       "          [-8.3519e-01, -2.2551e-01, -3.1996e-01,  ..., -3.8681e-01,\n",
       "           -1.9191e-01, -9.5654e-01],\n",
       "          [ 2.4914e-01,  9.4938e-02,  1.1176e+00,  ..., -5.0589e-01,\n",
       "           -3.3383e-01, -1.3522e-01]],\n",
       "\n",
       "         [[-1.2284e-01, -4.8630e-02,  1.0919e-01,  ..., -6.5264e-02,\n",
       "            4.8155e-02, -1.6271e-02],\n",
       "          [-6.7415e-02, -6.9291e-01, -6.2480e-01,  ...,  8.5045e-01,\n",
       "           -2.9717e-01,  5.5862e-01],\n",
       "          [-7.9211e-01, -8.9412e-01, -5.7407e-01,  ...,  3.8015e-01,\n",
       "            1.0910e+00,  1.2793e-01],\n",
       "          ...,\n",
       "          [ 1.0131e+00,  4.5311e-01,  1.5813e-01,  ...,  1.9441e+00,\n",
       "            6.0211e-02, -2.3853e-01],\n",
       "          [ 7.3518e-01,  3.7749e-01, -1.9096e-02,  ...,  1.0842e+00,\n",
       "            4.6030e-01, -6.8899e-01],\n",
       "          [-6.5430e-01,  4.2304e-01,  1.1602e-03,  ...,  9.3425e-01,\n",
       "            7.5014e-01, -5.8989e-02]],\n",
       "\n",
       "         [[-2.4307e-03, -8.5426e-03, -2.5716e-02,  ..., -2.9411e-02,\n",
       "            7.6575e-03, -8.3960e-03],\n",
       "          [-8.8071e-01, -3.9043e-01,  4.2479e-01,  ...,  1.9642e-01,\n",
       "            1.7370e-01, -1.2954e-01],\n",
       "          [-3.2875e-01,  8.9390e-03,  4.2404e-01,  ...,  8.1279e-02,\n",
       "            4.9719e-01,  7.6003e-01],\n",
       "          ...,\n",
       "          [ 1.4484e-01, -3.6057e-01,  5.7902e-02,  ..., -3.4351e-01,\n",
       "           -4.7357e-01, -6.0901e-01],\n",
       "          [-5.3475e-01, -3.6943e-01,  7.3180e-01,  ..., -4.9171e-03,\n",
       "           -6.3304e-01, -1.0254e-01],\n",
       "          [ 2.0578e-01, -1.7867e-01,  6.8455e-01,  ...,  3.5834e-02,\n",
       "           -1.3837e-02,  2.7631e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.9661e-02, -2.9991e-01,  2.2535e-01,  ...,  1.6900e+00,\n",
       "           -2.0796e-01, -8.0234e-02],\n",
       "          [ 8.9967e-01,  1.3272e+00, -3.4039e-02,  ..., -3.3739e+00,\n",
       "            3.8342e-02, -1.4744e+00],\n",
       "          [-1.6781e+00,  3.3536e-01, -8.0218e-02,  ..., -4.8235e+00,\n",
       "           -1.4574e+00, -1.5369e+00],\n",
       "          ...,\n",
       "          [-1.0259e+00, -2.4526e+00,  4.0958e-02,  ..., -3.9164e+00,\n",
       "            7.4988e-01, -1.6773e+00],\n",
       "          [-2.6527e+00, -1.3938e+00,  1.0479e+00,  ..., -5.2049e+00,\n",
       "            8.5339e-01, -2.3013e+00],\n",
       "          [-2.1762e+00, -9.5776e-01,  1.4174e+00,  ..., -4.7879e+00,\n",
       "            1.4175e+00, -1.6650e+00]],\n",
       "\n",
       "         [[ 1.5947e-01,  9.6775e-01, -1.4084e+00,  ..., -1.1616e-01,\n",
       "            2.7009e-01,  9.2005e-01],\n",
       "          [-6.7720e-01, -5.2290e+00,  1.2997e+00,  ..., -6.6298e-01,\n",
       "            3.7552e-01, -2.3964e+00],\n",
       "          [-1.5388e+00, -5.5338e+00,  3.3794e+00,  ...,  2.2239e+00,\n",
       "           -3.2145e-01, -1.1536e+00],\n",
       "          ...,\n",
       "          [-3.7906e-01, -1.6267e+00,  8.3205e-01,  ...,  7.2139e-01,\n",
       "           -1.0624e+00, -1.5843e+00],\n",
       "          [-2.5339e+00, -3.4916e+00,  2.8419e+00,  ..., -1.4686e+00,\n",
       "            1.2591e-01, -1.8631e+00],\n",
       "          [-1.5483e+00, -1.1640e+00,  4.8285e+00,  ..., -9.0216e-01,\n",
       "            2.7025e+00, -3.2148e+00]],\n",
       "\n",
       "         [[-6.6335e-01,  2.4633e-01, -3.8255e-02,  ...,  1.6430e-01,\n",
       "            3.9824e-02, -2.9166e-01],\n",
       "          [ 2.4210e+00, -1.4369e+00,  3.4028e-01,  ..., -1.3239e+00,\n",
       "            1.5431e-01, -1.7951e+00],\n",
       "          [ 6.5052e-01,  1.3960e+00,  9.9182e-01,  ...,  4.2986e-01,\n",
       "            4.4636e-01, -1.9163e+00],\n",
       "          ...,\n",
       "          [ 4.9440e-01,  5.2590e-02,  1.1295e+00,  ...,  1.9101e+00,\n",
       "            7.8032e-01,  1.1922e-01],\n",
       "          [-1.3315e-01,  1.7407e+00, -5.6965e-01,  ...,  2.5691e+00,\n",
       "            1.0889e+00,  1.5695e+00],\n",
       "          [-5.3511e-01,  6.7750e-01,  5.2601e-01,  ...,  1.2183e+00,\n",
       "            8.2614e-01, -1.2150e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9866e-02,  1.2743e-01,  1.5476e-01,  ..., -1.0005e-01,\n",
       "            2.2341e-02,  1.6101e-01],\n",
       "          [ 5.3518e-01, -3.6289e-01,  8.5857e-01,  ...,  1.2363e+00,\n",
       "           -8.4228e-01,  8.7401e-01],\n",
       "          [ 1.0508e+00, -1.4494e-02, -2.5292e-01,  ...,  1.5082e+00,\n",
       "           -8.0198e-01,  1.0908e+00],\n",
       "          ...,\n",
       "          [-8.5660e-01,  4.9044e-01,  1.1613e+00,  ...,  1.3415e+00,\n",
       "            3.8243e-01, -4.7132e-02],\n",
       "          [-1.5056e+00,  2.3377e-01,  8.6500e-01,  ...,  7.9097e-01,\n",
       "            1.2762e+00, -1.4323e+00],\n",
       "          [-1.5015e+00, -1.0638e+00,  1.4822e-01,  ..., -1.7502e-01,\n",
       "            1.0425e+00,  9.9498e-02]],\n",
       "\n",
       "         [[-2.9904e+00,  3.9823e-01, -2.7315e-02,  ..., -4.8106e-01,\n",
       "           -3.5885e-01,  1.2312e+00],\n",
       "          [ 4.7865e+00,  1.1224e+00, -1.4313e+00,  ..., -1.7061e-01,\n",
       "            5.2020e-01, -1.5345e+00],\n",
       "          [ 5.1572e+00,  3.4122e-01, -6.9849e-01,  ..., -1.2859e+00,\n",
       "            8.7546e-01, -1.3853e-01],\n",
       "          ...,\n",
       "          [ 7.5011e+00,  8.6909e-01, -4.2791e-01,  ..., -1.5102e+00,\n",
       "            1.0381e+00, -8.4632e-01],\n",
       "          [ 8.4674e+00, -7.9868e-02,  8.0951e-01,  ..., -1.1926e+00,\n",
       "           -1.0932e-01, -3.1421e-01],\n",
       "          [ 7.7260e+00, -2.4521e+00, -5.1398e-02,  ..., -1.4334e+00,\n",
       "           -4.6374e-01, -1.0065e+00]],\n",
       "\n",
       "         [[-1.1757e-02, -2.4375e-01,  6.3970e-03,  ..., -1.8419e-01,\n",
       "            3.2802e-01,  8.4267e-02],\n",
       "          [ 8.1156e-01, -1.1835e+00,  9.5979e-02,  ..., -1.5370e+00,\n",
       "           -9.1905e-01, -6.0800e-01],\n",
       "          [ 6.3200e-01, -1.2592e+00,  2.2186e+00,  ...,  4.9046e-01,\n",
       "            1.4990e+00, -1.1316e+00],\n",
       "          ...,\n",
       "          [-4.5667e-01,  2.1902e+00, -1.4511e+00,  ...,  1.0166e+00,\n",
       "            6.8980e-01, -1.0476e-01],\n",
       "          [ 1.4313e-01, -6.8612e-02, -1.3425e+00,  ...,  7.8669e-01,\n",
       "            2.7684e-01, -1.2278e-01],\n",
       "          [-4.4481e-01, -2.3068e-01,  1.0274e-01,  ..., -1.2079e-01,\n",
       "            1.2615e+00,  6.4097e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.0278, -0.0171,  0.0107,  ..., -0.0060, -0.0357,  0.3519],\n",
       "          [ 1.7027,  1.3010,  0.3519,  ..., -1.0515,  1.3706, -0.7535],\n",
       "          [ 0.4756, -1.0806,  0.0524,  ..., -0.7230, -0.1614, -1.8934],\n",
       "          ...,\n",
       "          [ 0.5024, -1.0969,  0.6270,  ...,  0.4196,  0.1540, -0.6758],\n",
       "          [ 0.6056, -1.7872,  1.1316,  ..., -0.1912, -0.9181, -1.0628],\n",
       "          [-0.4440, -0.6983,  0.2112,  ...,  1.8553, -0.9907,  0.0148]],\n",
       "\n",
       "         [[-0.0026, -0.0164,  0.0136,  ..., -0.0168,  0.0250,  0.0085],\n",
       "          [-0.0532, -0.5299,  0.7320,  ...,  0.1515,  1.2267, -0.5032],\n",
       "          [ 0.7868, -0.0341,  0.5862,  ..., -1.1710,  2.0566, -0.6633],\n",
       "          ...,\n",
       "          [ 0.3350,  0.3735,  2.0221,  ...,  1.3072,  0.3622, -0.4577],\n",
       "          [-1.2671, -0.8441, -0.5779,  ...,  1.1580, -0.1801, -0.4946],\n",
       "          [-0.8413, -0.0970, -0.2170,  ..., -1.0042,  0.3485, -0.6214]],\n",
       "\n",
       "         [[-0.0558,  0.0092, -0.0404,  ..., -0.0420,  0.0101, -0.0868],\n",
       "          [ 0.4480, -0.0867, -0.8518,  ...,  0.1632,  0.2755, -0.1444],\n",
       "          [-0.4688,  0.2020,  0.5660,  ...,  0.3978, -0.4976, -0.0692],\n",
       "          ...,\n",
       "          [ 0.2526,  0.8264,  1.2144,  ...,  0.3926,  1.3144, -0.0026],\n",
       "          [ 2.2642,  0.5099,  0.5607,  ...,  0.6900,  0.9244, -1.0065],\n",
       "          [ 0.9959,  0.1960,  0.8383,  ...,  1.0976,  0.5606, -0.8731]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3252, -0.1862, -0.0644,  ..., -0.4839,  0.2143,  0.1075],\n",
       "          [ 2.1855, -1.1824,  0.8311,  ...,  1.1550, -1.2484, -0.0447],\n",
       "          [ 1.3886, -1.9128,  0.8063,  ...,  0.5041,  0.6175, -1.2509],\n",
       "          ...,\n",
       "          [ 0.0884, -0.1459, -0.2210,  ...,  2.2433, -1.9233,  1.3436],\n",
       "          [-0.2442,  1.1452, -0.8207,  ...,  2.0394, -0.1709,  0.5835],\n",
       "          [ 0.5985,  0.7653,  0.0532,  ...,  1.8757,  0.3765, -1.2720]],\n",
       "\n",
       "         [[-0.0782, -0.1351, -0.0451,  ..., -0.1900, -0.1413,  0.1255],\n",
       "          [ 0.3624,  0.3306, -0.5187,  ..., -0.5133,  0.9617, -0.6003],\n",
       "          [ 0.5289, -0.2995, -0.3744,  ...,  0.3291, -0.4079, -0.0427],\n",
       "          ...,\n",
       "          [ 0.5754,  0.0359, -0.2979,  ..., -0.2837,  1.0772,  0.0332],\n",
       "          [ 1.0223,  0.4743, -0.5113,  ...,  0.6653,  0.1673, -0.1843],\n",
       "          [ 0.6362, -0.9988, -0.5730,  ...,  0.7690,  0.7423,  0.9749]],\n",
       "\n",
       "         [[-0.0296, -0.0350,  0.0893,  ...,  0.0733, -0.0378,  0.0180],\n",
       "          [ 0.8909,  0.7701,  1.1240,  ..., -1.1888,  0.7545,  0.5328],\n",
       "          [ 0.4011,  0.9779, -0.0594,  ..., -0.9527, -0.1039,  0.1308],\n",
       "          ...,\n",
       "          [ 0.4394,  1.2851, -1.0671,  ..., -0.1215, -0.2198,  0.6238],\n",
       "          [ 0.3539, -0.3963, -0.6541,  ...,  0.5305,  0.0741,  0.7208],\n",
       "          [ 1.3007, -0.1634, -0.5097,  ...,  0.3748, -0.0926,  0.4843]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[-3.2208e-01,  8.5275e-01, -1.7514e-01,  ...,  1.1177e+00,\n",
       "           -1.9341e-01,  1.3243e-01],\n",
       "          [-2.3571e-01, -3.6877e+00,  4.1944e-01,  ..., -3.6678e+00,\n",
       "           -7.9583e-01,  1.7264e+00],\n",
       "          [-6.1948e-01, -5.4724e+00,  9.7863e-01,  ..., -4.2206e+00,\n",
       "            8.2906e-01,  7.9293e-01],\n",
       "          ...,\n",
       "          [-5.3290e-01, -5.3888e+00,  4.2871e+00,  ..., -5.9156e+00,\n",
       "            1.0890e+00, -2.6744e-02],\n",
       "          [-7.5039e-01, -5.4460e+00,  4.8500e+00,  ..., -6.8254e+00,\n",
       "            1.9747e+00,  5.5499e-01],\n",
       "          [-1.2223e+00, -5.2213e+00,  3.3100e+00,  ..., -5.1384e+00,\n",
       "            1.1188e+00, -8.8621e-01]],\n",
       "\n",
       "         [[ 5.3644e-02,  8.4440e-01, -6.3737e-01,  ..., -5.7998e-02,\n",
       "            2.7502e-01, -4.0376e-03],\n",
       "          [-1.0289e+00,  1.1639e+00,  4.7768e-01,  ...,  1.7064e+00,\n",
       "           -1.4261e+00, -3.6160e-01],\n",
       "          [-1.2987e+00,  5.9766e-02, -1.4057e-01,  ...,  1.2147e+00,\n",
       "           -1.0341e+00, -1.4372e-01],\n",
       "          ...,\n",
       "          [-3.0904e-01,  9.3256e-02,  6.0526e-02,  ...,  6.1763e-01,\n",
       "           -1.0725e+00,  5.1035e+00],\n",
       "          [ 5.5483e-01,  1.2138e+00, -9.3975e-02,  ...,  2.6464e-01,\n",
       "           -7.0489e-01,  2.2566e+00],\n",
       "          [ 9.9975e-01,  1.4492e+00, -4.6350e-02,  ...,  1.6099e+00,\n",
       "           -2.5937e-01,  2.6368e+00]],\n",
       "\n",
       "         [[-3.0348e-01,  1.2087e-01, -9.8670e-01,  ..., -3.5042e-01,\n",
       "           -5.9555e-02, -1.3628e-01],\n",
       "          [-5.6698e-01, -5.1902e-01,  3.7064e+00,  ...,  9.9127e-01,\n",
       "           -5.4208e-01,  5.6095e-01],\n",
       "          [-4.6237e-01, -1.4002e+00,  3.6541e+00,  ...,  5.5281e-01,\n",
       "            6.9044e-01,  1.5515e+00],\n",
       "          ...,\n",
       "          [-1.0360e+00,  8.4420e-01,  3.3314e+00,  ...,  6.0769e-01,\n",
       "            9.5619e-01, -1.8619e-01],\n",
       "          [-6.9859e-01,  2.8395e-01,  3.2566e+00,  ...,  1.8897e-01,\n",
       "            3.6693e-01, -8.0512e-01],\n",
       "          [-7.9227e-01, -7.8452e-01,  3.2630e+00,  ..., -1.1632e-02,\n",
       "            2.9672e-01, -8.8356e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7352e-01,  7.9299e-02, -7.6930e-02,  ..., -2.7303e-02,\n",
       "            2.1384e-01,  1.6526e-02],\n",
       "          [-6.6235e-01,  2.0545e-02, -1.1807e+00,  ..., -4.7453e-01,\n",
       "            1.7676e+00,  3.5403e-01],\n",
       "          [-5.0167e-02, -1.8064e-01,  1.9596e+00,  ..., -2.0379e+00,\n",
       "            6.0228e-01, -3.3494e-01],\n",
       "          ...,\n",
       "          [-1.2142e-01,  1.5538e-01,  7.0608e-01,  ..., -8.0388e-01,\n",
       "            5.7740e-01,  1.0021e-01],\n",
       "          [-8.1524e-01,  5.6246e-01,  1.0096e+00,  ..., -2.5923e-01,\n",
       "            1.1878e+00, -1.7093e+00],\n",
       "          [-8.4223e-01,  1.6899e-01,  1.3560e+00,  ...,  1.0229e+00,\n",
       "            1.2926e+00,  2.4017e+00]],\n",
       "\n",
       "         [[ 1.9372e-01,  6.3608e-02,  3.3090e-01,  ...,  4.0699e-01,\n",
       "            1.3313e-02,  2.2959e-01],\n",
       "          [-6.9988e-02, -5.9546e-01,  3.6314e-02,  ..., -9.3888e-01,\n",
       "            2.1188e-01,  1.0841e+00],\n",
       "          [ 1.7465e+00, -5.6393e-01,  8.7394e-01,  ..., -1.2733e+00,\n",
       "            5.2033e-01,  1.0250e+00],\n",
       "          ...,\n",
       "          [-7.2983e-01,  4.1676e-01, -1.1805e+00,  ..., -9.0165e-01,\n",
       "            1.5435e+00,  4.1328e-01],\n",
       "          [-5.7587e-02, -1.1852e-01, -8.0705e-02,  ..., -9.6087e-01,\n",
       "            6.5846e-01,  4.0625e-01],\n",
       "          [ 1.3766e+00, -1.7351e-01, -2.9555e-01,  ...,  6.5301e-01,\n",
       "            1.0370e+00, -1.3594e-04]],\n",
       "\n",
       "         [[-2.9852e+00,  5.4510e-01,  5.3845e-01,  ..., -9.2774e-01,\n",
       "            3.1215e-01,  2.2279e-01],\n",
       "          [ 7.3409e+00, -9.4087e-02, -3.1780e+00,  ...,  2.3124e+00,\n",
       "           -7.7247e-01, -1.7377e-02],\n",
       "          [ 9.7021e+00, -1.8725e+00, -3.5579e+00,  ...,  4.2751e+00,\n",
       "           -1.0733e+00,  9.7269e-01],\n",
       "          ...,\n",
       "          [ 8.1296e+00, -1.9681e+00,  2.6739e-01,  ...,  7.8983e+00,\n",
       "            2.2892e+00, -1.3485e+00],\n",
       "          [ 8.1243e+00, -1.1273e+00,  6.4692e-01,  ...,  9.2105e+00,\n",
       "            1.5073e+00, -5.3258e-01],\n",
       "          [ 8.1529e+00, -9.8836e-02,  1.2820e+00,  ...,  8.8680e+00,\n",
       "            4.5072e-02, -1.7779e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 4.1148e-02, -4.7304e-02,  1.6648e-02,  ..., -7.8478e-02,\n",
       "            1.2204e-02, -8.9570e-02],\n",
       "          [ 4.4937e-02,  1.0764e-01,  3.8266e-01,  ...,  6.1237e-01,\n",
       "           -4.4976e-02,  7.8260e-01],\n",
       "          [-1.9330e-01,  3.2326e-01,  1.6698e-01,  ...,  1.2709e-01,\n",
       "            1.2844e-01,  1.5872e+00],\n",
       "          ...,\n",
       "          [-3.0806e-01, -1.2085e-01,  4.5865e-01,  ...,  1.4638e-01,\n",
       "            1.1277e-01, -7.0765e-01],\n",
       "          [-5.8523e-01,  2.2671e-01, -3.4034e-02,  ..., -3.0413e-01,\n",
       "           -6.0851e-01, -1.8130e-01],\n",
       "          [ 2.7758e-02,  2.2423e-02, -1.9035e-01,  ...,  5.7909e-01,\n",
       "           -6.5633e-01, -6.1523e-01]],\n",
       "\n",
       "         [[ 6.1045e-02,  2.3243e-02, -2.9095e-02,  ..., -2.2754e-02,\n",
       "            2.9019e-03, -5.2062e-04],\n",
       "          [-3.6342e-01, -2.3632e-01,  1.0512e-01,  ...,  8.9199e-01,\n",
       "            1.4228e+00, -5.1841e-01],\n",
       "          [ 1.8384e+00, -2.1550e-01, -1.0286e-01,  ...,  2.2077e+00,\n",
       "           -5.8530e-01, -3.3595e-01],\n",
       "          ...,\n",
       "          [-7.2416e-01, -7.6713e-01,  8.8881e-01,  ...,  4.6694e-01,\n",
       "            7.9564e-01, -1.2931e+00],\n",
       "          [ 2.9768e-01, -1.4634e+00, -3.9151e-01,  ...,  3.9420e-01,\n",
       "           -2.1789e-01, -6.1750e-01],\n",
       "          [ 5.2248e-01, -1.1252e-01, -3.4713e-01,  ...,  9.7016e-02,\n",
       "            1.9023e-02, -4.8163e-01]],\n",
       "\n",
       "         [[ 8.9294e-02, -2.0742e-03,  6.7563e-03,  ...,  2.6503e-02,\n",
       "           -7.9252e-02, -6.1595e-02],\n",
       "          [-9.5200e-02,  1.2508e+00,  1.1704e-01,  ..., -2.9620e-01,\n",
       "           -1.8895e+00, -5.4668e-01],\n",
       "          [-4.8700e-01,  2.3430e-01, -8.0278e-02,  ...,  5.7436e-03,\n",
       "           -4.0504e-01,  3.8539e-01],\n",
       "          ...,\n",
       "          [ 7.3235e-02,  1.8321e+00,  2.1002e+00,  ..., -5.7046e-01,\n",
       "           -2.4810e-01, -7.0525e-01],\n",
       "          [-5.0695e-01, -2.8992e-01,  7.6259e-01,  ...,  3.4996e-02,\n",
       "            1.4292e+00, -4.1002e-01],\n",
       "          [-1.6321e+00,  9.9872e-02, -4.6142e-02,  ...,  6.9052e-03,\n",
       "           -5.1397e-01,  2.1415e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0731e-02,  1.5029e-02,  1.6406e-02,  ..., -6.9612e-02,\n",
       "           -2.5731e-02,  1.1215e-02],\n",
       "          [-1.8049e+00, -8.4060e-01, -7.5315e-01,  ...,  3.7631e-01,\n",
       "            1.4643e+00,  3.3315e-01],\n",
       "          [-5.7092e-01, -9.1762e-01,  5.0669e-01,  ...,  2.0386e-01,\n",
       "            1.2547e+00, -1.1868e+00],\n",
       "          ...,\n",
       "          [-3.2565e-01,  1.5511e+00, -7.0266e-03,  ...,  1.0728e-01,\n",
       "           -3.8146e-01, -3.5478e-01],\n",
       "          [ 5.2265e-01,  5.6952e-01, -6.3067e-01,  ...,  7.8959e-01,\n",
       "            9.5031e-01,  9.7305e-01],\n",
       "          [-1.0029e+00,  2.3093e-01, -2.8086e-01,  ..., -4.7951e-01,\n",
       "            1.0203e+00,  1.0260e+00]],\n",
       "\n",
       "         [[ 6.2557e-02, -2.4847e-03,  2.2013e-02,  ...,  2.9075e-02,\n",
       "            9.7837e-03,  1.7982e-02],\n",
       "          [-2.9632e-01,  6.1725e-01, -6.2407e-01,  ..., -2.9634e-02,\n",
       "           -1.0021e+00,  1.0579e+00],\n",
       "          [ 2.7691e-01,  1.5653e+00,  4.6767e-01,  ...,  7.3260e-01,\n",
       "           -1.3309e-01,  6.6026e-01],\n",
       "          ...,\n",
       "          [ 8.3324e-01,  5.7357e-01, -4.0798e-01,  ..., -2.0247e-01,\n",
       "            1.0044e+00,  9.2157e-01],\n",
       "          [ 1.0355e+00,  6.8308e-01, -1.5220e-01,  ..., -3.0598e-02,\n",
       "            1.1148e+00,  5.8447e-01],\n",
       "          [ 7.7284e-01,  7.5080e-01,  3.2828e-01,  ..., -1.1631e+00,\n",
       "           -2.3618e-01,  9.4149e-01]],\n",
       "\n",
       "         [[ 8.1768e-02, -1.9573e-01, -6.6325e-02,  ..., -2.3836e-02,\n",
       "            2.0036e-01, -4.3269e-02],\n",
       "          [ 1.0101e-01, -1.4121e-01,  2.2430e-01,  ...,  4.5820e-01,\n",
       "            2.4786e-01, -1.6214e-01],\n",
       "          [ 6.6099e-01,  1.1048e+00,  4.7537e-01,  ..., -4.5856e-02,\n",
       "           -1.8273e-01,  4.3190e-01],\n",
       "          ...,\n",
       "          [ 1.5182e+00,  8.9739e-01, -1.3994e+00,  ...,  2.8669e-01,\n",
       "            8.6167e-02, -3.6576e-01],\n",
       "          [-3.4371e-01,  1.7461e-01, -7.1753e-02,  ..., -1.8624e-01,\n",
       "            1.0660e-02, -3.1440e-01],\n",
       "          [-8.8885e-01, -7.8151e-01,  2.3440e-01,  ..., -1.2581e-01,\n",
       "            3.4688e-02, -4.0131e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0536e+00, -2.5360e-01, -1.4415e-01,  ...,  6.3733e-01,\n",
       "            7.1924e-01, -3.0292e-01],\n",
       "          [-4.0195e+00, -1.0963e+00,  1.7182e+00,  ...,  1.1558e+00,\n",
       "           -4.8266e+00,  9.4503e-01],\n",
       "          [-4.8088e+00, -1.5816e+00,  1.0835e+00,  ...,  1.7916e+00,\n",
       "           -5.9645e+00,  1.5640e+00],\n",
       "          ...,\n",
       "          [-7.7809e+00,  1.8606e+00,  1.5537e+00,  ..., -4.6389e+00,\n",
       "           -4.5431e+00,  2.0143e-01],\n",
       "          [-8.7284e+00,  2.8804e+00,  1.8060e+00,  ..., -4.6248e+00,\n",
       "           -5.2051e+00, -6.9487e-01],\n",
       "          [-8.2705e+00,  2.8237e+00,  6.5443e-01,  ..., -2.6490e+00,\n",
       "           -4.3582e+00, -3.5571e-01]],\n",
       "\n",
       "         [[-1.4403e-01, -7.1783e-02,  1.7668e-01,  ..., -4.3279e-02,\n",
       "           -8.7432e-01, -1.8823e-01],\n",
       "          [-4.8142e-01,  9.7218e-01,  5.9513e-01,  ..., -7.6370e-01,\n",
       "           -4.3831e-01, -6.4716e-01],\n",
       "          [ 3.7398e-02,  1.1674e+00, -2.8397e-02,  ..., -4.5417e-01,\n",
       "           -1.8860e+00, -3.0403e-01],\n",
       "          ...,\n",
       "          [ 5.7619e-01, -1.1996e-01,  6.7592e-01,  ...,  9.4497e-01,\n",
       "           -6.0030e-01, -1.1133e+00],\n",
       "          [ 1.9473e+00, -5.4697e-01, -1.0935e-01,  ...,  1.5902e+00,\n",
       "           -1.0903e+00, -1.6936e+00],\n",
       "          [ 2.0926e+00,  1.0245e+00,  6.7614e-01,  ...,  2.5141e-01,\n",
       "           -1.2373e+00, -3.1731e+00]],\n",
       "\n",
       "         [[ 1.9909e-01,  2.9866e-01,  1.1228e+00,  ..., -4.4472e-01,\n",
       "            4.2596e-01, -4.9248e-01],\n",
       "          [-1.1398e+00, -1.7616e+00, -1.1536e+00,  ...,  8.0484e-02,\n",
       "           -1.4784e+00,  1.2447e+00],\n",
       "          [-6.3763e-01, -2.7431e+00, -2.2406e+00,  ..., -2.5608e-01,\n",
       "           -1.6898e+00,  8.9649e-01],\n",
       "          ...,\n",
       "          [-1.2072e+00,  2.3145e-01, -3.7642e+00,  ..., -1.6701e+00,\n",
       "            3.6383e-02,  3.0554e-01],\n",
       "          [-1.6725e+00,  4.6836e-01, -6.2146e+00,  ..., -3.5908e-01,\n",
       "           -1.8380e-01, -1.3187e+00],\n",
       "          [-7.6442e-01,  9.9524e-01, -5.0808e+00,  ..., -1.1363e+00,\n",
       "           -7.7593e-02, -1.4667e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5089e-01,  6.4240e-02, -2.3219e-01,  ..., -4.5589e-03,\n",
       "            1.6109e-01,  3.3425e-02],\n",
       "          [-2.4799e+00,  3.1565e-01, -2.8224e-01,  ...,  1.9091e+00,\n",
       "            1.6677e-02, -2.2545e-01],\n",
       "          [-2.5098e+00, -5.1746e-01, -1.0629e+00,  ...,  2.6185e+00,\n",
       "            3.0374e-02, -3.5271e-02],\n",
       "          ...,\n",
       "          [ 6.6646e-01,  5.2549e-01,  3.9020e-02,  ...,  9.9284e-01,\n",
       "            3.8403e-01, -1.2352e-01],\n",
       "          [ 1.8831e+00,  2.3697e-01,  7.8125e-01,  ...,  8.3594e-01,\n",
       "            1.2977e-01,  1.3680e-01],\n",
       "          [ 2.1172e-01,  1.0149e+00, -6.6963e-01,  ...,  1.6212e+00,\n",
       "           -5.8903e-01, -1.2378e-01]],\n",
       "\n",
       "         [[-3.3834e-01, -2.1683e+00,  1.1002e-01,  ..., -8.1372e-02,\n",
       "           -3.5144e-02,  9.0684e-01],\n",
       "          [ 2.0053e+00,  1.0791e+00,  1.3374e+00,  ..., -2.4012e-01,\n",
       "           -2.7358e-01,  5.5878e-02],\n",
       "          [ 4.4226e-01,  3.8310e+00,  1.9771e-01,  ...,  1.5380e+00,\n",
       "           -5.1493e-03, -9.2948e-01],\n",
       "          ...,\n",
       "          [ 1.3427e+00,  3.0002e+00,  3.8335e-01,  ...,  8.9047e-01,\n",
       "           -3.8696e-01,  1.2764e+00],\n",
       "          [ 3.0760e+00,  2.9731e+00,  1.0633e+00,  ..., -2.0210e-01,\n",
       "           -1.7526e+00, -1.8428e-01],\n",
       "          [ 3.0015e+00,  1.9095e+00, -1.9813e+00,  ...,  6.1238e-02,\n",
       "           -1.1149e+00, -1.1443e-01]],\n",
       "\n",
       "         [[ 3.6939e-01,  7.8904e-02, -1.3272e-01,  ...,  6.4292e-01,\n",
       "            1.3957e-01,  2.5835e-01],\n",
       "          [-2.9747e-01, -1.3493e+00, -1.8610e+00,  ...,  7.3069e-02,\n",
       "            8.2028e-01, -8.1921e-01],\n",
       "          [-1.1045e+00, -6.9667e-01,  4.7500e-01,  ..., -1.8038e+00,\n",
       "            4.5443e-01,  2.2041e-01],\n",
       "          ...,\n",
       "          [-2.3496e+00,  6.3784e-01,  2.3881e-01,  ..., -6.0001e-01,\n",
       "            1.9833e+00,  6.1995e-02],\n",
       "          [-1.8100e+00,  9.4474e-01, -2.3405e-01,  ...,  3.8942e-01,\n",
       "            3.1295e+00,  1.1963e+00],\n",
       "          [-1.6907e+00,  1.0048e+00,  2.3557e-01,  ...,  7.9127e-01,\n",
       "            2.8099e+00,  8.9465e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.9884e-02,  5.1915e-02, -4.3782e-02,  ..., -1.8164e-02,\n",
       "            1.0074e-02,  3.0064e-02],\n",
       "          [-2.3064e-01, -4.7864e-01,  6.8049e-01,  ...,  3.3842e-01,\n",
       "           -4.0564e-03, -5.6230e-01],\n",
       "          [-4.7843e-03, -1.4305e-01,  1.1812e+00,  ..., -6.8376e-02,\n",
       "           -1.1868e+00, -4.5093e-01],\n",
       "          ...,\n",
       "          [-1.0424e+00, -5.6573e-01,  5.7349e-01,  ...,  2.4162e-01,\n",
       "            9.0314e-01,  2.6257e-01],\n",
       "          [ 6.6354e-01, -1.3859e+00, -5.3758e-01,  ..., -4.4482e-01,\n",
       "           -8.3053e-01,  3.6726e-01],\n",
       "          [ 6.1318e-01,  3.2910e-01, -4.3346e-01,  ..., -2.3265e-01,\n",
       "           -4.1946e-01,  1.0781e+00]],\n",
       "\n",
       "         [[ 2.0646e-03, -1.4296e-02,  2.4685e-02,  ...,  1.7768e-02,\n",
       "           -5.4372e-02,  8.8987e-03],\n",
       "          [ 8.6335e-02,  2.6505e-01, -2.5070e-01,  ...,  1.6618e+00,\n",
       "            1.4136e+00,  7.0177e-01],\n",
       "          [-2.0781e-01,  2.5619e-01, -6.4890e-01,  ...,  2.2444e+00,\n",
       "            3.8814e-01,  7.3769e-01],\n",
       "          ...,\n",
       "          [ 1.2664e+00, -6.0042e-01, -4.6572e-01,  ..., -2.2863e+00,\n",
       "            2.0619e-01, -2.5362e-01],\n",
       "          [ 1.4005e-01,  7.5321e-01, -1.2592e+00,  ..., -6.9631e-02,\n",
       "            1.5311e+00,  1.7212e-01],\n",
       "          [ 8.4338e-01, -1.5856e-01, -2.3731e-01,  ...,  5.9116e-01,\n",
       "            1.5728e+00, -3.0062e-01]],\n",
       "\n",
       "         [[ 3.4065e-02, -2.6869e-02,  5.2846e-02,  ...,  3.0256e-02,\n",
       "           -4.6306e-03,  1.2962e-02],\n",
       "          [-1.1800e+00,  4.3794e-01, -1.5816e+00,  ...,  3.1330e-01,\n",
       "           -3.1370e-01, -1.6404e+00],\n",
       "          [-9.6113e-01, -1.2964e+00, -1.0820e+00,  ...,  1.7956e+00,\n",
       "            7.2904e-01,  3.7587e-01],\n",
       "          ...,\n",
       "          [ 1.4520e+00, -1.2073e+00,  1.9881e+00,  ..., -6.7712e-01,\n",
       "            2.8624e-02,  6.7833e-01],\n",
       "          [ 2.7194e+00, -1.7253e-01,  1.5059e+00,  ...,  4.1159e-01,\n",
       "            1.8963e+00, -1.2217e+00],\n",
       "          [ 1.1651e+00,  2.6421e-01,  1.1145e+00,  ..., -7.9057e-01,\n",
       "            8.6133e-01, -6.3032e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.7704e-01,  9.5664e-02,  5.3037e-02,  ...,  3.0627e-02,\n",
       "            2.1718e-02, -1.2443e-01],\n",
       "          [-4.2509e-01,  5.9189e-01,  4.7236e-01,  ..., -4.5182e-01,\n",
       "            3.1643e-01,  7.3289e-01],\n",
       "          [ 5.2793e-01, -5.7679e-01,  1.0371e+00,  ...,  4.7382e-01,\n",
       "            7.5385e-01, -4.4236e-01],\n",
       "          ...,\n",
       "          [-6.8949e-01,  2.4174e+00, -1.0969e+00,  ..., -1.3937e-01,\n",
       "            2.2933e-01, -8.2386e-01],\n",
       "          [-1.0600e+00,  8.1860e-01, -4.6421e-01,  ..., -2.6968e-01,\n",
       "            3.8284e-02, -1.4318e-01],\n",
       "          [ 4.8954e-02,  7.0854e-01,  4.8097e-01,  ..., -2.0970e-01,\n",
       "           -2.9161e-01,  7.1249e-01]],\n",
       "\n",
       "         [[-6.0286e-01, -7.5445e-03,  5.5734e-02,  ..., -1.3751e-02,\n",
       "            1.3531e-02, -7.8559e-03],\n",
       "          [-3.7562e-01, -3.2935e-02,  5.9335e-01,  ...,  1.7392e+00,\n",
       "           -9.3294e-01, -3.6377e-01],\n",
       "          [-6.3198e-01, -2.9617e-01, -2.0912e-01,  ...,  8.0240e-01,\n",
       "            1.3333e-01,  1.1593e+00],\n",
       "          ...,\n",
       "          [-1.6136e+00, -2.2766e-01,  9.6685e-01,  ..., -3.2586e-01,\n",
       "           -1.4878e-01,  1.2269e+00],\n",
       "          [-2.0751e+00, -4.0694e-01, -1.0414e+00,  ..., -7.2381e-01,\n",
       "           -1.3150e+00, -9.6535e-01],\n",
       "          [-2.3781e+00,  1.0023e+00, -6.5761e-01,  ...,  1.8935e-01,\n",
       "           -1.9804e-01,  1.4868e+00]],\n",
       "\n",
       "         [[-1.9971e-03,  8.3859e-02, -4.8367e-02,  ...,  5.5392e-02,\n",
       "            4.6674e-02, -4.5921e-02],\n",
       "          [-2.8041e-01, -7.3195e-01, -1.9145e-02,  ..., -4.9802e-01,\n",
       "           -1.3140e+00, -9.2487e-01],\n",
       "          [-6.6981e-01, -3.0590e-01, -7.2342e-02,  ..., -8.4792e-01,\n",
       "           -3.8853e-01,  1.2029e+00],\n",
       "          ...,\n",
       "          [ 2.4721e-01,  1.2687e+00, -6.8838e-02,  ..., -5.3978e-01,\n",
       "           -1.2696e+00,  8.0439e-01],\n",
       "          [-6.2538e-01,  1.4284e+00,  3.4591e-01,  ...,  2.1673e-02,\n",
       "           -2.0864e+00,  8.0913e-01],\n",
       "          [-5.2620e-02,  2.1214e-01, -2.6148e-01,  ...,  1.0201e+00,\n",
       "            3.1003e-01,  1.6031e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-2.1543e-02, -2.3244e+00,  1.6912e-01,  ..., -2.3256e-01,\n",
       "           -1.9870e-01,  8.4391e-02],\n",
       "          [-1.0688e+00,  4.5673e+00,  2.2492e-01,  ..., -1.1129e+00,\n",
       "            2.9338e-01,  6.0243e-01],\n",
       "          [-6.9766e-01,  4.6602e+00,  1.3237e+00,  ...,  3.2985e-02,\n",
       "           -4.8057e-01,  1.1180e+00],\n",
       "          ...,\n",
       "          [ 1.1033e-01,  6.5015e+00, -5.1658e-01,  ...,  5.6613e-02,\n",
       "            1.7947e+00, -1.2904e+00],\n",
       "          [ 4.5188e-01,  6.7069e+00,  3.1774e-01,  ...,  2.1333e-01,\n",
       "            3.6777e-03, -7.3919e-01],\n",
       "          [ 5.9694e-01,  5.3151e+00,  1.5551e+00,  ...,  7.0006e-01,\n",
       "            5.8854e-01, -4.9575e-01]],\n",
       "\n",
       "         [[-8.0147e-01,  2.2735e-01,  4.5134e-01,  ..., -5.0883e-01,\n",
       "            1.0630e+00,  1.1094e+00],\n",
       "          [ 1.0572e+00,  3.8916e-01, -5.7794e-01,  ...,  4.2968e-01,\n",
       "            2.3964e+00, -1.5259e+00],\n",
       "          [ 8.2594e-01,  1.4754e+00,  1.0109e+00,  ..., -8.0362e-01,\n",
       "            3.0637e+00, -3.0079e-01],\n",
       "          ...,\n",
       "          [-1.7442e-02, -3.4158e-01, -6.6678e-01,  ..., -1.6451e+00,\n",
       "           -2.5787e+00, -5.1708e-01],\n",
       "          [-5.9258e-01, -1.6097e+00, -1.2877e+00,  ...,  6.1325e-01,\n",
       "           -1.1756e+00, -7.6187e-02],\n",
       "          [-1.0018e+00, -2.1489e+00, -1.0057e+00,  ..., -7.9310e-02,\n",
       "            2.4185e-01,  2.8120e-01]],\n",
       "\n",
       "         [[-8.4958e-01,  4.9224e-01,  1.3529e-02,  ...,  4.8353e-01,\n",
       "           -2.2924e-01,  1.1441e+00],\n",
       "          [ 8.3421e-01,  3.8177e-01, -9.9450e-03,  ...,  2.0276e-01,\n",
       "            3.3332e-01, -1.4027e-01],\n",
       "          [ 5.3947e-01, -8.6654e-01,  1.4584e+00,  ...,  9.8295e-01,\n",
       "            9.9404e-02,  1.1418e+00],\n",
       "          ...,\n",
       "          [ 2.1118e+00,  1.3481e+00,  2.6295e-01,  ...,  1.7442e+00,\n",
       "            1.0134e-01, -6.2050e-01],\n",
       "          [ 2.4859e+00,  3.9423e-01,  1.3163e+00,  ...,  1.2425e+00,\n",
       "            7.1124e-01, -1.7208e-02],\n",
       "          [ 1.6697e+00,  8.8733e-02,  4.5137e-01,  ...,  2.4938e-01,\n",
       "            1.6744e-01, -1.0016e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9380e-01, -1.3464e-01,  1.3853e-01,  ...,  1.8213e-01,\n",
       "            1.7235e+00, -2.8331e+00],\n",
       "          [ 4.3859e-01, -1.3497e+00, -3.0828e-01,  ..., -3.6614e-01,\n",
       "           -3.9093e+00,  5.0205e+00],\n",
       "          [ 2.8327e-01, -4.3698e-01, -9.1707e-01,  ...,  7.9123e-01,\n",
       "           -4.7863e+00,  6.3097e+00],\n",
       "          ...,\n",
       "          [-1.6221e+00,  4.9227e-02,  5.0117e-01,  ...,  6.2627e-01,\n",
       "           -2.2799e+00,  8.4994e+00],\n",
       "          [-2.4272e+00,  5.9808e-02,  9.6270e-01,  ..., -3.2885e-01,\n",
       "           -2.1042e+00,  7.5129e+00],\n",
       "          [-1.2718e+00,  5.2416e-01,  9.0029e-01,  ...,  1.9105e-01,\n",
       "           -2.3123e+00,  7.2839e+00]],\n",
       "\n",
       "         [[ 1.8126e-01,  3.6972e-01,  2.1859e-01,  ..., -2.1335e-01,\n",
       "            3.6049e-02, -1.4231e-01],\n",
       "          [-7.3557e-01, -1.9036e+00, -1.3676e+00,  ...,  6.3733e-01,\n",
       "            1.1878e+00,  7.1999e-01],\n",
       "          [-5.8495e-01, -1.9994e-01, -3.6538e-01,  ...,  1.0159e+00,\n",
       "            6.5255e-01,  9.3959e-01],\n",
       "          ...,\n",
       "          [ 6.9798e-01,  5.6486e-01,  2.2795e-01,  ..., -2.2671e+00,\n",
       "           -4.7526e-01, -2.6346e-01],\n",
       "          [ 1.1170e+00,  5.6349e-01,  4.6231e-01,  ..., -1.5570e+00,\n",
       "           -1.7095e+00, -1.1452e+00],\n",
       "          [ 1.8103e+00, -6.7721e-03,  5.0287e-01,  ..., -1.6781e-01,\n",
       "           -1.0393e+00, -5.8296e-01]],\n",
       "\n",
       "         [[ 3.7898e-01,  1.0026e-01,  6.0708e-01,  ...,  5.0697e-01,\n",
       "            5.8229e-01, -3.2808e-01],\n",
       "          [ 2.3134e-01,  1.7402e-01,  1.5233e-01,  ..., -1.0236e+00,\n",
       "           -3.2772e+00, -1.9026e-01],\n",
       "          [-3.1852e-01, -2.7019e-01, -5.8078e-01,  ..., -7.7629e-01,\n",
       "           -4.4657e+00,  1.0559e+00],\n",
       "          ...,\n",
       "          [ 2.0485e+00, -5.4305e-01,  1.7715e-01,  ..., -1.1304e+00,\n",
       "           -1.0953e+00,  3.0902e-01],\n",
       "          [ 1.3257e+00, -1.5193e+00,  8.1129e-02,  ...,  7.9923e-01,\n",
       "           -1.8782e+00, -2.0037e-01],\n",
       "          [ 1.4573e+00, -1.0944e+00, -2.4968e-02,  ..., -1.8185e-01,\n",
       "           -1.4702e+00, -5.6035e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0640, -0.0093, -0.0232,  ...,  0.1203, -0.0728, -0.0305],\n",
       "          [-0.9339,  0.4928,  1.1343,  ..., -0.4401, -0.6832,  0.6281],\n",
       "          [ 0.5948,  0.4340,  1.1965,  ..., -1.5203, -0.0249,  0.3440],\n",
       "          ...,\n",
       "          [-1.2222,  0.5957,  1.4450,  ..., -0.3101,  0.3953,  0.6785],\n",
       "          [-1.2005, -0.1019,  0.0084,  ..., -1.8913, -0.8055,  0.2258],\n",
       "          [-0.0685,  0.6957,  1.0541,  ..., -0.8667, -0.7532, -0.3282]],\n",
       "\n",
       "         [[ 0.0154,  0.0355,  0.0495,  ...,  0.0073,  0.0062, -0.0060],\n",
       "          [-0.0118,  0.0310, -0.3019,  ...,  0.1411,  0.8279, -0.8432],\n",
       "          [ 0.0882,  0.5853, -0.1976,  ...,  0.8385,  1.4269, -2.2868],\n",
       "          ...,\n",
       "          [-1.4061, -0.6549,  0.5929,  ...,  0.3503,  0.6316, -0.7685],\n",
       "          [ 0.2844, -1.4747, -0.8263,  ...,  0.4115, -0.2815,  1.4714],\n",
       "          [-0.2339,  0.7649, -0.8262,  ...,  0.9974, -0.0788, -0.0586]],\n",
       "\n",
       "         [[ 0.0599, -0.0399,  0.0624,  ...,  0.0573, -0.0634, -0.0559],\n",
       "          [ 0.3488, -0.9394,  0.8745,  ...,  0.1113,  1.1521,  0.3257],\n",
       "          [-0.0946,  0.4037,  0.0970,  ...,  0.1887,  0.3931, -0.2897],\n",
       "          ...,\n",
       "          [ 0.3627,  0.0787,  0.7352,  ...,  0.9422,  0.3803, -0.1153],\n",
       "          [-0.1398,  0.9014, -0.0382,  ..., -1.4730, -0.9562, -0.7093],\n",
       "          [ 0.4342,  0.0664,  0.5491,  ..., -0.4692, -0.6729,  0.5040]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0790, -0.0351,  0.0365,  ..., -0.0759,  0.0287,  0.0118],\n",
       "          [ 0.7139,  0.2407,  1.1201,  ..., -0.3008, -0.4133,  0.8349],\n",
       "          [-0.0673, -0.5311, -0.3224,  ..., -0.7502, -0.5068,  0.0979],\n",
       "          ...,\n",
       "          [-0.8117,  0.0237, -0.2712,  ..., -0.1390,  0.4283, -0.4629],\n",
       "          [-1.5259, -1.5000, -0.8900,  ..., -0.3665, -0.6253, -1.6187],\n",
       "          [-0.4619, -1.2241, -0.8067,  ..., -0.0847,  0.1610, -1.4660]],\n",
       "\n",
       "         [[ 0.1415, -0.0573,  0.1239,  ...,  0.0703,  0.0341, -0.1218],\n",
       "          [ 0.4314, -0.2954, -0.7229,  ...,  1.0035, -1.3341, -1.1868],\n",
       "          [ 2.0765,  1.2028, -0.7928,  ...,  1.0606,  0.0130, -0.1634],\n",
       "          ...,\n",
       "          [-1.1712,  0.6882, -0.4710,  ...,  0.1568, -0.1871,  2.5416],\n",
       "          [-0.3484,  0.3127, -1.0263,  ..., -0.9464, -1.2304,  0.3416],\n",
       "          [ 0.7712,  0.0862, -1.5546,  ...,  0.7263, -0.4202, -1.7170]],\n",
       "\n",
       "         [[ 0.2112, -0.0553, -0.0571,  ...,  0.0321,  0.0357,  0.0153],\n",
       "          [-0.2932,  0.2864,  0.8706,  ..., -1.2993, -0.9441, -1.4470],\n",
       "          [-1.0550, -0.2235,  0.3924,  ..., -1.2431,  0.3511, -0.1970],\n",
       "          ...,\n",
       "          [-2.0626,  0.4563, -0.8687,  ...,  2.4011, -0.3161, -0.5739],\n",
       "          [-0.6548, -1.5042, -0.5661,  ...,  1.6434,  1.4399,  0.0796],\n",
       "          [-0.9851,  0.5143,  0.5595,  ...,  0.6775,  0.3136,  1.1826]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.3847e-02, -2.7103e-01, -4.6635e-01,  ...,  2.6817e-01,\n",
       "            3.1420e-01,  3.5319e-01],\n",
       "          [-5.9723e-01,  5.2486e-01,  1.5816e-02,  ...,  1.3616e+00,\n",
       "           -1.0957e+00,  3.7375e-01],\n",
       "          [-7.3258e-01, -1.3947e-01, -2.2255e+00,  ...,  3.3521e-01,\n",
       "            2.2482e-01,  7.9658e-01],\n",
       "          ...,\n",
       "          [ 1.2551e+00, -1.7154e+00,  1.8440e+00,  ..., -1.7841e+00,\n",
       "            1.3066e+00, -8.3231e-01],\n",
       "          [-7.8729e-01, -1.7735e+00,  1.9363e-01,  ..., -5.6412e-01,\n",
       "            1.7041e+00,  1.1578e+00],\n",
       "          [-1.2819e+00, -1.0764e+00,  4.7137e-01,  ...,  6.0678e-01,\n",
       "           -1.9767e-01,  1.5907e+00]],\n",
       "\n",
       "         [[-2.7085e-01,  1.5865e-01,  1.0802e-01,  ...,  4.4171e-02,\n",
       "           -1.1278e+00, -1.4037e-01],\n",
       "          [-2.1338e-02, -1.3783e+00,  1.5844e-01,  ...,  4.1910e-01,\n",
       "           -2.5215e+00, -2.3510e-01],\n",
       "          [ 7.2622e-01,  5.0730e-01, -4.9024e-01,  ...,  1.3332e+00,\n",
       "           -6.2483e-01, -3.1145e-01],\n",
       "          ...,\n",
       "          [-2.0859e+00,  1.8610e+00, -7.9441e-01,  ..., -7.5679e-01,\n",
       "           -9.9863e-01, -1.4289e+00],\n",
       "          [-1.7790e+00,  1.3898e+00, -1.0819e+00,  ..., -2.3195e+00,\n",
       "           -8.0305e-01, -4.1783e-01],\n",
       "          [-1.1365e+00, -2.1856e-01, -1.7497e+00,  ..., -4.4053e-01,\n",
       "           -4.0448e-01, -2.6843e+00]],\n",
       "\n",
       "         [[-1.2288e+00, -1.3354e-01,  5.5077e-01,  ..., -6.5791e-01,\n",
       "            4.5362e-01, -2.7105e-01],\n",
       "          [ 1.5936e+00,  2.5516e-02, -1.0983e+00,  ..., -2.4271e-01,\n",
       "            8.2043e-01,  7.0062e-01],\n",
       "          [ 2.2662e+00,  1.2950e-01,  1.9265e-01,  ..., -6.2548e-02,\n",
       "            3.7505e-01,  1.8120e+00],\n",
       "          ...,\n",
       "          [ 4.2548e+00,  1.5053e+00,  1.3611e+00,  ..., -7.5544e-02,\n",
       "           -7.4049e-01,  2.0979e+00],\n",
       "          [ 2.2662e+00,  2.0798e+00, -7.3323e-02,  ..., -1.9006e+00,\n",
       "           -5.3702e-01,  3.2623e+00],\n",
       "          [ 1.9082e+00,  1.5882e-01, -3.4619e-01,  ..., -2.5101e+00,\n",
       "           -8.3711e-02,  2.7423e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.0909e-01, -8.9453e-01, -3.9957e-01,  ..., -1.0118e+00,\n",
       "           -4.3280e-01,  4.7990e-01],\n",
       "          [-4.5124e-01, -5.2411e-01, -4.9088e-01,  ..., -8.5599e-01,\n",
       "           -3.3058e-03, -9.0452e-01],\n",
       "          [ 1.0241e+00, -8.5505e-01,  3.1868e-01,  ...,  6.6742e-01,\n",
       "           -1.7472e-01, -7.6062e-01],\n",
       "          ...,\n",
       "          [-3.5083e+00, -1.1295e+00, -3.8877e-01,  ..., -2.6010e-01,\n",
       "           -1.1008e+00, -2.2286e+00],\n",
       "          [-2.6842e+00,  1.7515e-01,  9.4580e-01,  ..., -1.5469e+00,\n",
       "           -1.0245e+00, -1.2855e+00],\n",
       "          [-1.0326e+00,  1.0967e-01,  6.4690e-01,  ..., -2.1966e+00,\n",
       "            7.2463e-01,  2.6480e-01]],\n",
       "\n",
       "         [[-8.8471e-01,  2.5338e+00,  3.0408e-01,  ...,  3.3582e-01,\n",
       "            1.9476e+00, -5.4479e-01],\n",
       "          [-1.4211e-01, -2.7920e+00, -7.7216e-01,  ...,  1.3296e+00,\n",
       "           -3.3939e+00,  2.5478e+00],\n",
       "          [ 1.5108e+00, -4.2079e+00, -3.3072e-01,  ..., -2.3755e-02,\n",
       "           -4.4663e+00,  2.6436e+00],\n",
       "          ...,\n",
       "          [ 6.3138e-01, -5.2251e+00, -2.5928e+00,  ..., -1.2289e+00,\n",
       "           -4.3507e+00,  8.0261e-01],\n",
       "          [-2.5877e+00, -5.3326e+00, -2.1617e+00,  ...,  2.7442e-01,\n",
       "           -5.2993e+00, -6.4584e-01],\n",
       "          [-7.2948e-01, -5.7631e+00, -2.7656e+00,  ...,  6.0889e-01,\n",
       "           -5.5792e+00,  3.2771e-01]],\n",
       "\n",
       "         [[-2.0108e+00, -3.5352e-01, -1.1085e+00,  ..., -4.1099e-01,\n",
       "            5.2458e-02,  2.4789e-01],\n",
       "          [ 1.1047e+00,  3.4132e-01,  1.4891e+00,  ...,  4.7234e-01,\n",
       "            1.9744e-01,  1.0771e-01],\n",
       "          [ 6.4075e-02, -9.8697e-01,  1.6087e+00,  ...,  3.4333e-01,\n",
       "            2.3664e-01,  3.7446e-01],\n",
       "          ...,\n",
       "          [ 2.3747e+00,  8.6246e-01,  1.4576e+00,  ..., -1.2971e+00,\n",
       "           -1.0753e-01,  1.4335e+00],\n",
       "          [ 2.1958e+00,  2.1849e+00, -4.6725e-01,  ..., -1.1357e-01,\n",
       "           -1.7599e+00, -1.5057e+00],\n",
       "          [ 1.8480e+00,  1.2296e+00,  7.6833e-01,  ...,  4.4813e-01,\n",
       "           -2.3666e+00, -2.2339e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-5.1458e-02, -8.1659e-02,  9.4697e-03,  ...,  1.1953e-01,\n",
       "           -1.9992e-02,  2.8277e-02],\n",
       "          [-2.1095e+00, -1.2403e+00, -5.1268e-01,  ..., -6.3670e-01,\n",
       "           -1.2152e+00, -3.8966e-01],\n",
       "          [ 1.6320e-01,  3.0276e-02, -1.2027e+00,  ..., -2.4204e-01,\n",
       "           -3.6643e-02,  3.9995e-01],\n",
       "          ...,\n",
       "          [ 2.9197e-01, -1.5698e-01, -2.5415e+00,  ...,  3.7218e-01,\n",
       "            4.8675e-01, -5.6260e-01],\n",
       "          [ 1.8562e-02,  5.4489e-03, -3.3347e-01,  ..., -4.7133e-01,\n",
       "            2.6101e-01,  3.6811e-01],\n",
       "          [ 1.4748e-01, -2.4943e-01, -5.6445e-01,  ..., -1.0897e-01,\n",
       "           -8.8601e-01,  4.4628e-01]],\n",
       "\n",
       "         [[ 2.1568e-02,  1.5539e-02, -3.5102e-02,  ...,  3.2413e-02,\n",
       "            6.9995e-03,  3.0660e-02],\n",
       "          [-7.9902e-01,  2.2430e-01, -3.0814e-01,  ...,  1.5968e+00,\n",
       "            2.5458e-01,  7.4196e-01],\n",
       "          [-1.0517e+00,  8.8730e-01, -7.6585e-01,  ...,  6.5768e-01,\n",
       "           -1.4903e+00, -4.2762e-01],\n",
       "          ...,\n",
       "          [ 1.3129e+00, -2.0406e+00,  6.6002e-01,  ..., -3.2444e-01,\n",
       "            5.7619e-01, -1.4693e+00],\n",
       "          [ 4.8709e-01, -1.0999e+00,  2.4888e+00,  ..., -1.2956e+00,\n",
       "           -5.8040e-01, -2.3050e-02],\n",
       "          [ 8.1469e-01,  4.2942e-01,  3.7680e-02,  ...,  1.1784e+00,\n",
       "           -7.9078e-01, -2.1712e-01]],\n",
       "\n",
       "         [[ 3.1912e-02,  2.7201e-02, -8.3305e-02,  ..., -1.6413e-03,\n",
       "           -1.9092e-02,  5.4865e-03],\n",
       "          [ 1.7704e-01, -6.5763e-01, -1.3385e+00,  ...,  8.6352e-01,\n",
       "           -9.8129e-01, -8.0263e-01],\n",
       "          [ 5.7721e-01,  1.5305e-02, -4.6273e-01,  ..., -6.3657e-01,\n",
       "           -2.6562e+00, -2.6315e-02],\n",
       "          ...,\n",
       "          [-2.6147e-01,  1.5075e+00, -1.5206e+00,  ..., -7.3710e-01,\n",
       "           -9.0024e-01, -1.0379e+00],\n",
       "          [-8.6499e-01, -1.5772e-02, -1.1340e+00,  ..., -1.0005e+00,\n",
       "           -8.3071e-01,  1.2634e+00],\n",
       "          [ 4.5180e-01, -4.5354e-01, -6.7541e-01,  ..., -4.3695e-02,\n",
       "           -1.1269e+00,  2.3739e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7371e-02,  3.1884e-02, -9.8777e-03,  ..., -1.4100e-02,\n",
       "           -2.0021e-02,  1.2138e-02],\n",
       "          [-2.0697e+00,  7.1125e-01,  2.3987e-01,  ..., -9.7527e-01,\n",
       "            2.8377e-01, -8.0342e-01],\n",
       "          [-6.8797e-01,  3.5830e-02,  3.1271e-01,  ...,  6.5905e-01,\n",
       "            8.0242e-01,  1.0323e-01],\n",
       "          ...,\n",
       "          [ 1.2094e+00,  2.3004e-01, -2.2405e+00,  ..., -1.5892e+00,\n",
       "            6.3443e-01,  7.8739e-01],\n",
       "          [-1.3929e-01, -1.1443e+00, -7.1235e-01,  ...,  4.1971e-01,\n",
       "           -3.5577e-01,  1.3237e+00],\n",
       "          [-4.9442e-02, -1.5250e+00,  1.4028e-01,  ..., -1.4411e+00,\n",
       "            2.0835e-02,  8.0173e-01]],\n",
       "\n",
       "         [[-6.6471e-02, -3.2223e-02,  2.2540e-02,  ...,  9.7842e-03,\n",
       "           -5.2845e-02, -1.0674e-01],\n",
       "          [ 4.7492e-01,  4.1843e-01,  7.0658e-01,  ..., -2.9978e-01,\n",
       "            4.9560e-01, -1.0473e-02],\n",
       "          [-1.2344e+00,  4.1311e-01,  8.6633e-01,  ...,  1.3362e-01,\n",
       "            1.0437e+00,  1.0893e-01],\n",
       "          ...,\n",
       "          [ 6.9229e-01, -1.1351e-01, -5.2254e-01,  ..., -1.7085e-01,\n",
       "           -1.1797e+00,  7.2258e-01],\n",
       "          [-6.5653e-01,  6.6902e-01, -5.8718e-01,  ...,  5.4327e-02,\n",
       "            2.1044e-01,  1.5007e+00],\n",
       "          [-1.4612e+00, -4.6235e-01, -8.9959e-01,  ...,  7.2742e-01,\n",
       "            7.9579e-01,  4.6653e-01]],\n",
       "\n",
       "         [[-6.3168e-03,  4.6279e-02, -3.8016e-02,  ..., -1.2970e-02,\n",
       "            1.2337e-02,  3.5677e-03],\n",
       "          [-7.2684e-01, -1.8047e+00, -1.0239e-01,  ...,  7.6665e-01,\n",
       "            7.1775e-01, -1.0636e+00],\n",
       "          [ 9.1779e-02, -9.9994e-01, -2.5585e-01,  ..., -1.3781e-01,\n",
       "            1.0020e+00, -3.6183e-01],\n",
       "          ...,\n",
       "          [ 1.1074e-02,  1.8218e+00, -4.0988e-01,  ...,  1.3821e+00,\n",
       "            5.4632e-01, -5.3400e-01],\n",
       "          [-5.8140e-01,  3.5643e-01,  7.9717e-01,  ...,  1.1752e+00,\n",
       "           -2.2859e-01, -3.9282e-01],\n",
       "          [-6.6415e-01, -2.8963e-01,  4.6113e-01,  ..., -1.3150e-01,\n",
       "            4.8643e-01,  7.9697e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5101,  0.4631, -0.8322,  ..., -1.0171, -1.2999,  0.2143],\n",
       "          [-0.4427,  0.5741, -1.7372,  ...,  0.5010, -0.7198, -1.7926],\n",
       "          [ 0.7938,  0.4489,  0.1935,  ...,  2.2088,  0.0429, -1.2324],\n",
       "          ...,\n",
       "          [ 0.3083, -1.2696,  1.7109,  ...,  3.7311,  1.1369, -0.8803],\n",
       "          [-2.1823,  0.0085,  1.1757,  ...,  3.1016,  1.4065,  0.8258],\n",
       "          [-0.3771, -1.4694,  1.3480,  ...,  2.1735,  0.7538,  0.1632]],\n",
       "\n",
       "         [[ 0.8477, -2.0397,  0.1364,  ...,  0.2266, -2.4383, -0.4368],\n",
       "          [ 0.9876,  0.9013,  1.3325,  ...,  0.1386, -0.3332, -0.6917],\n",
       "          [ 0.4651,  1.9516,  0.2691,  ...,  0.9694,  1.0128,  0.0626],\n",
       "          ...,\n",
       "          [-2.0317,  2.3457, -0.4197,  ...,  2.0323,  1.5111, -0.0163],\n",
       "          [-0.1594,  3.0156,  1.2891,  ...,  1.8561,  1.9353, -2.4024],\n",
       "          [-0.1356,  3.5987,  0.7454,  ..., -0.0686,  4.1070, -1.0455]],\n",
       "\n",
       "         [[ 0.9952,  0.3710, -0.1699,  ..., -0.8015, -1.3885, -0.3711],\n",
       "          [-1.0602, -1.8242,  0.3017,  ...,  1.1272, -0.9605,  0.8284],\n",
       "          [-1.5408, -0.2263, -0.1810,  ...,  0.2324, -0.9299,  0.6177],\n",
       "          ...,\n",
       "          [ 0.3667,  2.2254, -0.0407,  ..., -1.6974,  1.1666,  0.8603],\n",
       "          [ 0.1749, -0.3122,  1.6323,  ...,  0.4266,  2.2611,  1.0541],\n",
       "          [-0.2641,  0.6210,  2.7996,  ..., -0.5605,  1.2612,  1.4195]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2149, -0.5738,  0.4927,  ..., -0.6760,  1.0818,  0.2999],\n",
       "          [ 0.3689,  1.6962, -2.1299,  ..., -1.5814, -2.2931,  0.7936],\n",
       "          [-1.8338,  0.7458, -2.2345,  ..., -2.5514,  0.5263,  1.1821],\n",
       "          ...,\n",
       "          [-0.1022, -0.6803, -2.5743,  ...,  2.8974, -0.4569,  0.7959],\n",
       "          [-0.0657, -0.3382, -2.8243,  ...,  3.1917, -1.8509,  1.6147],\n",
       "          [ 3.2032, -1.4288, -2.6249,  ...,  1.5599, -0.6265,  1.1802]],\n",
       "\n",
       "         [[ 0.2440,  0.5373,  0.5357,  ...,  0.6948,  0.0655,  0.8298],\n",
       "          [-0.8385,  1.1848,  0.8192,  ...,  1.5199, -0.0356,  2.0233],\n",
       "          [ 0.1276,  1.5510,  0.8776,  ...,  0.7113, -0.1023, -0.3234],\n",
       "          ...,\n",
       "          [ 1.1959, -0.9347, -0.9652,  ...,  1.1234, -0.4306, -3.1003],\n",
       "          [ 1.2076, -2.5056,  1.7673,  ...,  0.3952, -1.1808, -2.4337],\n",
       "          [ 2.5081,  0.4287,  2.4254,  ..., -0.0801, -1.6620, -1.4836]],\n",
       "\n",
       "         [[-0.7319,  0.2990, -1.5733,  ..., -0.3913,  0.2252, -1.3146],\n",
       "          [-1.0269,  0.2509,  0.4754,  ...,  0.4423, -1.6120,  2.2821],\n",
       "          [ 0.2447,  0.5255,  1.0692,  ...,  0.4804,  0.8703, -0.7257],\n",
       "          ...,\n",
       "          [ 4.6190,  0.5586,  3.5036,  ...,  1.1084,  0.7114, -0.3728],\n",
       "          [ 3.3890, -1.5226,  2.7218,  ...,  0.0539,  0.1049,  4.0889],\n",
       "          [ 0.0930, -0.3444,  1.9178,  ...,  0.9680,  0.0886,  1.0617]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-5.2782e-03,  3.9979e-02, -7.1014e-02,  ...,  5.0068e-02,\n",
       "           -2.0735e-02, -8.0753e-02],\n",
       "          [ 4.4343e-01, -2.1822e-02,  7.7910e-01,  ...,  7.4280e-02,\n",
       "           -1.6666e+00,  6.9921e-01],\n",
       "          [ 1.1865e+00, -1.8466e-01,  9.5035e-01,  ...,  3.9345e-02,\n",
       "           -4.6491e-01,  8.0956e-01],\n",
       "          ...,\n",
       "          [ 8.5368e-02, -2.6742e+00, -2.0505e+00,  ...,  1.1162e+00,\n",
       "            9.3123e-01,  6.0352e-01],\n",
       "          [ 7.9557e-01, -8.8560e-01, -9.5293e-01,  ..., -1.5829e+00,\n",
       "            2.8942e-01, -1.2392e-01],\n",
       "          [ 1.3619e+00,  3.9052e-01, -2.1133e-01,  ...,  4.7233e-01,\n",
       "           -1.0028e+00,  3.5141e-01]],\n",
       "\n",
       "         [[ 4.7071e-02,  4.3442e-03,  2.4028e-02,  ..., -3.5989e-02,\n",
       "           -2.7297e-02, -4.5847e-03],\n",
       "          [-8.0506e-01,  1.0764e+00,  1.7797e+00,  ...,  3.2180e-02,\n",
       "           -5.3073e-01,  4.8943e-01],\n",
       "          [ 5.4961e-01,  9.4726e-01,  6.2674e-01,  ..., -2.4774e-01,\n",
       "           -4.5911e-01,  8.4731e-01],\n",
       "          ...,\n",
       "          [-1.0644e-01, -1.6863e+00, -1.6786e+00,  ..., -1.5259e+00,\n",
       "           -1.4524e+00, -8.0224e-01],\n",
       "          [ 2.9146e-01,  1.1829e+00, -2.9633e+00,  ...,  2.1933e+00,\n",
       "            1.4745e+00, -2.0343e+00],\n",
       "          [ 1.0083e+00,  2.0710e-01, -2.8576e-01,  ..., -4.3477e-02,\n",
       "           -2.9373e-01,  3.7992e-01]],\n",
       "\n",
       "         [[ 3.9197e-04,  1.3192e-02, -2.9043e-02,  ..., -1.1811e-02,\n",
       "            3.7878e-02,  6.5879e-02],\n",
       "          [ 3.7102e-01, -3.3334e-01,  6.8086e-01,  ..., -5.1528e-01,\n",
       "           -1.7715e+00, -3.0208e-01],\n",
       "          [-7.6700e-01, -8.6314e-01, -3.1332e-01,  ...,  2.0547e-02,\n",
       "           -1.0282e-01, -1.9337e-01],\n",
       "          ...,\n",
       "          [-8.9939e-01, -1.1278e+00,  1.3069e+00,  ..., -1.3850e+00,\n",
       "            6.3138e-01,  1.2633e-01],\n",
       "          [ 3.5333e-01, -4.4879e-01,  3.2513e-01,  ...,  1.5420e+00,\n",
       "            6.2370e-01, -1.7354e-01],\n",
       "          [ 3.8547e-01, -2.8499e-01,  1.6007e-01,  ...,  2.1023e+00,\n",
       "           -1.5519e+00,  2.3026e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.0295e-02,  4.8537e-02, -9.5632e-03,  ..., -4.4997e-02,\n",
       "           -1.5520e-02,  3.5403e-03],\n",
       "          [-6.5283e-02, -1.2374e+00, -4.2427e-01,  ...,  1.4061e+00,\n",
       "           -1.8578e-01,  1.1752e+00],\n",
       "          [-1.3915e-01, -8.6969e-01,  1.7049e-01,  ...,  1.0946e-01,\n",
       "           -9.9219e-01,  6.2790e-01],\n",
       "          ...,\n",
       "          [ 1.3238e+00,  2.1523e-01, -4.5903e-01,  ..., -3.8651e-01,\n",
       "            5.7136e-01, -1.4996e+00],\n",
       "          [ 1.5037e+00, -5.2559e-01, -1.0873e+00,  ...,  2.6831e-01,\n",
       "           -7.9303e-01, -3.3915e-01],\n",
       "          [ 1.6986e+00, -9.0949e-01, -1.1674e+00,  ...,  4.0312e-01,\n",
       "           -8.2686e-02, -2.5121e-01]],\n",
       "\n",
       "         [[ 7.3480e-02,  8.9037e-03,  6.0416e-02,  ...,  3.0144e-02,\n",
       "            5.4525e-02,  2.4464e-02],\n",
       "          [ 4.3124e-01,  2.5661e-01,  6.2133e-01,  ...,  2.5251e+00,\n",
       "           -1.7527e-02,  6.2167e-01],\n",
       "          [ 9.5746e-01, -4.7788e-01,  2.3097e+00,  ...,  8.0184e-01,\n",
       "            1.1948e+00,  2.6462e-01],\n",
       "          ...,\n",
       "          [-8.5703e-01, -1.3084e-01, -8.3592e-01,  ...,  1.9567e-01,\n",
       "           -1.0298e+00,  4.3596e-01],\n",
       "          [-1.3125e-01,  5.5256e-01, -6.7067e-01,  ...,  3.0192e-01,\n",
       "           -7.7178e-01,  6.4246e-02],\n",
       "          [-3.4626e-01,  1.2527e+00,  1.8646e+00,  ...,  7.5603e-01,\n",
       "            2.2153e-01, -7.3410e-01]],\n",
       "\n",
       "         [[-1.0680e-01,  3.6414e-02, -4.9533e-02,  ..., -7.6817e-02,\n",
       "            8.7306e-02, -8.0667e-03],\n",
       "          [ 2.8336e-01, -1.1362e+00,  7.3628e-01,  ..., -1.2451e+00,\n",
       "           -1.1988e+00,  1.4288e-01],\n",
       "          [ 1.7003e-01, -7.0002e-01, -4.4489e-01,  ..., -1.3450e+00,\n",
       "           -5.9251e-02,  4.5193e-01],\n",
       "          ...,\n",
       "          [-4.3879e-01,  5.5793e-01, -1.2577e+00,  ..., -5.8847e-01,\n",
       "           -8.4387e-01,  8.5172e-01],\n",
       "          [ 1.9196e-01,  3.9338e-01, -1.0167e+00,  ..., -1.3133e+00,\n",
       "            1.0447e-01,  2.5812e-01],\n",
       "          [-4.6091e-01, -7.9437e-02,  2.5590e-02,  ..., -1.0948e+00,\n",
       "           -3.2881e-01,  1.1716e+00]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.7133e+00, -3.2872e-01, -3.0182e-01,  ...,  1.5828e-01,\n",
       "            3.3646e-01, -5.0207e-01],\n",
       "          [ 3.0166e-01, -3.6156e-01, -4.7163e-02,  ...,  2.1873e+00,\n",
       "           -6.9433e-01, -3.8372e-02],\n",
       "          [ 1.6920e-02,  1.5891e-01,  4.5821e-01,  ...,  1.1587e+00,\n",
       "           -4.0432e-01,  4.8405e-01],\n",
       "          ...,\n",
       "          [ 3.4128e+00,  1.7685e+00,  6.7308e-01,  ...,  7.6119e-01,\n",
       "           -1.3188e+00, -1.8060e-02],\n",
       "          [ 2.2466e+00,  2.1097e+00, -4.4476e-02,  ...,  9.0104e-01,\n",
       "           -1.6945e+00, -1.2754e+00],\n",
       "          [ 2.2161e+00,  1.0407e+00,  7.9585e-01,  ...,  1.6557e+00,\n",
       "           -2.1729e+00, -6.0051e-01]],\n",
       "\n",
       "         [[ 1.1942e-01, -5.9102e-02,  2.2555e+00,  ...,  2.2610e-01,\n",
       "            7.4937e-02, -1.8318e-01],\n",
       "          [ 1.1670e+00, -6.9392e-01, -1.1912e+00,  ...,  1.2430e-01,\n",
       "           -1.3714e-01,  1.4766e-01],\n",
       "          [ 9.6982e-01, -5.9408e-01, -5.8421e-01,  ...,  6.9965e-01,\n",
       "           -3.2750e-01, -2.1382e-01],\n",
       "          ...,\n",
       "          [ 2.0305e-01, -1.3648e+00, -3.1305e+00,  ...,  3.2516e-01,\n",
       "           -1.9163e+00, -9.5953e-01],\n",
       "          [ 1.5322e-01, -1.6796e+00, -3.0080e+00,  ...,  3.0722e+00,\n",
       "           -1.4148e-01, -1.2712e+00],\n",
       "          [ 9.1243e-01, -9.2579e-01, -3.3971e+00,  ...,  1.1099e+00,\n",
       "           -7.8435e-01,  3.2902e-02]],\n",
       "\n",
       "         [[-1.8621e-01,  1.0720e+00,  4.7569e-01,  ..., -5.3742e-01,\n",
       "            3.1283e-01, -1.1606e-01],\n",
       "          [-9.3159e-01,  6.9961e-01,  8.8437e-01,  ...,  1.4470e+00,\n",
       "            5.9384e-01, -5.7699e-01],\n",
       "          [-3.2220e-01,  9.5779e-01, -5.2429e-01,  ...,  1.3277e+00,\n",
       "            7.9002e-01, -2.4478e-01],\n",
       "          ...,\n",
       "          [-1.4780e+00, -2.2965e+00, -4.8911e-01,  ...,  2.3278e+00,\n",
       "            3.0376e-01, -1.0390e+00],\n",
       "          [ 1.7148e-01,  1.2471e+00, -2.5431e-01,  ...,  2.2103e+00,\n",
       "            1.3057e+00, -7.3199e-01],\n",
       "          [-2.5975e-01,  8.8789e-02, -1.6634e+00,  ...,  2.4775e+00,\n",
       "            1.2346e+00,  8.8183e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.4576e-01,  9.7378e-01, -8.4748e-01,  ..., -7.1603e-01,\n",
       "            7.0008e-01,  8.4569e-01],\n",
       "          [ 7.9213e-01,  3.7240e-01, -1.5691e+00,  ..., -1.6343e+00,\n",
       "            2.6408e-01,  5.7421e-01],\n",
       "          [ 9.8920e-01,  1.2838e+00, -3.6445e-01,  ..., -9.4179e-01,\n",
       "           -5.7661e-01,  5.1411e-01],\n",
       "          ...,\n",
       "          [ 1.4984e-01,  2.6621e+00,  2.0918e+00,  ..., -1.6653e+00,\n",
       "           -2.2079e+00,  1.9368e+00],\n",
       "          [ 1.6308e-01,  2.2117e+00,  3.9989e-01,  ...,  6.9828e-02,\n",
       "           -4.6106e-01,  1.4474e+00],\n",
       "          [-2.1598e-01,  2.6444e-04,  6.2994e-01,  ..., -5.2526e-01,\n",
       "           -7.9593e-01,  6.6429e-01]],\n",
       "\n",
       "         [[-4.2274e-01,  3.6864e-01,  3.4033e-01,  ...,  6.7669e-01,\n",
       "            1.0052e-02, -9.6563e-02],\n",
       "          [-5.4143e-01,  1.3198e+00, -6.1182e-04,  ..., -2.8321e-02,\n",
       "           -5.2087e-01, -3.1865e-01],\n",
       "          [-1.3306e+00,  1.5208e+00, -7.1807e-01,  ...,  5.9455e-01,\n",
       "           -6.0484e-01, -7.9354e-01],\n",
       "          ...,\n",
       "          [ 1.4060e+00, -3.8287e-01, -5.4151e-01,  ..., -1.0189e+00,\n",
       "           -1.6387e+00,  1.6913e+00],\n",
       "          [ 2.2717e+00,  7.9313e-01, -4.3403e-01,  ..., -7.9705e-01,\n",
       "            5.4158e-02, -1.4670e+00],\n",
       "          [ 1.7118e+00,  5.7117e-01, -1.4936e+00,  ..., -1.1024e+00,\n",
       "           -1.9008e-01, -1.1366e+00]],\n",
       "\n",
       "         [[-7.1334e-01, -1.1549e-02,  4.3224e-01,  ..., -9.0223e-02,\n",
       "            9.9541e-03, -7.1138e-02],\n",
       "          [ 2.0246e-01, -7.9225e-01,  5.4485e-01,  ..., -3.6827e-01,\n",
       "           -2.3457e+00, -1.3852e+00],\n",
       "          [-7.9605e-01, -3.1638e+00,  2.7436e+00,  ...,  2.3264e+00,\n",
       "            1.0671e-01,  1.0889e+00],\n",
       "          ...,\n",
       "          [ 6.7735e-01, -3.0664e+00, -1.8773e-01,  ...,  9.5061e-01,\n",
       "            1.5670e+00, -1.1908e+00],\n",
       "          [-1.7722e+00, -2.3688e+00, -6.0061e-01,  ...,  1.6259e+00,\n",
       "            1.2796e+00, -4.5737e-01],\n",
       "          [-8.0521e-01, -1.7155e+00,  4.1517e-01,  ...,  1.4617e+00,\n",
       "            4.8930e-01, -6.7193e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0947, -0.1177, -0.1963,  ..., -0.2958,  0.2305, -0.1395],\n",
       "          [ 0.1843, -0.7639,  0.5029,  ...,  3.3875, -1.6446,  1.1150],\n",
       "          [-0.8160,  0.7279, -0.7039,  ...,  2.0674,  0.2236,  1.8786],\n",
       "          ...,\n",
       "          [ 0.4883,  2.1952,  3.4187,  ...,  2.1157, -3.6163, -0.5772],\n",
       "          [-0.8040,  0.9173,  3.3321,  ...,  1.4704, -0.8860,  0.9179],\n",
       "          [-0.3546,  1.9872, -0.6007,  ...,  0.1190, -3.8996,  1.5982]],\n",
       "\n",
       "         [[ 0.0762, -0.0204,  0.0297,  ..., -0.0240, -0.1098,  0.1773],\n",
       "          [-0.4099,  0.7718,  0.5974,  ..., -0.4127, -1.0300, -1.7840],\n",
       "          [ 1.0772,  0.0149, -1.5667,  ..., -0.2379, -0.0383, -0.3378],\n",
       "          ...,\n",
       "          [-0.7476, -1.3036, -0.9662,  ...,  1.3492,  1.8874,  2.0094],\n",
       "          [-0.1978, -0.7571, -1.9891,  ..., -1.1779,  0.1737,  0.6307],\n",
       "          [ 0.1449,  0.7403, -1.6022,  ...,  1.3594, -1.6225,  0.1296]],\n",
       "\n",
       "         [[-0.0111,  0.0425, -0.0606,  ...,  0.0227,  0.0054,  0.0565],\n",
       "          [-1.4641, -0.1191,  0.3820,  ...,  0.7235,  0.0739, -0.8409],\n",
       "          [ 0.9460, -0.7498,  0.2097,  ...,  1.2549,  0.6113,  0.6453],\n",
       "          ...,\n",
       "          [-1.5946, -0.9274,  2.2994,  ..., -0.0511, -3.8319, -1.0054],\n",
       "          [-3.1564, -1.4553,  1.8195,  ..., -1.8782, -0.4622,  1.3305],\n",
       "          [-0.1242,  0.3970, -0.7036,  ..., -0.7762, -0.1564, -0.7802]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0086, -0.0060,  0.0995,  ...,  0.0777, -0.0321,  0.0421],\n",
       "          [-0.3067,  1.2525,  0.3499,  ..., -1.1020,  1.1097,  0.5867],\n",
       "          [-0.0534,  0.8641, -0.1680,  ..., -1.1161,  0.1532,  0.3767],\n",
       "          ...,\n",
       "          [ 1.2170,  0.8690,  0.5323,  ...,  2.4142, -1.1500,  1.5022],\n",
       "          [ 0.0790, -0.5706, -0.9584,  ...,  0.5414,  0.0302, -0.1013],\n",
       "          [-0.3981,  1.0500, -0.5607,  ..., -0.0229, -0.1451,  0.9351]],\n",
       "\n",
       "         [[-0.1915, -0.0732,  0.0676,  ..., -0.0580,  0.0295, -0.0954],\n",
       "          [ 0.9159, -0.5715,  0.0333,  ..., -1.1310,  0.9373,  1.7271],\n",
       "          [ 0.1169, -0.6618, -0.0418,  ..., -0.4665,  0.5717,  0.4366],\n",
       "          ...,\n",
       "          [ 2.5079, -0.7249, -0.6315,  ..., -1.9214, -0.0997, -0.7961],\n",
       "          [-0.4413,  0.6244, -0.4877,  ...,  0.9100, -1.7807, -0.4793],\n",
       "          [ 0.3084,  0.1429,  0.3158,  ..., -0.2071,  1.0501,  0.7917]],\n",
       "\n",
       "         [[ 0.1118, -0.1385,  0.1534,  ..., -0.1420, -0.0146, -0.1811],\n",
       "          [-0.6334,  0.6112, -0.7742,  ..., -1.6886,  0.1966,  0.0138],\n",
       "          [ 0.7271,  0.8768, -1.5154,  ...,  1.9810,  1.3745,  1.3437],\n",
       "          ...,\n",
       "          [ 0.4299, -0.9497, -5.2481,  ..., -0.5052, -0.1451,  0.7803],\n",
       "          [ 0.0341, -2.2145, -1.4534,  ..., -0.1800,  0.3892,  1.3285],\n",
       "          [ 0.1253, -0.7750,  0.2096,  ..., -0.3361, -0.3413,  1.0638]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**batch.to(device))\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "outputs\n",
    "# model.transformer.wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aneesh.chavan/miniconda3/envs/droid-slam/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50263, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50263, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "#     print(name)\n",
    "#     # break\n",
    "# # model.lm_head.requires_grad = True\n",
    "for p in model.lm_head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9b474c48d6402db3ebe49a58c69a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> 11\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 269])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print(batch['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2118087392.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[102], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PrefixTunedGPT2(nn.Module):\n",
    "    def __init__(self, num_prompts, model, tokenizer):\n",
    "        super(PrefixTunedGPT2, self).__init__()\n",
    "        self.soft_prompt = nn.Embedding(num_prompts, 768)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        # get all embs\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "numel: integer multiplication overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m b \u001b[39m=\u001b[39m batch\n\u001b[1;32m      2\u001b[0m \u001b[39m# print(b)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39;49m(model\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mwte(torch\u001b[39m.\u001b[39;49mTensor([\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mint()\u001b[39m.\u001b[39;49mcuda()))\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/_tensor.py:427\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    424\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/_tensor_str.py:637\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    636\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/_tensor_str.py:568\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    566\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    567\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[1;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[1;32m    571\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/_tensor_str.py:328\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    325\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    329\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/miniconda3/envs/droid-slam/lib/python3.9/site-packages/torch/_tensor_str.py:115\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mlen\u001b[39m(value_str))\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     nonzero_finite_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmasked_select(\n\u001b[1;32m    116\u001b[0m         tensor_view, torch\u001b[39m.\u001b[39;49misfinite(tensor_view) \u001b[39m&\u001b[39;49m tensor_view\u001b[39m.\u001b[39;49mne(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m nonzero_finite_vals\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m         \u001b[39m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: numel: integer multiplication overflow"
     ]
    }
   ],
   "source": [
    "b = batch\n",
    "# print(b)\n",
    "print(model.transformer.wte(torch.Tensor([1]).int().cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptgpt2 = PrefixTunedGPT2(4, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrefixTunedGPT2(\n",
       "  (soft_prompt): Embedding(4, 768)\n",
       "  (model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50263, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{'input_ids': torch.Size([1, 155]),\n",
    " 'attention_mask': torch.Size([1, 155]),\n",
    " 'labels': torch.Size([1, 155])}\n",
    "\"\"\"\n",
    "\n",
    "ptgpt2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droid-slam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
